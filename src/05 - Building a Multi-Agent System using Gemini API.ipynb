{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Building a Multi-Agent System using Gemini API\n",
        "\n",
        "## üéØ Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will:\n",
        "- ‚úÖ Understand the fundamentals of multi-agent systems\n",
        "- ‚úÖ Master API key integration with Google Gemini API (both Colab and local environments)\n",
        "- ‚úÖ Learn how to orchestrate multiple AI agents to work together\n",
        "- ‚úÖ Build a complete AI Research Assistant with three specialized agents\n",
        "- ‚úÖ Understand prompt engineering for agent specialization\n",
        "- ‚úÖ Learn error handling and best practices for production systems\n",
        "- ‚úÖ Explore test cases and additional scenarios\n",
        "- ‚úÖ Design a full-stack solution architecture\n",
        "\n",
        "## üìö What You'll Build\n",
        "\n",
        "A sophisticated **AI Research Assistant** composed of three specialized agents:\n",
        "1. **The Planner Agent**: Breaks down complex topics into researchable questions\n",
        "2. **The Researcher Agent**: Searches the web for information using Google Search\n",
        "3. **The Synthesizer Agent**: Compiles research into a comprehensive report\n",
        "\n",
        "## üß† Key Concepts You'll Learn\n",
        "\n",
        "1. **Multi-Agent Systems**: How multiple AI agents collaborate to solve complex problems\n",
        "2. **Agent Orchestration**: Coordinating agents in a workflow\n",
        "3. **API Key Management**: Secure handling of API credentials\n",
        "4. **Prompt Engineering**: Designing effective prompts for specific agent roles\n",
        "5. **Tool Integration**: Using external tools (Google Search) with LLMs\n",
        "6. **Error Handling**: Building robust systems that handle failures gracefully\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ö†Ô∏è Prerequisites\n",
        "\n",
        "Before starting, make sure you have:\n",
        "1. A Google account (for Gemini API access)\n",
        "2. A Gemini API key ([Get it here](https://makersuite.google.com/app/apikey))\n",
        "3. Python 3.8+ installed\n",
        "4. Required packages installed (see next cell)\n",
        "\n",
        "**Note**: This notebook works in both Google Colab and local Jupyter environments. We'll show you how to configure API keys for both scenarios.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ Step 1: Install Required Packages\n",
        "\n",
        "First, let's install the Google GenAI library. This is the official Python SDK for the Gemini API.\n",
        "\n",
        "**‚ö†Ô∏è Important Note**: This notebook uses the new `google-genai` package, which replaces the deprecated `google-generativeai` package. The new package provides better support and access to the latest features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ google-genai installed successfully!\n",
            "üì¶ Version: <module 'google.genai.version' from '/Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages/google/genai/version.py'>\n",
            "üìù Note: Using the new google-genai package (google-generativeai is deprecated)\n"
          ]
        }
      ],
      "source": [
        "# Install the Google GenAI library (new package - replaces deprecated google-generativeai)\n",
        "# %pip install google-genai -q\n",
        "\n",
        "# Verify installation\n",
        "try:\n",
        "    import google.genai as genai\n",
        "    print(\"‚úÖ google-genai installed successfully!\")\n",
        "    print(f\"üì¶ Version: {genai.version if hasattr(genai, 'version') else 'N/A'}\")\n",
        "    print(\"üìù Note: Using the new google-genai package (google-generativeai is deprecated)\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Error importing google-genai: {e}\")\n",
        "    print(\"Please run: pip install google-genai\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîë Step 2: API Key Configuration\n",
        "\n",
        "**This is one of the most important steps!** Proper API key management is crucial for:\n",
        "- Security: Never expose your API keys in code\n",
        "- Flexibility: Support different environments (Colab, local, production)\n",
        "- Best Practices: Follow industry standards for credential management\n",
        "\n",
        "### Understanding API Key Security\n",
        "\n",
        "‚ö†Ô∏è **NEVER commit API keys to version control (Git)**\n",
        "- API keys are like passwords - they give access to your account\n",
        "- Exposed keys can be used by others, leading to unexpected charges\n",
        "- Always use environment variables or secure storage\n",
        "\n",
        "### Two Configuration Methods\n",
        "\n",
        "We'll show you both methods:\n",
        "1. **Google Colab**: Using Colab's built-in Secrets feature\n",
        "2. **Local Environment**: Using environment variables or `.env` files\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìù Configuration function loaded. Ready to configure API key.\n",
            "üì¶ Default model for all agents: gemini-2.5-flash\n",
            "\n",
            "üí° To change the model for ALL agents in the notebook:\n",
            "   1. Edit DEFAULT_MODEL at the top of this cell (currently: 'gemini-2.5-flash')\n",
            "   2. Re-run this cell to reload the configuration\n",
            "   3. All agent functions will automatically use the new model\n",
            "\n",
            "üí° To use this function:\n",
            "   - Local: Set GOOGLE_API_KEY environment variable\n",
            "   - Colab: Add GOOGLE_API_KEY to Secrets\n",
            "   - Testing: Pass api_key parameter directly\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "import google.genai as genai\n",
        "from typing import Optional\n",
        "\n",
        "# ============================================================================\n",
        "# GLOBAL CONFIGURATION: Change the model here to test different models\n",
        "# ============================================================================\n",
        "# Default model for all agents. Change this to test different models:\n",
        "# - 'gemini-2.5-flash' - Recommended for free tier (fast, efficient)\n",
        "# - 'gemini-2.0-flash-exp' - Experimental Flash model\n",
        "# - 'gemini-1.5-pro' - Requires paid tier\n",
        "# - 'gemini-2.5-pro' - Requires paid tier\n",
        "DEFAULT_MODEL = 'gemini-2.5-flash'\n",
        "\n",
        "def configure_gemini(api_key: Optional[str] = None) -> genai.Client:\n",
        "    \"\"\"\n",
        "    Configure the Gemini API with secure API key management.\n",
        "    \n",
        "    This function supports multiple methods for API key retrieval:\n",
        "    1. Direct parameter (for testing)\n",
        "    2. Environment variable (recommended for local development)\n",
        "    3. Google Colab Secrets (for Colab notebooks)\n",
        "    \n",
        "    Args:\n",
        "        api_key: Optional API key string. If not provided, will try to get from:\n",
        "                 - GOOGLE_API_KEY environment variable\n",
        "                 - Colab userdata secrets (if in Colab)\n",
        "    \n",
        "    Note: The model is selected via DEFAULT_MODEL constant at the top of this cell.\n",
        "          All agent functions will use DEFAULT_MODEL unless you explicitly pass a model_name.\n",
        "    \n",
        "    Returns:\n",
        "        Configured genai.Client instance\n",
        "    \n",
        "    Raises:\n",
        "        ValueError: If API key cannot be found\n",
        "    \"\"\"\n",
        "    # Method 1: Use provided API key (for testing)\n",
        "    if api_key:\n",
        "        client = genai.Client(api_key=api_key)\n",
        "        print(\"‚úÖ API key configured from parameter\")\n",
        "        return client\n",
        "    \n",
        "    # Method 2: Try environment variable (recommended for local)\n",
        "    api_key = os.getenv('GOOGLE_API_KEY')\n",
        "    if api_key:\n",
        "        client = genai.Client(api_key=api_key)\n",
        "        print(\"‚úÖ API key configured from environment variable\")\n",
        "        return client\n",
        "    \n",
        "    # Method 3: Try Colab Secrets (only works in Colab)\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        secret_name = \"GOOGLE_API_KEY\"\n",
        "        api_key = userdata.get(secret_name)\n",
        "        if api_key:\n",
        "            client = genai.Client(api_key=api_key)\n",
        "            print(\"‚úÖ API key configured from Colab Secrets\")\n",
        "            return client\n",
        "    except ImportError:\n",
        "        # Not in Colab, continue to error\n",
        "        pass\n",
        "    \n",
        "    # If we get here, no API key was found\n",
        "    raise ValueError(\n",
        "        \"API key not found! Please provide it using one of these methods:\\n\"\n",
        "        \"1. Set environment variable: export GOOGLE_API_KEY='your-key-here'\\n\"\n",
        "        \"2. In Colab: Add 'GOOGLE_API_KEY' to Secrets (üîë icon in sidebar)\\n\"\n",
        "        \"3. Pass directly: configure_gemini(api_key='your-key-here')\\n\\n\"\n",
        "        \"Get your API key from: https://makersuite.google.com/app/apikey\"\n",
        "    )\n",
        "\n",
        "# Test the configuration (will fail if no key is set - that's expected!)\n",
        "print(\"üìù Configuration function loaded. Ready to configure API key.\")\n",
        "print(f\"üì¶ Default model for all agents: {DEFAULT_MODEL}\")\n",
        "print(\"\\nüí° To change the model for ALL agents in the notebook:\")\n",
        "print(f\"   1. Edit DEFAULT_MODEL at the top of this cell (currently: '{DEFAULT_MODEL}')\")\n",
        "print(\"   2. Re-run this cell to reload the configuration\")\n",
        "print(\"   3. All agent functions will automatically use the new model\")\n",
        "print(\"\\nüí° To use this function:\")\n",
        "print(\"   - Local: Set GOOGLE_API_KEY environment variable\")\n",
        "print(\"   - Colab: Add GOOGLE_API_KEY to Secrets\")\n",
        "print(\"   - Testing: Pass api_key parameter directly\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ‚ö†Ô∏è Important: Free Tier Model Restrictions\n",
        "\n",
        "**Free Tier Limitations (as of 2025):**\n",
        "\n",
        "- ‚úÖ **Available on Free Tier:**\n",
        "  - `gemini-2.5-flash` - Fast, efficient model (recommended for free tier) ‚≠ê\n",
        "  - `gemini-2.0-flash-exp` - Experimental Flash model\n",
        "  - Limited quotas (typically 15 requests per minute)\n",
        "\n",
        "- ‚ùå **NOT Available on Free Tier:**\n",
        "  - `gemini-1.5-pro` - Requires paid tier\n",
        "  - `gemini-1.5-flash` - No longer available on free tier\n",
        "  - `gemini-2.5-pro` - Requires paid tier\n",
        "  - Most \"pro\" models require a paid subscription\n",
        "\n",
        "**üí° Recommendation:** Use `gemini-2.5-flash` as your default model for free tier accounts. It's fast, capable, and free!\n",
        "\n",
        "**To check your available models, run the model listing cell below.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üîß API Usage Helper Function\n",
        "\n",
        "The new `google.genai` API uses `client.models.generate_content()` instead of `chats.create().send_message()`. \n",
        "Below is a helper function that works correctly with both free and paid tiers:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Helper function loaded. Use generate_content_safe() for reliable API calls.\n"
          ]
        }
      ],
      "source": [
        "def generate_content_safe(client: genai.Client, model_name: str = None, prompt: str = \"\"):\n",
        "    \"\"\"\n",
        "    Helper function to generate content using the correct API method.\n",
        "    Works with both free and paid tier models.\n",
        "    \n",
        "    Args:\n",
        "        client: Configured genai.Client instance\n",
        "        model_name: Model name (defaults to DEFAULT_MODEL if not provided)\n",
        "        prompt: The prompt text\n",
        "    \n",
        "    Returns:\n",
        "        Response text as string, or None on error\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Use DEFAULT_MODEL if model_name not provided\n",
        "        if model_name is None:\n",
        "            model_name = DEFAULT_MODEL\n",
        "        \n",
        "        # Use models.generate_content() - the correct method\n",
        "        response = client.models.generate_content(\n",
        "            model=model_name,\n",
        "            contents=prompt\n",
        "        )\n",
        "        \n",
        "        # Extract text from response - handle different response formats\n",
        "        if hasattr(response, 'text'):\n",
        "            return response.text\n",
        "        elif hasattr(response, 'candidates') and len(response.candidates) > 0:\n",
        "            # Standard response format\n",
        "            candidate = response.candidates[0]\n",
        "            if hasattr(candidate, 'content') and hasattr(candidate.content, 'parts'):\n",
        "                if len(candidate.content.parts) > 0:\n",
        "                    return candidate.content.parts[0].text\n",
        "        elif hasattr(response, 'content') and hasattr(response.content, 'text'):\n",
        "            return response.content.text\n",
        "        \n",
        "        # Fallback: try to convert to string\n",
        "        return str(response)\n",
        "        \n",
        "    except Exception as e:\n",
        "        error_msg = str(e)\n",
        "        if \"404\" in error_msg or \"not found\" in error_msg.lower():\n",
        "            print(f\"‚ùå Model '{model_name}' not found. This model may not be available on free tier.\")\n",
        "            print(f\"üí° Try using '{DEFAULT_MODEL}' instead, or change DEFAULT_MODEL in the configuration cell.\")\n",
        "        elif \"quota\" in error_msg.lower() or \"limit\" in error_msg.lower():\n",
        "            print(f\"‚ùå Quota exceeded. Please check your API usage limits.\")\n",
        "        else:\n",
        "            print(f\"‚ùå Error: {error_msg}\")\n",
        "        return None\n",
        "\n",
        "# Test the helper function\n",
        "print(\"‚úÖ Helper function loaded. Use generate_content_safe() for reliable API calls.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üîê Setting Up Your API Key (Choose Your Environment)\n",
        "\n",
        "#### Option A: Google Colab Setup\n",
        "\n",
        "1. Click the **üîë (Key)** icon in the left sidebar\n",
        "2. Click **\"Add new secret\"**\n",
        "3. Name: `GOOGLE_API_KEY`\n",
        "4. Value: Your API key from [Google AI Studio](https://makersuite.google.com/app/apikey)\n",
        "5. Click **\"Add secret\"**\n",
        "\n",
        "The code above will automatically detect and use it!\n",
        "\n",
        "#### Option B: Local Jupyter Notebook Setup\n",
        "\n",
        "**Method 1: Environment Variable (Recommended)**\n",
        "```bash\n",
        "# In your terminal (before starting Jupyter):\n",
        "export GOOGLE_API_KEY='your-api-key-here'\n",
        "\n",
        "# Or add to your ~/.bashrc or ~/.zshrc for persistence:\n",
        "echo 'export GOOGLE_API_KEY=\"your-api-key-here\"' >> ~/.zshrc\n",
        "source ~/.zshrc\n",
        "```\n",
        "\n",
        "**Method 2: Using .env file (Alternative)**\n",
        "```python\n",
        "# Install python-dotenv first: pip install python-dotenv\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()  # Loads .env file\n",
        "```\n",
        "\n",
        "**Method 3: Direct Input (For Testing Only)**\n",
        "```python\n",
        "# ‚ö†Ô∏è Only for testing! Never commit this to Git!\n",
        "client = configure_gemini(api_key='your-api-key-here')\n",
        "```\n",
        "\n",
        "### üß™ Test Your Configuration\n",
        "\n",
        "Run the cell below to test if your API key is configured correctly:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Both GOOGLE_API_KEY and GEMINI_API_KEY are set. Using GOOGLE_API_KEY.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ API key configured from environment variable\n",
            "Hello\n"
          ]
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "# Test API key configuration    \n",
        "client = configure_gemini()\n",
        "# Use DEFAULT_MODEL from configuration cell\n",
        "response = client.models.generate_content(model=DEFAULT_MODEL, contents=\"Say 'Hello' in one word.\")\n",
        "# Extract response text\n",
        "if hasattr(response, 'text'):\n",
        "    print(response.text)\n",
        "elif hasattr(response, 'candidates') and len(response.candidates) > 0:\n",
        "    print(response.candidates[0].content.parts[0].text)\n",
        "else:\n",
        "    print(str(response))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sdk_http_response=HttpResponse(\n",
            "  headers=<dict len=11>\n",
            ") candidates=[Candidate(\n",
            "  content=Content(\n",
            "    parts=[\n",
            "      Part(\n",
            "        text='Hello'\n",
            "      ),\n",
            "    ],\n",
            "    role='model'\n",
            "  ),\n",
            "  finish_reason=<FinishReason.STOP: 'STOP'>,\n",
            "  index=0\n",
            ")] create_time=None model_version='gemini-2.5-flash' prompt_feedback=None response_id='GI1cacSDMITPz7IPlf-xqAw' usage_metadata=GenerateContentResponseUsageMetadata(\n",
            "  candidates_token_count=1,\n",
            "  prompt_token_count=9,\n",
            "  prompt_tokens_details=[\n",
            "    ModalityTokenCount(\n",
            "      modality=<MediaModality.TEXT: 'TEXT'>,\n",
            "      token_count=9\n",
            "    ),\n",
            "  ],\n",
            "  thoughts_token_count=45,\n",
            "  total_token_count=55\n",
            ") automatic_function_calling_history=[] parsed=None\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üìã List Available Gemini Models\n",
        "\n",
        "Use the cell below to see all available Gemini models and their capabilities:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Both GOOGLE_API_KEY and GEMINI_API_KEY are set. Using GOOGLE_API_KEY.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ API key configured from environment variable\n",
            "üîç Fetching available Gemini models...\n",
            "\n",
            "‚úÖ Found 32 available model(s):\n",
            "\n",
            "================================================================================\n",
            "\n",
            "1. Model: gemini-2.5-flash\n",
            "   Display Name: Gemini 2.5 Flash\n",
            "   Description: Stable version of Gemini 2.5 Flash, our mid-size multimodal model that supports up to 1 million tokens, released in June of 2025.\n",
            "   Input Token Limit: 1,048,576\n",
            "   Output Token Limit: 65,536\n",
            "\n",
            "2. Model: gemini-2.5-pro\n",
            "   Display Name: Gemini 2.5 Pro\n",
            "   Description: Stable release (June 17th, 2025) of Gemini 2.5 Pro\n",
            "   Input Token Limit: 1,048,576\n",
            "   Output Token Limit: 65,536\n",
            "\n",
            "3. Model: gemini-2.0-flash-exp\n",
            "   Display Name: Gemini 2.0 Flash Experimental\n",
            "   Description: Gemini 2.0 Flash Experimental\n",
            "   Input Token Limit: 1,048,576\n",
            "   Output Token Limit: 8,192\n",
            "\n",
            "4. Model: gemini-2.0-flash\n",
            "   Display Name: Gemini 2.0 Flash\n",
            "   Description: Gemini 2.0 Flash\n",
            "   Input Token Limit: 1,048,576\n",
            "   Output Token Limit: 8,192\n",
            "\n",
            "5. Model: gemini-2.0-flash-001\n",
            "   Display Name: Gemini 2.0 Flash 001\n",
            "   Description: Stable version of Gemini 2.0 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in January of 2025.\n",
            "   Input Token Limit: 1,048,576\n",
            "   Output Token Limit: 8,192\n",
            "\n",
            "6. Model: gemini-2.0-flash-exp-image-generation\n",
            "   Display Name: Gemini 2.0 Flash (Image Generation) Experimental\n",
            "   Description: Gemini 2.0 Flash (Image Generation) Experimental\n",
            "   Input Token Limit: 1,048,576\n",
            "   Output Token Limit: 8,192\n",
            "\n",
            "7. Model: gemini-2.0-flash-lite-001\n",
            "   Display Name: Gemini 2.0 Flash-Lite 001\n",
            "   Description: Stable version of Gemini 2.0 Flash-Lite\n",
            "   Input Token Limit: 1,048,576\n",
            "   Output Token Limit: 8,192\n",
            "\n",
            "8. Model: gemini-2.0-flash-lite\n",
            "   Display Name: Gemini 2.0 Flash-Lite\n",
            "   Description: Gemini 2.0 Flash-Lite\n",
            "   Input Token Limit: 1,048,576\n",
            "   Output Token Limit: 8,192\n",
            "\n",
            "9. Model: gemini-2.0-flash-lite-preview-02-05\n",
            "   Display Name: Gemini 2.0 Flash-Lite Preview 02-05\n",
            "   Description: Preview release (February 5th, 2025) of Gemini 2.0 Flash-Lite\n",
            "   Input Token Limit: 1,048,576\n",
            "   Output Token Limit: 8,192\n",
            "\n",
            "10. Model: gemini-2.0-flash-lite-preview\n",
            "   Display Name: Gemini 2.0 Flash-Lite Preview\n",
            "   Description: Preview release (February 5th, 2025) of Gemini 2.0 Flash-Lite\n",
            "   Input Token Limit: 1,048,576\n",
            "   Output Token Limit: 8,192\n",
            "\n",
            "11. Model: gemini-exp-1206\n",
            "   Display Name: Gemini Experimental 1206\n",
            "   Description: Experimental release (March 25th, 2025) of Gemini 2.5 Pro\n",
            "   Input Token Limit: 1,048,576\n",
            "   Output Token Limit: 65,536\n",
            "\n",
            "12. Model: gemini-2.5-flash-preview-tts\n",
            "   Display Name: Gemini 2.5 Flash Preview TTS\n",
            "   Description: Gemini 2.5 Flash Preview TTS\n",
            "   Input Token Limit: 8,192\n",
            "   Output Token Limit: 16,384\n",
            "\n",
            "13. Model: gemini-2.5-pro-preview-tts\n",
            "   Display Name: Gemini 2.5 Pro Preview TTS\n",
            "   Description: Gemini 2.5 Pro Preview TTS\n",
            "   Input Token Limit: 8,192\n",
            "   Output Token Limit: 16,384\n",
            "\n",
            "14. Model: gemini-flash-latest\n",
            "   Display Name: Gemini Flash Latest\n",
            "   Description: Latest release of Gemini Flash\n",
            "   Input Token Limit: 1,048,576\n",
            "   Output Token Limit: 65,536\n",
            "\n",
            "15. Model: gemini-flash-lite-latest\n",
            "   Display Name: Gemini Flash-Lite Latest\n",
            "   Description: Latest release of Gemini Flash-Lite\n",
            "   Input Token Limit: 1,048,576\n",
            "   Output Token Limit: 65,536\n",
            "\n",
            "16. Model: gemini-pro-latest\n",
            "   Display Name: Gemini Pro Latest\n",
            "   Description: Latest release of Gemini Pro\n",
            "   Input Token Limit: 1,048,576\n",
            "   Output Token Limit: 65,536\n",
            "\n",
            "17. Model: gemini-2.5-flash-lite\n",
            "   Display Name: Gemini 2.5 Flash-Lite\n",
            "   Description: Stable version of Gemini 2.5 Flash-Lite, released in July of 2025\n",
            "   Input Token Limit: 1,048,576\n",
            "   Output Token Limit: 65,536\n",
            "\n",
            "18. Model: gemini-2.5-flash-image-preview\n",
            "   Display Name: Nano Banana\n",
            "   Description: Gemini 2.5 Flash Preview Image\n",
            "   Input Token Limit: 32,768\n",
            "   Output Token Limit: 32,768\n",
            "\n",
            "19. Model: gemini-2.5-flash-image\n",
            "   Display Name: Nano Banana\n",
            "   Description: Gemini 2.5 Flash Preview Image\n",
            "   Input Token Limit: 32,768\n",
            "   Output Token Limit: 32,768\n",
            "\n",
            "20. Model: gemini-2.5-flash-preview-09-2025\n",
            "   Display Name: Gemini 2.5 Flash Preview Sep 2025\n",
            "   Description: Gemini 2.5 Flash Preview Sep 2025\n",
            "   Input Token Limit: 1,048,576\n",
            "   Output Token Limit: 65,536\n",
            "\n",
            "21. Model: gemini-2.5-flash-lite-preview-09-2025\n",
            "   Display Name: Gemini 2.5 Flash-Lite Preview Sep 2025\n",
            "   Description: Preview release (Septempber 25th, 2025) of Gemini 2.5 Flash-Lite\n",
            "   Input Token Limit: 1,048,576\n",
            "   Output Token Limit: 65,536\n",
            "\n",
            "22. Model: gemini-3-pro-preview\n",
            "   Display Name: Gemini 3 Pro Preview\n",
            "   Description: Gemini 3 Pro Preview\n",
            "   Input Token Limit: 1,048,576\n",
            "   Output Token Limit: 65,536\n",
            "\n",
            "23. Model: gemini-3-flash-preview\n",
            "   Display Name: Gemini 3 Flash Preview\n",
            "   Description: Gemini 3 Flash Preview\n",
            "   Input Token Limit: 1,048,576\n",
            "   Output Token Limit: 65,536\n",
            "\n",
            "24. Model: gemini-3-pro-image-preview\n",
            "   Display Name: Nano Banana Pro\n",
            "   Description: Gemini 3 Pro Image Preview\n",
            "   Input Token Limit: 131,072\n",
            "   Output Token Limit: 32,768\n",
            "\n",
            "25. Model: gemini-robotics-er-1.5-preview\n",
            "   Display Name: Gemini Robotics-ER 1.5 Preview\n",
            "   Description: Gemini Robotics-ER 1.5 Preview\n",
            "   Input Token Limit: 1,048,576\n",
            "   Output Token Limit: 65,536\n",
            "\n",
            "26. Model: gemini-2.5-computer-use-preview-10-2025\n",
            "   Display Name: Gemini 2.5 Computer Use Preview 10-2025\n",
            "   Description: Gemini 2.5 Computer Use Preview 10-2025\n",
            "   Input Token Limit: 131,072\n",
            "   Output Token Limit: 65,536\n",
            "\n",
            "27. Model: gemini-embedding-exp-03-07\n",
            "   Display Name: Gemini Embedding Experimental 03-07\n",
            "   Description: Obtain a distributed representation of a text.\n",
            "   Input Token Limit: 8,192\n",
            "   Output Token Limit: 1\n",
            "\n",
            "28. Model: gemini-embedding-exp\n",
            "   Display Name: Gemini Embedding Experimental\n",
            "   Description: Obtain a distributed representation of a text.\n",
            "   Input Token Limit: 8,192\n",
            "   Output Token Limit: 1\n",
            "\n",
            "29. Model: gemini-embedding-001\n",
            "   Display Name: Gemini Embedding 001\n",
            "   Description: Obtain a distributed representation of a text.\n",
            "   Input Token Limit: 2,048\n",
            "   Output Token Limit: 1\n",
            "\n",
            "30. Model: gemini-2.5-flash-native-audio-latest\n",
            "   Display Name: Gemini 2.5 Flash Native Audio Latest\n",
            "   Description: Latest release of Gemini 2.5 Flash Native Audio\n",
            "   Input Token Limit: 131,072\n",
            "   Output Token Limit: 8,192\n",
            "\n",
            "31. Model: gemini-2.5-flash-native-audio-preview-09-2025\n",
            "   Display Name: Gemini 2.5 Flash Native Audio Preview 09-2025\n",
            "   Description: Gemini 2.5 Flash Native Audio Preview 09-2025\n",
            "   Input Token Limit: 131,072\n",
            "   Output Token Limit: 8,192\n",
            "\n",
            "32. Model: gemini-2.5-flash-native-audio-preview-12-2025\n",
            "   Display Name: Gemini 2.5 Flash Native Audio Preview 12-2025\n",
            "   Description: Gemini 2.5 Flash Native Audio Preview 12-2025\n",
            "   Input Token Limit: 131,072\n",
            "   Output Token Limit: 8,192\n",
            "\n",
            "================================================================================\n",
            "\n",
            "üí° To use a model, pass its name to the agent functions:\n",
            "   Example: planner_agent(client, topic, model_name='gemini-2.5-flash')\n"
          ]
        }
      ],
      "source": [
        "def list_gemini_models(client: genai.Client = None, show_details: bool = True):\n",
        "    \"\"\"\n",
        "    List all available Gemini models.\n",
        "    \n",
        "    Args:\n",
        "        client: Optional genai.Client instance. If not provided, will try to configure one.\n",
        "        show_details: If True, shows detailed information about each model\n",
        "    \n",
        "    Returns:\n",
        "        List of available model names\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Get client if not provided\n",
        "        if client is None:\n",
        "            client = configure_gemini()\n",
        "        \n",
        "        print(\"üîç Fetching available Gemini models...\\n\")\n",
        "        \n",
        "        # List all models\n",
        "        models = client.models.list()\n",
        "        \n",
        "        # Filter for Gemini models (they typically start with 'gemini')\n",
        "        gemini_models = [m for m in models if 'gemini' in m.name.lower()]\n",
        "        \n",
        "        if not gemini_models:\n",
        "            print(\"‚ö†Ô∏è  No Gemini models found. Showing all available models:\")\n",
        "            gemini_models = list(models)\n",
        "        \n",
        "        print(f\"‚úÖ Found {len(gemini_models)} available model(s):\\n\")\n",
        "        print(\"=\" * 80)\n",
        "        \n",
        "        model_names = []\n",
        "        for i, model in enumerate(gemini_models, 1):\n",
        "            model_name = model.name.split('/')[-1] if '/' in model.name else model.name\n",
        "            model_names.append(model_name)\n",
        "            \n",
        "            print(f\"\\n{i}. Model: {model_name}\")\n",
        "            \n",
        "            if show_details:\n",
        "                # Display model details if available\n",
        "                if hasattr(model, 'display_name') and model.display_name:\n",
        "                    print(f\"   Display Name: {model.display_name}\")\n",
        "                \n",
        "                if hasattr(model, 'description') and model.description:\n",
        "                    print(f\"   Description: {model.description}\")\n",
        "                \n",
        "                if hasattr(model, 'supported_generation_methods'):\n",
        "                    methods = model.supported_generation_methods\n",
        "                    if methods:\n",
        "                        print(f\"   Supported Methods: {', '.join(methods)}\")\n",
        "                \n",
        "                if hasattr(model, 'input_token_limit'):\n",
        "                    print(f\"   Input Token Limit: {model.input_token_limit:,}\")\n",
        "                \n",
        "                if hasattr(model, 'output_token_limit'):\n",
        "                    print(f\"   Output Token Limit: {model.output_token_limit:,}\")\n",
        "        \n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(f\"\\nüí° To use a model, pass its name to the agent functions:\")\n",
        "        print(f\"   Example: planner_agent(client, topic, model_name='{model_names[0] if model_names else 'gemini-2.5-flash'}')\")\n",
        "        \n",
        "        return model_names\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error listing models: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return []\n",
        "\n",
        "# List available models\n",
        "# Make sure you've configured your API key first!\n",
        "try:\n",
        "    client = configure_gemini()\n",
        "    available_models = list_gemini_models(client)\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error: {e}\")\n",
        "    print(\"Make sure you've configured your API key in the previous cell!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üöÄ Quick Reference: Simple Model List\n",
        "\n",
        "For a quick list of just model names:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Both GOOGLE_API_KEY and GEMINI_API_KEY are set. Using GOOGLE_API_KEY.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ API key configured from environment variable\n",
            "üìã Available Gemini Models:\n",
            "----------------------------------------\n",
            "  ‚Ä¢ gemini-2.5-flash\n",
            "  ‚Ä¢ gemini-2.5-pro\n",
            "  ‚Ä¢ gemini-2.0-flash-exp\n",
            "  ‚Ä¢ gemini-2.0-flash\n",
            "  ‚Ä¢ gemini-2.0-flash-001\n",
            "  ‚Ä¢ gemini-2.0-flash-exp-image-generation\n",
            "  ‚Ä¢ gemini-2.0-flash-lite-001\n",
            "  ‚Ä¢ gemini-2.0-flash-lite\n",
            "  ‚Ä¢ gemini-2.0-flash-lite-preview-02-05\n",
            "  ‚Ä¢ gemini-2.0-flash-lite-preview\n",
            "  ‚Ä¢ gemini-exp-1206\n",
            "  ‚Ä¢ gemini-2.5-flash-preview-tts\n",
            "  ‚Ä¢ gemini-2.5-pro-preview-tts\n",
            "  ‚Ä¢ gemini-flash-latest\n",
            "  ‚Ä¢ gemini-flash-lite-latest\n",
            "  ‚Ä¢ gemini-pro-latest\n",
            "  ‚Ä¢ gemini-2.5-flash-lite\n",
            "  ‚Ä¢ gemini-2.5-flash-image-preview\n",
            "  ‚Ä¢ gemini-2.5-flash-image\n",
            "  ‚Ä¢ gemini-2.5-flash-preview-09-2025\n",
            "  ‚Ä¢ gemini-2.5-flash-lite-preview-09-2025\n",
            "  ‚Ä¢ gemini-3-pro-preview\n",
            "  ‚Ä¢ gemini-3-flash-preview\n",
            "  ‚Ä¢ gemini-3-pro-image-preview\n",
            "  ‚Ä¢ gemini-robotics-er-1.5-preview\n",
            "  ‚Ä¢ gemini-2.5-computer-use-preview-10-2025\n",
            "  ‚Ä¢ gemini-embedding-exp-03-07\n",
            "  ‚Ä¢ gemini-embedding-exp\n",
            "  ‚Ä¢ gemini-embedding-001\n",
            "  ‚Ä¢ gemini-2.5-flash-native-audio-latest\n",
            "  ‚Ä¢ gemini-2.5-flash-native-audio-preview-09-2025\n",
            "  ‚Ä¢ gemini-2.5-flash-native-audio-preview-12-2025\n",
            "----------------------------------------\n",
            "\n",
            "üí° Total: 32 model(s) available\n"
          ]
        }
      ],
      "source": [
        "# Quick list of available Gemini models\n",
        "try:\n",
        "    client = configure_gemini()\n",
        "    models = client.models.list()\n",
        "    \n",
        "    # Extract just the model names (remove path prefix)\n",
        "    gemini_models = [\n",
        "        m.name.split('/')[-1] \n",
        "        for m in models \n",
        "        if 'gemini' in m.name.lower()\n",
        "    ]\n",
        "    \n",
        "    if gemini_models:\n",
        "        print(\"üìã Available Gemini Models:\")\n",
        "        print(\"-\" * 40)\n",
        "        for model in gemini_models:\n",
        "            print(f\"  ‚Ä¢ {model}\")\n",
        "        print(\"-\" * 40)\n",
        "        print(f\"\\nüí° Total: {len(gemini_models)} model(s) available\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  No Gemini models found. Check your API key and permissions.\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error: {e}\")\n",
        "    print(\"Make sure you've configured your API key!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Both GOOGLE_API_KEY and GEMINI_API_KEY are set. Using GOOGLE_API_KEY.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ API key configured from environment variable\n",
            "üéâ Configuration successful! Your API key is working.\n",
            "‚úÖ API Test Response: Hello\n"
          ]
        }
      ],
      "source": [
        "# Test API key configuration\n",
        "try:\n",
        "    client = configure_gemini()\n",
        "    print(\"üéâ Configuration successful! Your API key is working.\")\n",
        "    \n",
        "    # Quick test: Generate a simple response using the correct API method\n",
        "    # Use models.generate_content() - the correct method for new API\n",
        "    # Uses DEFAULT_MODEL from the configuration cell\n",
        "    response = client.models.generate_content(\n",
        "        model=DEFAULT_MODEL,\n",
        "        contents=\"Say 'Hello' in one word.\"\n",
        "    )\n",
        "    \n",
        "    # Extract text from response\n",
        "    if hasattr(response, 'text'):\n",
        "        response_text = response.text\n",
        "    elif hasattr(response, 'candidates') and len(response.candidates) > 0:\n",
        "        response_text = response.candidates[0].content.parts[0].text\n",
        "    else:\n",
        "        response_text = str(response)\n",
        "    \n",
        "    print(f\"‚úÖ API Test Response: {response_text}\")\n",
        "    \n",
        "except ValueError as e:\n",
        "    print(f\"‚ùå Configuration Error:\\n{e}\")\n",
        "    print(\"\\nüìñ Please follow the setup instructions above to configure your API key.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Unexpected Error: {e}\")\n",
        "    print(\"This might indicate an invalid API key or network issue.\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ü§ñ Understanding Multi-Agent Systems\n",
        "\n",
        "### What is a Multi-Agent System?\n",
        "\n",
        "A **Multi-Agent System (MAS)** is a system composed of multiple autonomous agents that:\n",
        "- Work together to solve complex problems\n",
        "- Each agent has a specific role/expertise\n",
        "- Agents communicate and pass information between each other\n",
        "- The system is more capable than any single agent alone\n",
        "\n",
        "### Why Use Multi-Agent Systems?\n",
        "\n",
        "1. **Specialization**: Each agent can be optimized for a specific task\n",
        "2. **Modularity**: Easy to modify or replace individual agents\n",
        "3. **Scalability**: Can add more agents as needed\n",
        "4. **Reliability**: If one agent fails, others can continue\n",
        "5. **Complexity Management**: Break complex problems into manageable pieces\n",
        "\n",
        "### Our Architecture\n",
        "\n",
        "```\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ   User      ‚îÇ\n",
        "‚îÇ  (Topic)    ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "       ‚îÇ\n",
        "       ‚ñº\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ  Planner Agent  ‚îÇ  ‚Üê Breaks topic into questions\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "         ‚îÇ\n",
        "         ‚ñº\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ Researcher Agent‚îÇ  ‚Üê Searches for each question\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "         ‚îÇ\n",
        "         ‚ñº\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇSynthesizer Agent‚îÇ  ‚Üê Combines into final report\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "         ‚îÇ\n",
        "         ‚ñº\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ Final Report‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n",
        "\n",
        "### Key Design Principles\n",
        "\n",
        "1. **Single Responsibility**: Each agent does ONE thing well\n",
        "2. **Clear Interfaces**: Agents communicate through well-defined data structures\n",
        "3. **Error Handling**: Each agent handles its own errors gracefully\n",
        "4. **Orchestration**: A main function coordinates the workflow\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Agent 1: The Planner Agent\n",
        "\n",
        "### Role and Responsibilities\n",
        "\n",
        "The **Planner Agent** is like a project manager. Its job is to:\n",
        "- Take a broad, complex topic\n",
        "- Break it down into 3-5 specific, researchable questions\n",
        "- Ensure questions are answerable and focused\n",
        "- Provide structure to the research process\n",
        "\n",
        "### Why We Need a Planner\n",
        "\n",
        "A vague topic like \"climate change\" is too broad. The Planner breaks it into:\n",
        "- \"What are the main causes of climate change?\"\n",
        "- \"How does climate change affect sea levels?\"\n",
        "- \"What are the economic impacts of climate change?\"\n",
        "- etc.\n",
        "\n",
        "### Prompt Engineering Strategy\n",
        "\n",
        "Notice how we:\n",
        "1. **Define the persona**: \"You are an expert research planner\"\n",
        "2. **Specify the task**: \"Break down the topic into questions\"\n",
        "3. **Constrain the output**: \"Return as Python list of strings\"\n",
        "4. **Provide examples**: Show the expected format\n",
        "\n",
        "This is **prompt engineering** - a critical skill in AI development!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def planner_agent(client: genai.Client, topic: str, model_name: str = None) -> list[str]:\n",
        "    \"\"\"\n",
        "    Planner Agent: Breaks down a broad topic into specific research questions.\n",
        "    \n",
        "    This agent uses prompt engineering to ensure the LLM:\n",
        "    - Understands its role (expert research planner)\n",
        "    - Follows the task (break down topic)\n",
        "    - Returns structured output (list of strings)\n",
        "    \n",
        "    Args:\n",
        "        client: Configured genai.Client instance\n",
        "        topic: The broad research topic to break down\n",
        "        model_name: Optional model name. If not provided, uses DEFAULT_MODEL global constant.\n",
        "                    Change DEFAULT_MODEL in the configuration cell to test different models.\n",
        "    \n",
        "    Returns:\n",
        "        List of specific research questions (strings)\n",
        "        Returns empty list on error\n",
        "    \n",
        "    Example:\n",
        "        >>> client = configure_gemini()\n",
        "        >>> questions = planner_agent(client, \"Artificial Intelligence\")\n",
        "        >>> print(questions)\n",
        "        ['What is the history of AI?', 'What are current AI applications?', ...]\n",
        "    \"\"\"\n",
        "    # Use DEFAULT_MODEL if model_name not provided\n",
        "    if model_name is None:\n",
        "        model_name = DEFAULT_MODEL\n",
        "    \n",
        "    print(\"üìã Planner Agent: Creating a research plan...\")\n",
        "    print(f\"   Using model: {model_name}\")\n",
        "    \n",
        "    # Carefully crafted prompt with clear instructions\n",
        "    prompt = f\"\"\"\n",
        "    You are an expert research planner. Your task is to break down the following topic\n",
        "    into 3-5 specific, answerable questions. These questions should be:\n",
        "    - Focused and researchable\n",
        "    - Cover different aspects of the topic\n",
        "    - Suitable for web search\n",
        "    \n",
        "    TOPIC: \"{topic}\"\n",
        "    \n",
        "    Return ONLY a Python list of strings in this exact format:\n",
        "    [\"question 1\", \"question 2\", \"question 3\"]\n",
        "    \n",
        "    Do not include any other text, explanations, or markdown formatting.\n",
        "    \"\"\"\n",
        "    \n",
        "    try:\n",
        "        # Use models.generate_content() - the correct method for new API\n",
        "        response = client.models.generate_content(\n",
        "            model=model_name,\n",
        "            contents=prompt\n",
        "        )\n",
        "        \n",
        "        # Extract text from response\n",
        "        if hasattr(response, 'text'):\n",
        "            plan_text = response.text.strip()\n",
        "        elif hasattr(response, 'candidates') and len(response.candidates) > 0:\n",
        "            # Standard response format\n",
        "            plan_text = response.candidates[0].content.parts[0].text.strip()\n",
        "        elif hasattr(response, 'content') and hasattr(response.content, 'text'):\n",
        "            plan_text = response.content.text.strip()\n",
        "        else:\n",
        "            # Fallback: try to convert to string\n",
        "            plan_text = str(response).strip()\n",
        "        \n",
        "        # Remove markdown code blocks if present\n",
        "        if plan_text.startswith(\"```\"):\n",
        "            plan_text = plan_text.split(\"```\")[1]\n",
        "            if plan_text.startswith(\"python\") or plan_text.startswith(\"json\"):\n",
        "                plan_text = plan_text.split(\"\\n\", 1)[1]\n",
        "        \n",
        "        # Extract the list\n",
        "        # Remove brackets and quotes, then split by comma\n",
        "        plan_text = plan_text.replace('[', '').replace(']', '').strip()\n",
        "        # Handle both single and double quotes\n",
        "        plan_text = plan_text.replace('\"', '').replace(\"'\", '')\n",
        "        \n",
        "        # Split by comma and clean up\n",
        "        plan = [q.strip() for q in plan_text.split(',') if q.strip()]\n",
        "        \n",
        "        # Validate we got questions\n",
        "        if not plan:\n",
        "            print(\"‚ö†Ô∏è  Warning: Planner returned empty plan. Trying alternative parsing...\")\n",
        "            # Fallback: try to extract questions from natural language\n",
        "            lines = plan_text.split('\\n')\n",
        "            plan = [line.strip().lstrip('- ').lstrip('* ').lstrip('1. ').lstrip('2. ').lstrip('3. ').lstrip('4. ').lstrip('5. ')\n",
        "                   for line in lines if line.strip() and ('?' in line or line.strip().startswith('What') or line.strip().startswith('How'))]\n",
        "            plan = [q for q in plan if len(q) > 10][:5]  # Filter and limit\n",
        "        \n",
        "        if plan:\n",
        "            print(f\"‚úÖ Plan created with {len(plan)} questions:\")\n",
        "            for i, question in enumerate(plan, 1):\n",
        "                print(f\"   {i}. {question}\")\n",
        "        else:\n",
        "            print(\"‚ùå Failed to parse research plan\")\n",
        "        \n",
        "        return plan\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error in Planner Agent: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return []\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Both GOOGLE_API_KEY and GEMINI_API_KEY are set. Using GOOGLE_API_KEY.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ API key configured from environment variable\n",
            "üß™ Testing Planner Agent with topic: 'Quantum Computing'\n",
            "\n",
            "üìã Planner Agent: Creating a research plan...\n",
            "   Using model: gemini-2.5-flash\n",
            "‚úÖ Plan created with 13 questions:\n",
            "   1. What are the fundamental principles and key components (e.g.\n",
            "   2. qubits\n",
            "   3. superposition\n",
            "   4. entanglement) that enable quantum computing?\n",
            "   5. What is the current state of quantum computing development\n",
            "   6. and what are the primary technological and engineering challenges facing its widespread practical application?\n",
            "   7. What are the most promising potential real-world applications of quantum computing across different industries\n",
            "   8. and how might they impact existing technologies?\n",
            "   9. What are the leading quantum computing hardware architectures (e.g.\n",
            "   10. superconducting\n",
            "   11. trapped ion\n",
            "   12. photonic)\n",
            "   13. and what are their respective advantages and disadvantages?\n",
            "\n",
            "‚úÖ Success! Generated 13 research questions.\n"
          ]
        }
      ],
      "source": [
        "# Test the Planner Agent\n",
        "# Make sure you've configured your API key in the earlier cell!\n",
        "\n",
        "try:\n",
        "    client = configure_gemini()\n",
        "    test_topic = \"Quantum Computing\"\n",
        "    print(f\"üß™ Testing Planner Agent with topic: '{test_topic}'\\n\")\n",
        "    \n",
        "    questions = planner_agent(client, test_topic)\n",
        "    \n",
        "    if questions:\n",
        "        print(f\"\\n‚úÖ Success! Generated {len(questions)} research questions.\")\n",
        "    else:\n",
        "        print(\"\\n‚ùå Failed to generate questions. Check your API key and try again.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error: {e}\")\n",
        "    print(\"Make sure you've configured your API key in Step 2!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîç Agent 2: The Researcher Agent\n",
        "\n",
        "### Role and Responsibilities\n",
        "\n",
        "The **Researcher Agent** is the information gatherer. Its job is to:\n",
        "- Take a specific research question\n",
        "- Use Google Search to find current, relevant information\n",
        "- Return detailed answers based on web search results\n",
        "- Provide factual, up-to-date information\n",
        "\n",
        "### Key Concept: Tool Integration\n",
        "\n",
        "This agent demonstrates a crucial concept: **Tool Integration**. The Gemini API supports \"tools\" - external capabilities that the model can use. In this case, we're giving the model access to Google Search.\n",
        "\n",
        "### How Google Search Tool Works\n",
        "\n",
        "1. We define a `Tool` object with `GoogleSearch` (note: `GoogleSearchRetrieval` is deprecated)\n",
        "2. Pass it to `generate_content()` with the `config` parameter\n",
        "3. The model automatically decides when to use the search tool\n",
        "4. It retrieves real-time information from the web\n",
        "5. Uses that information to generate a comprehensive answer\n",
        "\n",
        "This is called **Retrieval-Augmented Generation (RAG)** - combining retrieval (search) with generation (LLM).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def search_agent(client: genai.Client, question: str, model_name: str = None) -> str:\n",
        "    \"\"\"\n",
        "    Researcher Agent: Searches the web for information about a specific question.\n",
        "    \n",
        "    This agent uses Gemini's Google Search Retrieval tool to:\n",
        "    - Access real-time web information\n",
        "    - Find current, relevant sources\n",
        "    - Generate comprehensive answers based on search results\n",
        "    \n",
        "    Args:\n",
        "        client: Configured genai.Client instance\n",
        "        question: The specific research question to answer\n",
        "        model_name: Optional model name. If not provided, uses DEFAULT_MODEL global constant.\n",
        "                    Change DEFAULT_MODEL in the configuration cell to test different models.\n",
        "    \n",
        "    Returns:\n",
        "        Detailed answer string based on web search\n",
        "        Returns empty string on error\n",
        "    \n",
        "    Example:\n",
        "        >>> client = configure_gemini()\n",
        "        >>> answer = search_agent(client, \"What is quantum computing?\")\n",
        "        >>> print(answer)\n",
        "        \"Quantum computing is a type of computation that uses quantum...\"\n",
        "    \"\"\"\n",
        "    # Use DEFAULT_MODEL if model_name not provided\n",
        "    if model_name is None:\n",
        "        model_name = DEFAULT_MODEL\n",
        "    \n",
        "    print(f\"üîç Researcher Agent: Researching question: '{question}'...\")\n",
        "    print(f\"   Using model: {model_name}\")\n",
        "    \n",
        "    try:\n",
        "        # Craft a prompt that encourages comprehensive research\n",
        "        prompt = f\"\"\"\n",
        "        Provide a detailed, well-researched answer to the following question.\n",
        "        Use information from recent and authoritative sources.\n",
        "        Include key facts, statistics, and explanations.\n",
        "        \n",
        "        Question: {question}\n",
        "        \n",
        "        Answer:\n",
        "        \"\"\"\n",
        "        \n",
        "        # Use models.generate_content() with Google Search tool enabled\n",
        "        # The new API enables Google Search through the config parameter\n",
        "        from google.genai import types\n",
        "        \n",
        "        config = types.GenerateContentConfig(\n",
        "            tools=[types.Tool(\n",
        "                google_search=types.GoogleSearch()\n",
        "            )]\n",
        "        )\n",
        "        \n",
        "        # Use models.generate_content() - the correct method for new API\n",
        "        response = client.models.generate_content(\n",
        "            model=model_name,\n",
        "            contents=prompt,\n",
        "            config=config\n",
        "        )\n",
        "        \n",
        "        # Extract text from response\n",
        "        if hasattr(response, 'text'):\n",
        "            answer_text = response.text\n",
        "        elif hasattr(response, 'candidates') and len(response.candidates) > 0:\n",
        "            # Standard response format\n",
        "            answer_text = response.candidates[0].content.parts[0].text\n",
        "        elif hasattr(response, 'content') and hasattr(response.content, 'text'):\n",
        "            answer_text = response.content.text\n",
        "        else:\n",
        "            answer_text = str(response)\n",
        "        \n",
        "        if answer_text:\n",
        "            print(f\"   ‚úÖ Information found ({len(answer_text)} characters)\")\n",
        "            return answer_text\n",
        "        else:\n",
        "            print(\"   ‚ö†Ô∏è  No text in response\")\n",
        "            return \"\"\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Error in Researcher Agent: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return \"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üß™ Test the Researcher Agent\n",
        "\n",
        "Let's test the Researcher Agent with a sample question:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Both GOOGLE_API_KEY and GEMINI_API_KEY are set. Using GOOGLE_API_KEY.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ API key configured from environment variable\n",
            "üß™ Testing Researcher Agent with question: 'What are the latest developments in quantum computing in 2024?'\n",
            "\n",
            "üîç Researcher Agent: Researching question: 'What are the latest developments in quantum computing in 2024?'...\n",
            "   Using model: gemini-2.5-flash\n",
            "   ‚úÖ Information found (6283 characters)\n",
            "\n",
            "‚úÖ Success! Retrieved answer (6283 characters)\n",
            "\n",
            "üìÑ Answer Preview (first 500 chars):\n",
            "------------------------------------------------------------\n",
            "Quantum computing witnessed significant advancements in 2024, moving closer to practical applications across various sectors. Breakthroughs in hardware, software, algorithms, and increased investment are accelerating its development, although challenges like error rates and scalability persist.\n",
            "\n",
            "**Key Developments and Breakthroughs in 2024:**\n",
            "\n",
            "*   **Increased Qubit Stability and Error Correction:** A critical challenge in quantum computing has been maintaining the stability of qubits. In 2024, r...\n",
            "------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Test the Researcher Agent\n",
        "try:\n",
        "    client = configure_gemini()\n",
        "    test_question = \"What are the latest developments in quantum computing in 2024?\"\n",
        "    print(f\"üß™ Testing Researcher Agent with question: '{test_question}'\\n\")\n",
        "    \n",
        "    answer = search_agent(client, test_question)\n",
        "    \n",
        "    if answer:\n",
        "        print(f\"\\n‚úÖ Success! Retrieved answer ({len(answer)} characters)\")\n",
        "        print(f\"\\nüìÑ Answer Preview (first 500 chars):\")\n",
        "        print(\"-\" * 60)\n",
        "        print(answer[:500] + \"...\" if len(answer) > 500 else answer)\n",
        "        print(\"-\" * 60)\n",
        "    else:\n",
        "        print(\"\\n‚ùå Failed to retrieve information. Check your API key and try again.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error: {e}\")\n",
        "    print(\"Make sure you've configured your API key in Step 2!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úçÔ∏è Agent 3: The Synthesizer Agent\n",
        "\n",
        "### Role and Responsibilities\n",
        "\n",
        "The **Synthesizer Agent** is the writer and compiler. Its job is to:\n",
        "- Take all the research results from the Researcher Agent\n",
        "- Combine them into a coherent, well-structured report\n",
        "- Ensure the report covers all aspects of the original topic\n",
        "- Create a professional, readable final document\n",
        "\n",
        "### Key Design Principles\n",
        "\n",
        "1. **Context Aggregation**: Combine all research notes into a single context\n",
        "2. **Structured Output**: Generate a report with introduction, body, and conclusion\n",
        "3. **Source Constraint**: Only use information from provided research (prevents hallucination)\n",
        "4. **Quality Control**: Ensure coherence and flow\n",
        "\n",
        "### Why \"Use Research Notes ONLY\"?\n",
        "\n",
        "This constraint is **critical** for:\n",
        "- **Accuracy**: Prevents the model from making up information\n",
        "- **Traceability**: All information comes from the Researcher Agent\n",
        "- **Reliability**: More predictable and verifiable outputs\n",
        "- **Control**: You know exactly what sources were used\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def synthesizer_agent(\n",
        "    client: genai.Client, \n",
        "    topic: str, \n",
        "    research_results: list[tuple[str, str]],\n",
        "    model_name: str = None\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Synthesizer Agent: Combines research results into a comprehensive report.\n",
        "    \n",
        "    This agent takes all the fragmented research and weaves it into a coherent,\n",
        "    well-structured report. It's critical that this agent ONLY uses the provided\n",
        "    research notes to prevent hallucination.\n",
        "    \n",
        "    Args:\n",
        "        client: Configured genai.Client instance\n",
        "        topic: The original research topic\n",
        "        research_results: List of tuples (question, research_data)\n",
        "        model_name: Optional model name. If not provided, uses DEFAULT_MODEL global constant.\n",
        "                    Change DEFAULT_MODEL in the configuration cell to test different models.\n",
        "    \n",
        "    Returns:\n",
        "        Comprehensive research report as a string\n",
        "        Returns error message on failure\n",
        "    \n",
        "    Example:\n",
        "        >>> client = configure_gemini()\n",
        "        >>> results = [(\"Q1\", \"Answer 1\"), (\"Q2\", \"Answer 2\")]\n",
        "        >>> report = synthesizer_agent(client, \"AI\", results)\n",
        "        >>> print(report)\n",
        "        \"# Research Report: AI\\n\\n## Introduction\\n...\"\n",
        "    \"\"\"\n",
        "    # Use DEFAULT_MODEL if model_name not provided\n",
        "    if model_name is None:\n",
        "        model_name = DEFAULT_MODEL\n",
        "    \n",
        "    print(\"‚úçÔ∏è  Synthesizer Agent: Writing the final report...\")\n",
        "    print(f\"   Using model: {model_name}\")\n",
        "    \n",
        "    # Aggregate all research into a structured format\n",
        "    # This becomes the context for the final synthesis\n",
        "    research_notes = \"\"\n",
        "    for i, (question, data) in enumerate(research_results, 1):\n",
        "        research_notes += f\"\"\"\n",
        "### Research Question {i}: {question}\n",
        "\n",
        "Research Findings:\n",
        "{data}\n",
        "\n",
        "---\n",
        "\"\"\"\n",
        "    \n",
        "    # Carefully crafted prompt with strict constraints\n",
        "    prompt = f\"\"\"\n",
        "    You are an expert research analyst and technical writer. Your task is to synthesize\n",
        "    the provided research notes into a comprehensive, well-structured report on the topic: \"{topic}\".\n",
        "    \n",
        "    IMPORTANT CONSTRAINTS:\n",
        "    1. Use ONLY the information provided in the research notes below\n",
        "    2. Do NOT add information that is not in the research notes\n",
        "    3. If information is missing, acknowledge it rather than making it up\n",
        "    4. Maintain accuracy and cite the research questions when relevant\n",
        "    \n",
        "    REPORT STRUCTURE:\n",
        "    - **Introduction**: Overview of the topic and what will be covered\n",
        "    - **Main Body**: Organized sections covering key findings from the research\n",
        "    - **Conclusion**: Summary of main points and key takeaways\n",
        "    \n",
        "    Write in a professional, clear, and engaging style suitable for a research report.\n",
        "    \n",
        "    ## Research Notes ##\n",
        "    {research_notes}\n",
        "    \n",
        "    ## Report ##\n",
        "    \"\"\"\n",
        "    \n",
        "    try:\n",
        "        # Use models.generate_content() - the correct method for new API\n",
        "        response = client.models.generate_content(\n",
        "            model=model_name,\n",
        "            contents=prompt\n",
        "        )\n",
        "        \n",
        "        # Extract text from response\n",
        "        if hasattr(response, 'text'):\n",
        "            report_text = response.text\n",
        "        elif hasattr(response, 'candidates') and len(response.candidates) > 0:\n",
        "            # Standard response format\n",
        "            report_text = response.candidates[0].content.parts[0].text\n",
        "        elif hasattr(response, 'content') and hasattr(response.content, 'text'):\n",
        "            report_text = response.content.text\n",
        "        else:\n",
        "            report_text = str(response)\n",
        "        \n",
        "        if report_text:\n",
        "            print(f\"   ‚úÖ Report generated ({len(report_text)} characters)\")\n",
        "            return report_text\n",
        "        else:\n",
        "            print(\"   ‚ö†Ô∏è  No text in response\")\n",
        "            return \"Error: Could not generate the final report.\"\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Error in Synthesizer Agent: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return \"Error: Could not generate the final report.\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéº The Orchestrator: Main Function\n",
        "\n",
        "### What is Orchestration?\n",
        "\n",
        "**Orchestration** is the coordination of multiple agents to work together in a workflow. The orchestrator:\n",
        "- Manages the flow of data between agents\n",
        "- Handles errors and edge cases\n",
        "- Ensures each agent runs at the right time\n",
        "- Collects and passes information correctly\n",
        "\n",
        "### Our Orchestration Flow\n",
        "\n",
        "```\n",
        "1. Configure API ‚Üí Get model\n",
        "2. Get user input ‚Üí Topic\n",
        "3. Planner Agent ‚Üí List of questions\n",
        "4. For each question:\n",
        "   ‚îî‚îÄ> Researcher Agent ‚Üí Research data\n",
        "5. Collect all research ‚Üí List of (question, data) tuples\n",
        "6. Synthesizer Agent ‚Üí Final report\n",
        "7. Display report ‚Üí User\n",
        "```\n",
        "\n",
        "### Error Handling Strategy\n",
        "\n",
        "At each step, we check for errors:\n",
        "- If configuration fails ‚Üí Exit gracefully\n",
        "- If planning fails ‚Üí Exit gracefully\n",
        "- If research fails ‚Üí Continue with available data\n",
        "- If synthesis fails ‚Üí Show error message\n",
        "\n",
        "This makes the system **robust** and **user-friendly**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_research_assistant(topic: str, api_key: str = None) -> dict:\n",
        "    \"\"\"\n",
        "    Main orchestrator function that coordinates all agents.\n",
        "    \n",
        "    This function demonstrates the complete multi-agent workflow:\n",
        "    1. Configuration\n",
        "    2. Planning\n",
        "    3. Research (parallelizable in future)\n",
        "    4. Synthesis\n",
        "    5. Output\n",
        "    \n",
        "    Args:\n",
        "        topic: The research topic\n",
        "        api_key: Optional API key (if not using environment variable)\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with:\n",
        "        - 'success': bool\n",
        "        - 'topic': str\n",
        "        - 'plan': list of questions\n",
        "        - 'research_results': list of (question, data) tuples\n",
        "        - 'report': final report string\n",
        "        - 'error': error message if failed\n",
        "    \"\"\"\n",
        "    result = {\n",
        "        'success': False,\n",
        "        'topic': topic,\n",
        "        'plan': [],\n",
        "        'research_results': [],\n",
        "        'report': '',\n",
        "        'error': None\n",
        "    }\n",
        "    \n",
        "    try:\n",
        "        # Step 1: Configure the client\n",
        "        print(\"üîß Step 1: Configuring Gemini API...\")\n",
        "        client = configure_gemini(api_key=api_key)\n",
        "        print(\"‚úÖ Configuration successful!\\n\")\n",
        "        \n",
        "    except ValueError as e:\n",
        "        error_msg = str(e)\n",
        "        print(f\"‚ùå Configuration Error: {error_msg}\")\n",
        "        result['error'] = error_msg\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Unexpected configuration error: {e}\"\n",
        "        print(f\"‚ùå {error_msg}\")\n",
        "        result['error'] = error_msg\n",
        "        return result\n",
        "    \n",
        "    # Step 2: Validate input\n",
        "    if not topic or not topic.strip():\n",
        "        error_msg = \"Topic cannot be empty\"\n",
        "        print(f\"‚ùå {error_msg}\")\n",
        "        result['error'] = error_msg\n",
        "        return result\n",
        "    \n",
        "    print(f\"üìö Starting research process for: '{topic}'\\n\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    # Step 3: Planning phase\n",
        "    print(\"\\nüéØ Phase 1: Planning\")\n",
        "    print(\"-\" * 70)\n",
        "    research_plan = planner_agent(client, topic)\n",
        "    result['plan'] = research_plan\n",
        "    \n",
        "    if not research_plan:\n",
        "        error_msg = \"Could not create a research plan\"\n",
        "        print(f\"\\n‚ùå {error_msg}\")\n",
        "        result['error'] = error_msg\n",
        "        return result\n",
        "    \n",
        "    # Step 4: Research phase\n",
        "    print(f\"\\nüîç Phase 2: Research ({len(research_plan)} questions)\")\n",
        "    print(\"-\" * 70)\n",
        "    research_results = []\n",
        "    \n",
        "    for i, question in enumerate(research_plan, 1):\n",
        "        print(f\"\\n[{i}/{len(research_plan)}] \", end=\"\")\n",
        "        research_data = search_agent(client, question)\n",
        "        if research_data:\n",
        "            research_results.append((question, research_data))\n",
        "        else:\n",
        "            print(f\"   ‚ö†Ô∏è  Skipping question due to error\")\n",
        "    \n",
        "    result['research_results'] = research_results\n",
        "    \n",
        "    if not research_results:\n",
        "        error_msg = \"Could not find any information during research\"\n",
        "        print(f\"\\n‚ùå {error_msg}\")\n",
        "        result['error'] = error_msg\n",
        "        return result\n",
        "    \n",
        "    # Step 5: Synthesis phase\n",
        "    print(f\"\\n‚úçÔ∏è  Phase 3: Synthesis\")\n",
        "    print(\"-\" * 70)\n",
        "    final_report = synthesizer_agent(client, topic, research_results)\n",
        "    result['report'] = final_report\n",
        "    \n",
        "    if not final_report or \"Error:\" in final_report:\n",
        "        error_msg = \"Could not generate final report\"\n",
        "        print(f\"\\n‚ùå {error_msg}\")\n",
        "        result['error'] = error_msg\n",
        "        return result\n",
        "    \n",
        "    # Success!\n",
        "    result['success'] = True\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"‚úÖ Research completed successfully!\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    return result\n",
        "\n",
        "\n",
        "def display_report(result: dict):\n",
        "    \"\"\"\n",
        "    Display the final research report in a formatted way.\n",
        "    \n",
        "    Args:\n",
        "        result: Result dictionary from run_research_assistant()\n",
        "    \"\"\"\n",
        "    if not result['success']:\n",
        "        print(f\"\\n‚ùå Research failed: {result.get('error', 'Unknown error')}\")\n",
        "        return\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"üìÑ FINAL RESEARCH REPORT\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"\\n## Topic: {result['topic']}\\n\")\n",
        "    print(result['report'])\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"--- END OF REPORT ---\")\n",
        "    print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Both GOOGLE_API_KEY and GEMINI_API_KEY are set. Using GOOGLE_API_KEY.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Running Complete Multi-Agent Research System\n",
            "======================================================================\n",
            "Topic: Large Language Models and their applications in healthcare\n",
            "\n",
            "üîß Step 1: Configuring Gemini API...\n",
            "‚úÖ API key configured from environment variable\n",
            "‚úÖ Configuration successful!\n",
            "\n",
            "üìö Starting research process for: 'Large Language Models and their applications in healthcare'\n",
            "\n",
            "======================================================================\n",
            "\n",
            "üéØ Phase 1: Planning\n",
            "----------------------------------------------------------------------\n",
            "üìã Planner Agent: Creating a research plan...\n",
            "   Using model: gemini-2.5-flash\n",
            "‚úÖ Plan created with 3 questions:\n",
            "   1. What are the primary current applications of Large Language Models in clinical decision support and patient care?\n",
            "   2. What are the key benefits and potential advantages of integrating Large Language Models into medical research and drug discovery processes?\n",
            "   3. What are the significant ethical considerations and technical challenges related to the development and deployment of Large Language Models in healthcare environments?\n",
            "\n",
            "üîç Phase 2: Research (3 questions)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "[1/3] üîç Researcher Agent: Researching question: 'What are the primary current applications of Large Language Models in clinical decision support and patient care?'...\n",
            "   Using model: gemini-2.5-flash\n",
            "   ‚úÖ Information found (5006 characters)\n",
            "\n",
            "[2/3] üîç Researcher Agent: Researching question: 'What are the key benefits and potential advantages of integrating Large Language Models into medical research and drug discovery processes?'...\n",
            "   Using model: gemini-2.5-flash\n",
            "   ‚úÖ Information found (6135 characters)\n",
            "\n",
            "[3/3] üîç Researcher Agent: Researching question: 'What are the significant ethical considerations and technical challenges related to the development and deployment of Large Language Models in healthcare environments?'...\n",
            "   Using model: gemini-2.5-flash\n",
            "   ‚úÖ Information found (8269 characters)\n",
            "\n",
            "‚úçÔ∏è  Phase 3: Synthesis\n",
            "----------------------------------------------------------------------\n",
            "‚úçÔ∏è  Synthesizer Agent: Writing the final report...\n",
            "   Using model: gemini-2.5-flash\n",
            "   ‚úÖ Report generated (15809 characters)\n",
            "\n",
            "======================================================================\n",
            "‚úÖ Research completed successfully!\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "üìÑ FINAL RESEARCH REPORT\n",
            "======================================================================\n",
            "\n",
            "## Topic: Large Language Models and their applications in healthcare\n",
            "\n",
            "## Large Language Models and their Applications in Healthcare\n",
            "\n",
            "### Introduction\n",
            "\n",
            "Large Language Models (LLMs) represent a significant advancement in artificial intelligence, leveraging sophisticated natural language processing (NLP) capabilities to understand, interpret, and generate human-like text. Their transformative potential is rapidly being realized across various sectors, with healthcare emerging as a particularly impactful domain. This report synthesizes research findings on the current primary applications of LLMs in clinical decision support and patient care, their key benefits and advantages in medical research and drug discovery processes, and the significant ethical considerations and technical challenges associated with their development and deployment in healthcare environments.\n",
            "\n",
            "### Main Body\n",
            "\n",
            "#### Current Applications in Clinical Decision Support and Patient Care\n",
            "\n",
            "Large Language Models are revolutionizing various facets of healthcare, primarily by enhancing clinical decision support (CDS) and patient care through their advanced NLP capabilities, as highlighted by findings for Research Question 1. Their applications range from streamlining administrative tasks to augmenting diagnostic accuracy and personalizing patient interactions.\n",
            "\n",
            "*   **Clinical Decision Support (CDS) and Diagnostics:** LLMs significantly contribute to CDS by analyzing vast amounts of healthcare data, including electronic health records (EHRs) and complex medical literature. They assist healthcare professionals in identifying potential diagnoses, suggesting appropriate tests, and recommending evidence-based treatments. By processing patient symptoms, medical records, and the latest research, LLMs can provide tailored insights for diagnosis and treatment planning, potentially improving patient outcomes and reducing errors. They can also bridge gaps in human expertise by identifying early disease indicators and generate and refine differential diagnoses by aligning patient information with extensive medical knowledge. Enhancements with clinical practice guidelines (CPGs) allow for more accurate recommendations for specific conditions.\n",
            "\n",
            "*   **Patient Care and Communication Enhancement:** LLMs are improving patient engagement and healthcare accessibility. They function as virtual medical assistants and health chatbots, offering personalized healthcare support, general health information, answering patient queries, providing medication reminders, and monitoring health conditions. LLMs are invaluable in bridging communication gaps, simplifying complex medical information, and translating it for patients, empowering them to make informed decisions. They can also assist in obtaining comprehensive patient histories through natural conversation, reducing provider burden.\n",
            "\n",
            "*   **Clinical Documentation and Administrative Automation:** LLMs automate and streamline numerous administrative tasks, significantly reducing the clerical burden on healthcare professionals. This includes intelligently processing millions of patient notes, extracting key information, tagging content, identifying sentiment, and automating the creation and summarization of patient notes from conversations. They can also manage appointment requests, route calls, provide prescription support, and handle patient correspondence, enhancing efficiency and patient experience. Furthermore, LLMs assist with coding and billing processes, improving revenue cycle management and compliance.\n",
            "\n",
            "*   **Personalized Health Plans and Monitoring:** By analyzing individual patient data, medical literature, and clinical guidelines, LLMs can offer tailored insights for diagnosis, treatment planning, and ongoing monitoring. They enable the creation of personalized preventive care plans by analyzing patient histories and genetic information to identify high-risk individuals for conditions like diabetes, heart disease, and cancer. Integration with IoT devices allows for continuous monitoring of physiological data, aiding in early intervention.\n",
            "\n",
            "*   **Medical Research and Education (Indirect Impact):** While primarily focused on direct patient care, LLMs also indirectly support clinical practice by accelerating medical research and enhancing medical education. They can analyze vast datasets for drug discovery, predict drug interactions and side effects, and expedite clinical trials. In education, LLMs act as virtual patients, personalized tutors, and tools for generating study materials, enhancing learning for future medical professionals and keeping current clinicians updated.\n",
            "\n",
            "*   **Emerging Applications:** Emerging applications include multimodal LLMs, which can process and interpret various inputs, such as radiological images and patient-reported symptoms. This can enhance diagnostic accuracy and clinical decision support by analyzing medical images in conjunction with textual information, for instance, in radiology to detect abnormalities in mammograms or flag issues potentially missed by human specialists.\n",
            "\n",
            "#### Benefits in Medical Research and Drug Discovery\n",
            "\n",
            "The integration of LLMs into medical research and drug discovery processes offers a transformative opportunity, providing numerous benefits and advantages that can significantly accelerate development, reduce costs, and enhance precision, as detailed in findings for Research Question 2.\n",
            "\n",
            "*   **Accelerated Drug Discovery and Development Timelines:** The traditional drug discovery process is lengthy. LLMs can drastically accelerate this by automating initial research stages, such as literature review and compound selection. Generative AI, including LLMs, can reduce timelines from an industry average of 10-15 years to as little as 1-2 years, representing up to a 70% reduction, as demonstrated by platforms cutting early design efforts by 70% and delivering preclinical candidates in 13-18 months.\n",
            "\n",
            "*   **Enhanced Target Identification and Validation:** LLMs analyze extensive biomedical literature, genomic data, and patent information to identify potential drug targets, uncovering complex relationships between genes, diseases, and existing drugs. Specialized LLMs perform comprehensive analyses of protein sequences, including evolutionary conservation, functional annotation, protein folding, and binding site prediction, which are crucial for understanding disease mechanisms and identifying viable targets.\n",
            "\n",
            "*   **Optimized Drug Design and Lead Optimization:** LLMs are invaluable in designing novel drug molecules and optimizing existing lead compounds.\n",
            "    *   **De Novo Molecule Generation:** Trained in molecular languages like SMILES, LLMs can generate new molecules with desired properties and virtually simulate drug interaction with biological systems, allowing for refinement of drug structures.\n",
            "    *   **Lead Optimization:** Combined with multi-objective Bayesian optimization frameworks, LLMs efficiently explore vast chemical spaces to guide the search for novel drug-like molecules that satisfy multiple objectives, a process often bottlenecked by traditional methods. They can identify key binding regions in antibodies and generate improved sequences to increase binding affinity.\n",
            "    *   **Prediction of ADMET Properties:** LLMs can predict Absorption, Distribution, Metabolism, Excretion, and Toxicity (ADMET) attributes of compounds early, helping to distinguish promising candidates from those with negative characteristics and reducing late-stage failures.\n",
            "\n",
            "*   **Accelerated Literature Review and Hypothesis Generation:** LLMs efficiently process and summarize vast amounts of medical research papers, patents, and clinical trial results, identifying new trends and insights. They can extract scientific knowledge that is often arduous to compile. More significantly, LLMs can act as biomedical hypothesis generators, formulating novel and validated hypotheses even when tested on unseen literature, accelerating biomedical discovery.\n",
            "\n",
            "*   **Improved Clinical Trial Design and Patient Stratification:** LLMs can revolutionize clinical trial processes.\n",
            "    *   **Streamlined Design:** They enhance trial design by identifying promising drug candidates and patient demographics and can dynamically adjust trial parameters by predicting outcomes, potentially reducing time and cost.\n",
            "    *   **Patient-Trial Matching:** LLMs are highly effective in analyzing EHRs and clinical protocols to facilitate patient-trial matching, significantly reducing human screening time, improving risk stratification, and providing more accurate predictions compared to conventional techniques.\n",
            "    *   **Adverse Drug Event (ADE) Prediction:** LLMs can improve the early identification and classification of ADEs by automating the analysis of clinical narratives and social media data, with fine-tuned LLMs showing accuracy rates of 85-86% in classifying ADE severity.\n",
            "\n",
            "*   **Cost Reduction:** By accelerating various stages of drug discovery and development, LLMs significantly contribute to reducing overall development costs. They reduce the number of failed experiments, allowing companies to focus resources on the most promising candidates, potentially cutting drug discovery timelines in half and unlocking substantial value in pharma R&D.\n",
            "\n",
            "#### Ethical Considerations and Technical Challenges\n",
            "\n",
            "The development and deployment of LLMs in clinical environments introduce significant ethical considerations and technical challenges that require careful attention to ensure patient safety, data security, and equitable care, as identified in findings for Research Question 3.\n",
            "\n",
            "**Ethical Considerations:**\n",
            "\n",
            "*   **Patient Safety and Misinformation (Hallucinations):** LLMs can generate confidently articulated but incorrect or misleading information (hallucinations). In a clinical context, such misinformation poses a serious risk to patient safety, potentially leading to errors in diagnosis, treatment, or medical advice. Untested systems could cause harm to patients and healthcare workers.\n",
            "*   **Bias and Fairness:** LLMs, trained on vast datasets, can inadvertently perpetuate biases present in real-world data, leading to unequal healthcare outcomes and exacerbating existing disparities related to gender, race, ethnicity, socioeconomic status, or communication style. Research indicates LLMs may recommend reduced medical care for women and vulnerable populations based on irrelevant factors.\n",
            "*   **Privacy and Data Security:** LLMs require massive amounts of sensitive Protected Health Information (PHI) for training, raising concerns about potential memorization and disclosure of sensitive information. Using public LLMs with PHI can violate regulations like HIPAA due to data retraining or human review without consent or a Business Associate Agreement (BAA). Deployment on uncontrolled cloud servers also introduces privacy risks.\n",
            "*   **Transparency and Explainability:** Many LLMs operate as \"black boxes,\" making their decision-making processes challenging to understand. In healthcare, clinicians need to understand *why* an AI recommended a diagnosis or treatment to build trust, ensure accountability, and enable clinical validation. The lack of transparency can hinder acceptance among professionals and patients.\n",
            "*   **Accountability and Responsibility:** Determining liability when an LLM provides an incorrect recommendation or causes harm remains complex and unresolved. This poses legal risks to both clinicians and developers. A critical lack of accountability exists as LLMs have no self-assessment. Establishing clear frameworks for audit, accountability, and safety benchmarking is essential.\n",
            "*   **Impact on Healthcare Professionals:** Concerns exist about potential over-reliance on AI, deskilling of professionals, and changes to the doctor-patient relationship. Clinicians need to maintain human oversight to verify information and complement AI capabilities with professional medical judgment.\n",
            "*   **Informed Consent:** The use of patient data for training LLMs necessitates clear consent processes, ensuring patients are aware of how their data is used and protected.\n",
            "\n",
            "**Technical Challenges:**\n",
            "\n",
            "*   **Data Quality, Availability, and Diversity:** LLMs require large-scale, diverse, and representative datasets, which are often scarce in medical contexts due to privacy laws and information blocking. Lack of diversity exacerbates biases.\n",
            "*   **Model Accuracy, Reliability, and Generalizability:** Despite benchmark scores, LLMs often struggle with clinical reliability, failing to adhere to guidelines, misinterpreting lab results, and exhibiting \"prompt fragility.\" Hallucinations remain a challenge, and generalizability to rare diseases or diverse patient populations is limited without adequate representation in training data.\n",
            "*   **Integration with Existing Systems:** Seamless integration into existing healthcare IT infrastructure, such as EHRs, presents complex challenges related to interoperability, data flow integrity, and workflow adaptation without disrupting clinical practice.\n",
            "*   **Computational Resources and Cost:** Training and running large LLMs require extensive computing capabilities and significant energy consumption, leading to substantial costs that can be a barrier for healthcare institutions.\n",
            "*   **Security and Robustness (Adversarial Attacks):** LLMs are vulnerable to adversarial attacks, where crafted inputs can mislead models to produce inaccurate or harmful outputs, overriding safety guardrails. Fine-tuning attacks and risks of Personal Identifiable Information (PII) leakage in Retrieval Augmented Generation (RAG) systems are also significant threats.\n",
            "*   **Regulatory Compliance and Validation:** The rapid evolution of LLMs outpaces current regulatory frameworks. There is a critical need for clear guidelines, robust validation methods, and standardized assessment tools to evaluate accuracy, safety, and quality before clinical deployment.\n",
            "*   **Continuous Learning and Updates:** Medical knowledge and guidelines constantly evolve, necessitating mechanisms for continuous learning and updates for LLMs without computationally expensive complete retraining.\n",
            "*   **Real-time Performance and Latency:** For critical clinical decision support or real-time patient interactions, LLMs need to provide responses with minimal latency, posing a technical challenge for efficient inference at scale.\n",
            "\n",
            "### Conclusion\n",
            "\n",
            "Large Language Models are poised to significantly impact healthcare, offering transformative capabilities in clinical decision support, patient care, medical research, and drug discovery. Their applications range from enhancing diagnostic accuracy and personalizing patient interactions to accelerating drug development timelines and automating administrative tasks. By analyzing vast datasets, LLMs can provide tailored insights, optimize drug design, and streamline clinical trials, promising faster, more precise, and cost-effective delivery of new treatments.\n",
            "\n",
            "However, the widespread integration of LLMs into healthcare is accompanied by significant ethical considerations and technical challenges. Issues such as patient safety risks from misinformation (hallucinations), algorithmic bias, privacy and data security concerns, lack of transparency, and complexities of accountability demand rigorous attention. Technically, challenges include data quality, model reliability, integration with existing systems, computational costs, security vulnerabilities, and the need for robust regulatory frameworks and continuous learning mechanisms. Addressing these multifaceted challenges through careful development, robust validation, regulatory oversight, and continuous human supervision is paramount to safely and effectively harness the immense promise of LLMs for improving patient outcomes and revolutionizing healthcare.\n",
            "\n",
            "======================================================================\n",
            "--- END OF REPORT ---\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# Example 1: Test with a technology topic\n",
        "topic = \"Large Language Models and their applications in healthcare\"\n",
        "\n",
        "print(\"üöÄ Running Complete Multi-Agent Research System\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Topic: {topic}\\n\")\n",
        "\n",
        "result = run_research_assistant(topic)\n",
        "\n",
        "# Display the results\n",
        "display_report(result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üéì Try Your Own Topic\n",
        "\n",
        "Modify the cell below to research any topic you're interested in:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Both GOOGLE_API_KEY and GEMINI_API_KEY are set. Using GOOGLE_API_KEY.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Running Complete Multi-Agent Research System\n",
            "======================================================================\n",
            "Topic: Attack on Venezuela and its impact on the economy\n",
            "\n",
            "üîß Step 1: Configuring Gemini API...\n",
            "‚úÖ API key configured from environment variable\n",
            "‚úÖ Configuration successful!\n",
            "\n",
            "üìö Starting research process for: 'Attack on Venezuela and its impact on the economy'\n",
            "\n",
            "======================================================================\n",
            "\n",
            "üéØ Phase 1: Planning\n",
            "----------------------------------------------------------------------\n",
            "üìã Planner Agent: Creating a research plan...\n",
            "   Using model: gemini-2.5-flash\n",
            "‚úÖ Plan created with 6 questions:\n",
            "   1. What are the specific international economic sanctions imposed on Venezuela since 2017\n",
            "   2. and which key economic sectors have been most affected?\n",
            "   3. How have international economic sanctions impacted Venezuelas GDP\n",
            "   4. inflation rates\n",
            "   5. and and oil production between 2017 and 2023?\n",
            "   6. What has been the effect of foreign-supported political destabilization efforts on foreign direct investment and capital flight in Venezuela since 2015?\n",
            "\n",
            "üîç Phase 2: Research (6 questions)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "[1/6] üîç Researcher Agent: Researching question: 'What are the specific international economic sanctions imposed on Venezuela since 2017'...\n",
            "   Using model: gemini-2.5-flash\n",
            "   ‚úÖ Information found (7339 characters)\n",
            "\n",
            "[2/6] üîç Researcher Agent: Researching question: 'and which key economic sectors have been most affected?'...\n",
            "   Using model: gemini-2.5-flash\n",
            "   ‚úÖ Information found (285 characters)\n",
            "\n",
            "[3/6] üîç Researcher Agent: Researching question: 'How have international economic sanctions impacted Venezuelas GDP'...\n",
            "   Using model: gemini-2.5-flash\n",
            "   ‚ùå Error in Researcher Agent: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\\nPlease retry in 53.053611875s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '53s'}]}}\n",
            "   ‚ö†Ô∏è  Skipping question due to error\n",
            "\n",
            "[4/6] üîç Researcher Agent: Researching question: 'inflation rates'...\n",
            "   Using model: gemini-2.5-flash\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/var/folders/kn/4r8ws4g95q1dwsv9btm_lc000000gn/T/ipykernel_47542/1775844127.py\", line 56, in search_agent\n",
            "    response = client.models.generate_content(\n",
            "        model=model_name,\n",
            "        contents=prompt,\n",
            "        config=config\n",
            "    )\n",
            "  File \"/Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages/google/genai/models.py\", line 5203, in generate_content\n",
            "    response = self._generate_content(\n",
            "        model=model, contents=contents, config=parsed_config\n",
            "    )\n",
            "  File \"/Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages/google/genai/models.py\", line 3985, in _generate_content\n",
            "    response = self._api_client.request(\n",
            "        'post', path, request_dict, http_options\n",
            "    )\n",
            "  File \"/Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1388, in request\n",
            "    response = self._request(http_request, http_options, stream=False)\n",
            "  File \"/Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1224, in _request\n",
            "    return self._retry(self._request_once, http_request, stream)  # type: ignore[no-any-return]\n",
            "           ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 475, in __call__\n",
            "    do = self.iter(retry_state=retry_state)\n",
            "  File \"/Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 376, in iter\n",
            "    result = action(retry_state)\n",
            "  File \"/Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 418, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "          ~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 185, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "          ~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n",
            "    return self.__get_result()\n",
            "           ~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 478, in __call__\n",
            "    result = fn(*args, **kwargs)\n",
            "  File \"/Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1201, in _request_once\n",
            "    errors.APIError.raise_for_response(response)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^\n",
            "  File \"/Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages/google/genai/errors.py\", line 121, in raise_for_response\n",
            "    cls.raise_error(response.status_code, response_json, response)\n",
            "    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages/google/genai/errors.py\", line 146, in raise_error\n",
            "    raise ClientError(status_code, response_json, response)\n",
            "google.genai.errors.ClientError: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\\nPlease retry in 53.053611875s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '53s'}]}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/var/folders/kn/4r8ws4g95q1dwsv9btm_lc000000gn/T/ipykernel_47542/1775844127.py\", line 56, in search_agent\n",
            "    response = client.models.generate_content(\n",
            "        model=model_name,\n",
            "        contents=prompt,\n",
            "        config=config\n",
            "    )\n",
            "  File \"/Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages/google/genai/models.py\", line 5203, in generate_content\n",
            "    response = self._generate_content(\n",
            "        model=model, contents=contents, config=parsed_config\n",
            "    )\n",
            "  File \"/Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages/google/genai/models.py\", line 3985, in _generate_content\n",
            "    response = self._api_client.request(\n",
            "        'post', path, request_dict, http_options\n",
            "    )\n",
            "  File \"/Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1388, in request\n",
            "    response = self._request(http_request, http_options, stream=False)\n",
            "  File \"/Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1224, in _request\n",
            "    return self._retry(self._request_once, http_request, stream)  # type: ignore[no-any-return]\n",
            "           ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 475, in __call__\n",
            "    do = self.iter(retry_state=retry_state)\n",
            "  File \"/Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 376, in iter\n",
            "    result = action(retry_state)\n",
            "  File \"/Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 418, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "          ~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 185, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "          ~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n",
            "    return self.__get_result()\n",
            "           ~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 478, in __call__\n",
            "    result = fn(*args, **kwargs)\n",
            "  File \"/Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1201, in _request_once\n",
            "    errors.APIError.raise_for_response(response)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^\n",
            "  File \"/Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages/google/genai/errors.py\", line 121, in raise_for_response\n",
            "    cls.raise_error(response.status_code, response_json, response)\n",
            "    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages/google/genai/errors.py\", line 146, in raise_error\n",
            "    raise ClientError(status_code, response_json, response)\n",
            "google.genai.errors.ClientError: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\\nPlease retry in 52.888575242s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'model': 'gemini-2.5-flash', 'location': 'global'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '52s'}]}}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   ‚ùå Error in Researcher Agent: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\\nPlease retry in 52.888575242s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'model': 'gemini-2.5-flash', 'location': 'global'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '52s'}]}}\n",
            "   ‚ö†Ô∏è  Skipping question due to error\n",
            "\n",
            "[5/6] üîç Researcher Agent: Researching question: 'and and oil production between 2017 and 2023?'...\n",
            "   Using model: gemini-2.5-flash\n",
            "   ‚ùå Error in Researcher Agent: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\\nPlease retry in 52.733598507s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'model': 'gemini-2.5-flash', 'location': 'global'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '52s'}]}}\n",
            "   ‚ö†Ô∏è  Skipping question due to error\n",
            "\n",
            "[6/6] üîç Researcher Agent: Researching question: 'What has been the effect of foreign-supported political destabilization efforts on foreign direct investment and capital flight in Venezuela since 2015?'...\n",
            "   Using model: gemini-2.5-flash\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/var/folders/kn/4r8ws4g95q1dwsv9btm_lc000000gn/T/ipykernel_47542/1775844127.py\", line 56, in search_agent\n",
            "    response = client.models.generate_content(\n",
            "        model=model_name,\n",
            "        contents=prompt,\n",
            "        config=config\n",
            "    )\n",
            "  File \"/Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages/google/genai/models.py\", line 5203, in generate_content\n",
            "    response = self._generate_content(\n",
            "        model=model, contents=contents, config=parsed_config\n",
            "    )\n",
            "  File \"/Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages/google/genai/models.py\", line 3985, in _generate_content\n",
            "    response = self._api_client.request(\n",
            "        'post', path, request_dict, http_options\n",
            "    )\n",
            "  File \"/Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1388, in request\n",
            "    response = self._request(http_request, http_options, stream=False)\n",
            "  File \"/Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1224, in _request\n",
            "    return self._retry(self._request_once, http_request, stream)  # type: ignore[no-any-return]\n",
            "           ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 475, in __call__\n",
            "    do = self.iter(retry_state=retry_state)\n",
            "  File \"/Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 376, in iter\n",
            "    result = action(retry_state)\n",
            "  File \"/Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 418, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "          ~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 185, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "          ~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n",
            "    return self.__get_result()\n",
            "           ~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 478, in __call__\n",
            "    result = fn(*args, **kwargs)\n",
            "  File \"/Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1201, in _request_once\n",
            "    errors.APIError.raise_for_response(response)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^\n",
            "  File \"/Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages/google/genai/errors.py\", line 121, in raise_for_response\n",
            "    cls.raise_error(response.status_code, response_json, response)\n",
            "    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages/google/genai/errors.py\", line 146, in raise_error\n",
            "    raise ClientError(status_code, response_json, response)\n",
            "google.genai.errors.ClientError: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\\nPlease retry in 52.733598507s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'model': 'gemini-2.5-flash', 'location': 'global'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '52s'}]}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/var/folders/kn/4r8ws4g95q1dwsv9btm_lc000000gn/T/ipykernel_47542/1775844127.py\", line 56, in search_agent\n",
            "    response = client.models.generate_content(\n",
            "        model=model_name,\n",
            "        contents=prompt,\n",
            "        config=config\n",
            "    )\n",
            "  File \"/Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages/google/genai/models.py\", line 5203, in generate_content\n",
            "    response = self._generate_content(\n",
            "        model=model, contents=contents, config=parsed_config\n",
            "    )\n",
            "  File \"/Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages/google/genai/models.py\", line 3985, in _generate_content\n",
            "    response = self._api_client.request(\n",
            "        'post', path, request_dict, http_options\n",
            "    )\n",
            "  File \"/Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1388, in request\n",
            "    response = self._request(http_request, http_options, stream=False)\n",
            "  File \"/Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1224, in _request\n",
            "    return self._retry(self._request_once, http_request, stream)  # type: ignore[no-any-return]\n",
            "           ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 475, in __call__\n",
            "    do = self.iter(retry_state=retry_state)\n",
            "  File \"/Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 376, in iter\n",
            "    result = action(retry_state)\n",
            "  File \"/Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 418, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "          ~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 185, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "          ~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n",
            "    return self.__get_result()\n",
            "           ~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 478, in __call__\n",
            "    result = fn(*args, **kwargs)\n",
            "  File \"/Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1201, in _request_once\n",
            "    errors.APIError.raise_for_response(response)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^\n",
            "  File \"/Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages/google/genai/errors.py\", line 121, in raise_for_response\n",
            "    cls.raise_error(response.status_code, response_json, response)\n",
            "    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages/google/genai/errors.py\", line 146, in raise_error\n",
            "    raise ClientError(status_code, response_json, response)\n",
            "google.genai.errors.ClientError: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\\nPlease retry in 52.610391309s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '52s'}]}}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   ‚ùå Error in Researcher Agent: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\\nPlease retry in 52.610391309s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '52s'}]}}\n",
            "   ‚ö†Ô∏è  Skipping question due to error\n",
            "\n",
            "‚úçÔ∏è  Phase 3: Synthesis\n",
            "----------------------------------------------------------------------\n",
            "‚úçÔ∏è  Synthesizer Agent: Writing the final report...\n",
            "   Using model: gemini-2.5-flash\n",
            "   ‚ùå Error in Synthesizer Agent: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\\nPlease retry in 52.471717048s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '52s'}]}}\n",
            "\n",
            "‚ùå Could not generate final report\n",
            "\n",
            "‚ùå Research failed: Could not generate final report\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/var/folders/kn/4r8ws4g95q1dwsv9btm_lc000000gn/T/ipykernel_47542/3168451373.py\", line 78, in synthesizer_agent\n",
            "    response = client.models.generate_content(\n",
            "        model=model_name,\n",
            "        contents=prompt\n",
            "    )\n",
            "  File \"/Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages/google/genai/models.py\", line 5203, in generate_content\n",
            "    response = self._generate_content(\n",
            "        model=model, contents=contents, config=parsed_config\n",
            "    )\n",
            "  File \"/Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages/google/genai/models.py\", line 3985, in _generate_content\n",
            "    response = self._api_client.request(\n",
            "        'post', path, request_dict, http_options\n",
            "    )\n",
            "  File \"/Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1388, in request\n",
            "    response = self._request(http_request, http_options, stream=False)\n",
            "  File \"/Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1224, in _request\n",
            "    return self._retry(self._request_once, http_request, stream)  # type: ignore[no-any-return]\n",
            "           ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 475, in __call__\n",
            "    do = self.iter(retry_state=retry_state)\n",
            "  File \"/Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 376, in iter\n",
            "    result = action(retry_state)\n",
            "  File \"/Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 418, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "          ~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 185, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "          ~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n",
            "    return self.__get_result()\n",
            "           ~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 478, in __call__\n",
            "    result = fn(*args, **kwargs)\n",
            "  File \"/Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1201, in _request_once\n",
            "    errors.APIError.raise_for_response(response)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^\n",
            "  File \"/Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages/google/genai/errors.py\", line 121, in raise_for_response\n",
            "    cls.raise_error(response.status_code, response_json, response)\n",
            "    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages/google/genai/errors.py\", line 146, in raise_error\n",
            "    raise ClientError(status_code, response_json, response)\n",
            "google.genai.errors.ClientError: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\\nPlease retry in 52.471717048s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '52s'}]}}\n"
          ]
        }
      ],
      "source": [
        "# üéØ YOUR TURN: Research your own topic!\n",
        "# Change the topic below to anything you want to research\n",
        "\n",
        "your_topic = \"Attack on Venezuela and its impact on the economy\"\n",
        "\n",
        "print(\"üöÄ Running Complete Multi-Agent Research System\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Topic: {your_topic}\\n\")\n",
        "\n",
        "result = run_research_assistant(your_topic)\n",
        "display_report(result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß™ Test Cases and Scenarios\n",
        "\n",
        "Testing is crucial for building reliable systems. Let's create comprehensive test cases to validate our multi-agent system works correctly in various scenarios.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_planner_agent(client, test_cases):\n",
        "    \"\"\"Test the Planner Agent with various topics.\"\"\"\n",
        "    print(\"üß™ Testing Planner Agent\\n\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    results = []\n",
        "    for i, topic in enumerate(test_cases, 1):\n",
        "        print(f\"\\nTest {i}: '{topic}'\")\n",
        "        print(\"-\" * 70)\n",
        "        questions = planner_agent(client, topic)\n",
        "        results.append({\n",
        "            'topic': topic,\n",
        "            'questions': questions,\n",
        "            'success': len(questions) >= 3\n",
        "        })\n",
        "        print(f\"‚úÖ Generated {len(questions)} questions\" if questions else \"‚ùå Failed\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "\n",
        "def test_researcher_agent(client, test_questions):\n",
        "    \"\"\"Test the Researcher Agent with various questions.\"\"\"\n",
        "    print(\"\\nüß™ Testing Researcher Agent\\n\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    results = []\n",
        "    for i, question in enumerate(test_questions, 1):\n",
        "        print(f\"\\nTest {i}: '{question[:60]}...'\")\n",
        "        print(\"-\" * 70)\n",
        "        answer = search_agent(client, question)\n",
        "        results.append({\n",
        "            'question': question,\n",
        "            'answer_length': len(answer),\n",
        "            'success': len(answer) > 100  # At least 100 characters\n",
        "        })\n",
        "        print(f\"‚úÖ Retrieved {len(answer)} characters\" if answer else \"‚ùå Failed\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "\n",
        "def test_synthesizer_agent(client, test_data):\n",
        "    \"\"\"Test the Synthesizer Agent with sample research data.\"\"\"\n",
        "    print(\"\\nüß™ Testing Synthesizer Agent\\n\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    topic = test_data['topic']\n",
        "    research_results = test_data['research_results']\n",
        "    \n",
        "    print(f\"Topic: {topic}\")\n",
        "    print(f\"Research Results: {len(research_results)} items\")\n",
        "    print(\"-\" * 70)\n",
        "    \n",
        "    report = synthesizer_agent(client, topic, research_results)\n",
        "    \n",
        "    result = {\n",
        "        'topic': topic,\n",
        "        'report_length': len(report),\n",
        "        'success': len(report) > 200 and \"Error:\" not in report\n",
        "    }\n",
        "    \n",
        "    print(f\"‚úÖ Generated {len(report)} character report\" if result['success'] else \"‚ùå Failed\")\n",
        "    \n",
        "    return result\n",
        "\n",
        "\n",
        "def run_all_tests():\n",
        "    \"\"\"Run comprehensive test suite.\"\"\"\n",
        "    print(\"üß™ COMPREHENSIVE TEST SUITE\")\n",
        "    print(\"=\" * 70)\n",
        "    print(\"This will test all agents with various scenarios.\\n\")\n",
        "    \n",
        "    try:\n",
        "        client = configure_gemini()\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Cannot run tests: {e}\")\n",
        "        return\n",
        "    \n",
        "    # Test Case 1: Planner Agent - Various Topics\n",
        "    planner_test_cases = [\n",
        "        \"Climate change impacts\",\n",
        "        \"Artificial Intelligence ethics\",\n",
        "        \"Renewable energy technologies\"\n",
        "    ]\n",
        "    planner_results = test_planner_agent(client, planner_test_cases)\n",
        "    \n",
        "    # Test Case 2: Researcher Agent - Various Questions\n",
        "    researcher_test_questions = [\n",
        "        \"What is machine learning?\",\n",
        "        \"How does solar energy work?\",\n",
        "        \"What are the benefits of electric vehicles?\"\n",
        "    ]\n",
        "    researcher_results = test_researcher_agent(client, researcher_test_questions)\n",
        "    \n",
        "    # Test Case 3: Synthesizer Agent - Sample Data\n",
        "    synthesizer_test_data = {\n",
        "        'topic': 'Artificial Intelligence',\n",
        "        'research_results': [\n",
        "            (\"What is AI?\", \"Artificial Intelligence is the simulation of human intelligence...\"),\n",
        "            (\"How is AI used?\", \"AI is used in healthcare, finance, transportation...\"),\n",
        "            (\"What are AI risks?\", \"AI risks include job displacement, bias, privacy concerns...\")\n",
        "        ]\n",
        "    }\n",
        "    synthesizer_result = test_synthesizer_agent(client, synthesizer_test_data)\n",
        "    \n",
        "    # Summary\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"üìä TEST SUMMARY\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"Planner Agent: {sum(r['success'] for r in planner_results)}/{len(planner_results)} passed\")\n",
        "    print(f\"Researcher Agent: {sum(r['success'] for r in researcher_results)}/{len(researcher_results)} passed\")\n",
        "    print(f\"Synthesizer Agent: {'‚úÖ Passed' if synthesizer_result['success'] else '‚ùå Failed'}\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "# Uncomment to run tests (uses API credits)\n",
        "# run_all_tests()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìã Additional Scenarios\n",
        "\n",
        "Let's explore different scenarios to understand how the system behaves in various situations:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scenario 1: Current Events (Requires up-to-date information)\n",
        "print(\"üì∞ Scenario 1: Current Events Research\")\n",
        "print(\"=\" * 70)\n",
        "current_event_topic = \"Latest developments in space exploration in 2024\"\n",
        "result1 = run_research_assistant(current_event_topic)\n",
        "if result1['success']:\n",
        "    print(f\"\\n‚úÖ Successfully researched current events topic\")\n",
        "    print(f\"   Generated {len(result1['plan'])} research questions\")\n",
        "    print(f\"   Collected {len(result1['research_results'])} research results\")\n",
        "    print(f\"   Final report: {len(result1['report'])} characters\")\n",
        "else:\n",
        "    print(f\"\\n‚ùå Failed: {result1.get('error', 'Unknown error')}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scenario 2: Technical/Complex Topic\n",
        "print(\"\\nüî¨ Scenario 2: Technical/Complex Topic\")\n",
        "print(\"=\" * 70)\n",
        "technical_topic = \"Quantum computing algorithms and their applications in cryptography\"\n",
        "result2 = run_research_assistant(technical_topic)\n",
        "if result2['success']:\n",
        "    print(f\"\\n‚úÖ Successfully researched technical topic\")\n",
        "    print(f\"   Generated {len(result2['plan'])} research questions\")\n",
        "    print(f\"   Collected {len(result2['research_results'])} research results\")\n",
        "    print(f\"   Final report: {len(result2['report'])} characters\")\n",
        "else:\n",
        "    print(f\"\\n‚ùå Failed: {result2.get('error', 'Unknown error')}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scenario 3: Business/Economic Topic\n",
        "print(\"\\nüíº Scenario 3: Business/Economic Topic\")\n",
        "print(\"=\" * 70)\n",
        "business_topic = \"Impact of remote work on urban real estate markets\"\n",
        "result3 = run_research_assistant(business_topic)\n",
        "if result3['success']:\n",
        "    print(f\"\\n‚úÖ Successfully researched business topic\")\n",
        "    print(f\"   Generated {len(result3['plan'])} research questions\")\n",
        "    print(f\"   Collected {len(result3['research_results'])} research results\")\n",
        "    print(f\"   Final report: {len(result3['report'])} characters\")\n",
        "else:\n",
        "    print(f\"\\n‚ùå Failed: {result3.get('error', 'Unknown error')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üèóÔ∏è Full-Stack Solution Architecture Plan\n",
        "\n",
        "Now that you understand how multi-agent systems work, let's design a **production-ready, full-stack solution** for a real-world application.\n",
        "\n",
        "### üéØ Real-World Example: AI-Powered Research Platform\n",
        "\n",
        "Imagine building a platform where users can:\n",
        "- Submit research topics\n",
        "- Get comprehensive research reports\n",
        "- Save and manage their research history\n",
        "- Share reports with teams\n",
        "- Export reports in various formats (PDF, DOCX, Markdown)\n",
        "\n",
        "### üìê System Architecture\n",
        "\n",
        "```\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ                        Frontend Layer                        ‚îÇ\n",
        "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ\n",
        "‚îÇ  ‚îÇ   Web    ‚îÇ  ‚îÇ  Mobile  ‚îÇ  ‚îÇ  Desktop ‚îÇ  ‚îÇ   API    ‚îÇ  ‚îÇ\n",
        "‚îÇ  ‚îÇ   App    ‚îÇ  ‚îÇ   App    ‚îÇ  ‚îÇ   App    ‚îÇ  ‚îÇ  Client   ‚îÇ  ‚îÇ\n",
        "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "        ‚îÇ              ‚îÇ              ‚îÇ              ‚îÇ\n",
        "        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                       ‚îÇ\n",
        "        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "        ‚îÇ      API Gateway            ‚îÇ\n",
        "        ‚îÇ  (Authentication, Rate      ‚îÇ\n",
        "        ‚îÇ   Limiting, Load Balancing) ‚îÇ\n",
        "        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                       ‚îÇ\n",
        "        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "        ‚îÇ    Backend Services         ‚îÇ\n",
        "        ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ\n",
        "        ‚îÇ  ‚îÇ  Research Service      ‚îÇ ‚îÇ ‚Üê Our Multi-Agent System\n",
        "        ‚îÇ  ‚îÇ  - Planner Agent       ‚îÇ ‚îÇ\n",
        "        ‚îÇ  ‚îÇ  - Researcher Agent    ‚îÇ ‚îÇ\n",
        "        ‚îÇ  ‚îÇ  - Synthesizer Agent   ‚îÇ ‚îÇ\n",
        "        ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ\n",
        "        ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ\n",
        "        ‚îÇ  ‚îÇ  User Service         ‚îÇ ‚îÇ\n",
        "        ‚îÇ  ‚îÇ  - Authentication     ‚îÇ ‚îÇ\n",
        "        ‚îÇ  ‚îÇ  - Profile Management‚îÇ ‚îÇ\n",
        "        ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ\n",
        "        ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ\n",
        "        ‚îÇ  ‚îÇ  Report Service       ‚îÇ ‚îÇ\n",
        "        ‚îÇ  ‚îÇ  - Storage            ‚îÇ ‚îÇ\n",
        "        ‚îÇ  ‚îÇ  - Export             ‚îÇ ‚îÇ\n",
        "        ‚îÇ  ‚îÇ  - Sharing            ‚îÇ ‚îÇ\n",
        "        ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ\n",
        "        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                       ‚îÇ\n",
        "        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "        ‚îÇ    Data Layer               ‚îÇ\n",
        "        ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ\n",
        "        ‚îÇ  ‚îÇPostgreSQL‚îÇ  ‚îÇ  Redis   ‚îÇ‚îÇ\n",
        "        ‚îÇ  ‚îÇ(Users,   ‚îÇ  ‚îÇ(Cache,   ‚îÇ‚îÇ\n",
        "        ‚îÇ  ‚îÇ Reports) ‚îÇ  ‚îÇ  Queue)  ‚îÇ‚îÇ\n",
        "        ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ\n",
        "        ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ\n",
        "        ‚îÇ  ‚îÇ  S3/     ‚îÇ  ‚îÇ  Vector  ‚îÇ‚îÇ\n",
        "        ‚îÇ  ‚îÇ  GCS    ‚îÇ  ‚îÇ   DB     ‚îÇ‚îÇ\n",
        "        ‚îÇ  ‚îÇ(Files)  ‚îÇ  ‚îÇ(Search)  ‚îÇ‚îÇ\n",
        "        ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ\n",
        "        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                       ‚îÇ\n",
        "        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "        ‚îÇ    External Services        ‚îÇ\n",
        "        ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ\n",
        "        ‚îÇ  ‚îÇ  Gemini  ‚îÇ  ‚îÇ  Google  ‚îÇ‚îÇ\n",
        "        ‚îÇ  ‚îÇ   API    ‚îÇ  ‚îÇ  Search  ‚îÇ‚îÇ\n",
        "        ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ\n",
        "        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n",
        "\n",
        "### üõ†Ô∏è Technology Stack Recommendations\n",
        "\n",
        "#### Frontend\n",
        "- **Web**: React/Next.js or Vue.js\n",
        "- **Mobile**: React Native or Flutter\n",
        "- **Desktop**: Electron (for cross-platform)\n",
        "\n",
        "#### Backend\n",
        "- **API Framework**: FastAPI (Python) or Express.js (Node.js)\n",
        "- **Task Queue**: Celery (Python) or Bull (Node.js) for async processing\n",
        "- **WebSockets**: For real-time progress updates\n",
        "\n",
        "#### Database\n",
        "- **Primary DB**: PostgreSQL (users, reports, metadata)\n",
        "- **Cache**: Redis (session, rate limiting, queue)\n",
        "- **Vector DB**: Pinecone or Weaviate (for semantic search of reports)\n",
        "\n",
        "#### Infrastructure\n",
        "- **Cloud**: AWS, GCP, or Azure\n",
        "- **Containerization**: Docker + Kubernetes\n",
        "- **CI/CD**: GitHub Actions or GitLab CI\n",
        "- **Monitoring**: Prometheus + Grafana\n",
        "\n",
        "### üìù Implementation Phases\n",
        "\n",
        "#### Phase 1: Core Backend (Weeks 1-2)\n",
        "1. Set up FastAPI project structure\n",
        "2. Implement authentication (JWT tokens)\n",
        "3. Port multi-agent system to async service\n",
        "4. Create database models (User, Report, ResearchTask)\n",
        "5. Implement basic API endpoints\n",
        "\n",
        "#### Phase 2: Research Service Enhancement (Weeks 3-4)\n",
        "1. Add task queue for async research processing\n",
        "2. Implement progress tracking (WebSockets)\n",
        "3. Add research result caching\n",
        "4. Implement rate limiting per user\n",
        "5. Add error handling and retry logic\n",
        "\n",
        "#### Phase 3: Frontend Development (Weeks 5-6)\n",
        "1. Create React/Next.js frontend\n",
        "2. Implement authentication UI\n",
        "3. Build research submission form\n",
        "4. Create real-time progress dashboard\n",
        "5. Design report display and export features\n",
        "\n",
        "#### Phase 4: Advanced Features (Weeks 7-8)\n",
        "1. Add report templates and customization\n",
        "2. Implement report sharing and collaboration\n",
        "3. Add user dashboard with research history\n",
        "4. Implement search across saved reports\n",
        "5. Add export to PDF/DOCX/Markdown\n",
        "\n",
        "#### Phase 5: Production Readiness (Weeks 9-10)\n",
        "1. Add comprehensive logging and monitoring\n",
        "2. Implement automated testing (unit, integration, E2E)\n",
        "3. Set up CI/CD pipeline\n",
        "4. Performance optimization and load testing\n",
        "5. Security audit and hardening\n",
        "6. Documentation and deployment guides\n",
        "\n",
        "### üîê Security Considerations\n",
        "\n",
        "1. **API Key Management**\n",
        "   - Store Gemini API keys in secure vault (AWS Secrets Manager, HashiCorp Vault)\n",
        "   - Rotate keys regularly\n",
        "   - Use different keys for dev/staging/prod\n",
        "\n",
        "2. **Authentication & Authorization**\n",
        "   - JWT tokens with refresh tokens\n",
        "   - Role-based access control (RBAC)\n",
        "   - Rate limiting per user tier\n",
        "\n",
        "3. **Data Protection**\n",
        "   - Encrypt sensitive data at rest\n",
        "   - Use HTTPS for all communications\n",
        "   - Implement input validation and sanitization\n",
        "   - Protect against SQL injection, XSS attacks\n",
        "\n",
        "4. **API Security**\n",
        "   - API rate limiting\n",
        "   - Request validation\n",
        "   - CORS configuration\n",
        "   - API versioning\n",
        "\n",
        "### üìä Scalability Considerations\n",
        "\n",
        "1. **Horizontal Scaling**\n",
        "   - Stateless API services (can scale horizontally)\n",
        "   - Load balancer for distribution\n",
        "   - Database connection pooling\n",
        "\n",
        "2. **Caching Strategy**\n",
        "   - Cache research results for common topics\n",
        "   - Cache user sessions\n",
        "   - Cache frequently accessed reports\n",
        "\n",
        "3. **Async Processing**\n",
        "   - Use task queues for long-running research tasks\n",
        "   - Implement job prioritization\n",
        "   - Add retry mechanisms with exponential backoff\n",
        "\n",
        "4. **Database Optimization**\n",
        "   - Index frequently queried fields\n",
        "   - Implement database replication for read scaling\n",
        "   - Use connection pooling\n",
        "\n",
        "### üí∞ Cost Optimization\n",
        "\n",
        "1. **API Usage**\n",
        "   - Cache research results to avoid duplicate API calls\n",
        "   - Implement smart rate limiting\n",
        "   - Use cheaper models for simple tasks\n",
        "\n",
        "2. **Infrastructure**\n",
        "   - Use auto-scaling to match demand\n",
        "   - Implement cost monitoring and alerts\n",
        "   - Use reserved instances for predictable workloads\n",
        "\n",
        "3. **Storage**\n",
        "   - Implement data retention policies\n",
        "   - Compress old reports\n",
        "   - Use tiered storage (hot/cold)\n",
        "\n",
        "### üß™ Testing Strategy\n",
        "\n",
        "1. **Unit Tests**: Test each agent independently\n",
        "2. **Integration Tests**: Test agent orchestration\n",
        "3. **E2E Tests**: Test complete user workflows\n",
        "4. **Load Tests**: Test system under high load\n",
        "5. **Security Tests**: Penetration testing, vulnerability scanning\n",
        "\n",
        "### üìà Monitoring & Observability\n",
        "\n",
        "1. **Metrics**\n",
        "   - API response times\n",
        "   - Research task completion rates\n",
        "   - Error rates by agent\n",
        "   - API usage and costs\n",
        "\n",
        "2. **Logging**\n",
        "   - Structured logging (JSON format)\n",
        "   - Log aggregation (ELK stack or similar)\n",
        "   - Error tracking (Sentry)\n",
        "\n",
        "3. **Alerting**\n",
        "   - API errors above threshold\n",
        "   - High latency alerts\n",
        "   - Cost overruns\n",
        "   - System downtime\n",
        "\n",
        "### üöÄ Deployment Strategy\n",
        "\n",
        "1. **Environments**\n",
        "   - Development (local)\n",
        "   - Staging (mirrors production)\n",
        "   - Production\n",
        "\n",
        "2. **Deployment Method**\n",
        "   - Blue-green deployment for zero downtime\n",
        "   - Canary releases for gradual rollout\n",
        "   - Feature flags for A/B testing\n",
        "\n",
        "3. **Disaster Recovery**\n",
        "   - Regular database backups\n",
        "   - Multi-region deployment\n",
        "   - Automated failover\n",
        "\n",
        "---\n",
        "\n",
        "## üéì Next Steps for Learning\n",
        "\n",
        "1. **Extend the Current System**\n",
        "   - Add more agent types (Fact Checker, Citation Manager)\n",
        "   - Implement parallel research (multiple questions simultaneously)\n",
        "   - Add agent memory/context sharing\n",
        "\n",
        "2. **Learn Advanced Concepts**\n",
        "   - LangGraph for complex agent workflows\n",
        "   - Agent memory and state management\n",
        "   - Tool calling and function execution\n",
        "   - Multi-modal agents (text + images)\n",
        "\n",
        "3. **Build Your Own Project**\n",
        "   - Choose a domain (legal research, medical literature, etc.)\n",
        "   - Design your agent architecture\n",
        "   - Implement and iterate\n",
        "   - Deploy to production\n",
        "\n",
        "4. **Join the Community**\n",
        "   - Contribute to open-source multi-agent projects\n",
        "   - Share your learnings\n",
        "   - Participate in AI agent competitions\n",
        "\n",
        "---\n",
        "\n",
        "## üìö Additional Resources\n",
        "\n",
        "- [Google Gemini API Documentation](https://ai.google.dev/docs)\n",
        "- [Multi-Agent Systems Research](https://arxiv.org/search/?query=multi-agent+systems)\n",
        "- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)\n",
        "- [FastAPI Documentation](https://fastapi.tiangolo.com/)\n",
        "- [System Design Primer](https://github.com/donnemartin/system-design-primer)\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Summary\n",
        "\n",
        "In this notebook, you've learned:\n",
        "\n",
        "1. ‚úÖ **API Key Integration**: Secure configuration for Colab and local environments\n",
        "2. ‚úÖ **Multi-Agent Architecture**: How to design specialized agents\n",
        "3. ‚úÖ **Agent Orchestration**: Coordinating multiple agents in a workflow\n",
        "4. ‚úÖ **Prompt Engineering**: Crafting effective prompts for specific roles\n",
        "5. ‚úÖ **Tool Integration**: Using external tools (Google Search) with LLMs\n",
        "6. ‚úÖ **Error Handling**: Building robust, production-ready systems\n",
        "7. ‚úÖ **Testing**: Creating comprehensive test cases\n",
        "8. ‚úÖ **Full-Stack Design**: Planning a complete production system\n",
        "\n",
        "**Congratulations!** You now have the knowledge to build sophisticated multi-agent systems. The next step is to build your own project and experiment with different architectures!\n",
        "\n",
        "---\n",
        "\n",
        "*Happy Learning! üöÄ*\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "AI Agents",
      "language": "python",
      "name": "ai-agents"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
