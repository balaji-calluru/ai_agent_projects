{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Building an AI Research Agent for Image Analysis\n",
        "\n",
        "## Overview\n",
        "\n",
        "This comprehensive guide will teach you how to build an AI research agent capable of conducting in-depth research based on image analysis. Using the **[Granite 3.2 Vision Model](https://github.com/ibm-granite-community)** alongside the **[Granite 3.2 8B Language Model](https://github.com/ibm-granite-community)** with enhanced reasoning capabilities, you'll learn to create an advanced image researcher that runs entirely locally using [Ollama](https://ollama.ai), [Open WebUI](https://github.com/open-webui/open-webui), and [Granite](https://github.com/ibm-granite-community).\n",
        "\n",
        "### What You'll Learn\n",
        "\n",
        "- Understanding Vision Models and their capabilities\n",
        "- Building multi-agent systems with [CrewAI](https://docs.crewai.com)\n",
        "- Implementing RAG (Retrieval-Augmented Generation) for research\n",
        "- Orchestrating parallel, asynchronous research tasks\n",
        "- Processing images and extracting researchable concepts\n",
        "- Synthesizing findings into comprehensive reports\n",
        "- Handling edge cases and optimizing agent performance\n",
        "- Troubleshooting common issues in agentic AI systems\n",
        "\n",
        "### What is Agentic AI for Image Analysis?\n",
        "\n",
        "**Agentic AI for Image Analysis** refers to AI systems that can:\n",
        "- **Perceive**: Analyze images using vision models to extract visual information\n",
        "- **Plan**: Identify researchable concepts and create structured research plans\n",
        "- **Research**: Conduct parallel research using RAG to gather information from web and documents\n",
        "- **Synthesize**: Combine findings into comprehensive, well-cited reports\n",
        "- **Learn**: Improve research quality through better prompt engineering and tool usage\n",
        "\n",
        "### Use Cases\n",
        "\n",
        "- **Architecture Diagrams**: Understand components, protocols, and system relationships\n",
        "- **Business Dashboards**: Explain KPIs, metrics, and trends in BI tools\n",
        "- **Artwork and Historical Photos**: Analyze artistic styles, historical context, and related works\n",
        "- **Scientific Visualizations**: Interpret complex charts, lab results, or datasets\n",
        "- **Medical Imaging**: Research conditions, treatments, and related medical information\n",
        "- **Educational Content**: Extract and research concepts from educational diagrams\n",
        "\n",
        "---\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "1. [Prerequisites](#prerequisites)\n",
        "2. [Understanding Key Concepts](#understanding-key-concepts)\n",
        "3. [Environment Setup](#environment-setup)\n",
        "4. [Project Setup](#project-setup)\n",
        "5. [Building the Vision Model Integration](#building-the-vision-model-integration)\n",
        "6. [Creating the Research Item Identifier Agent](#creating-the-research-item-identifier-agent)\n",
        "7. [Building the Research Crew](#building-the-research-crew)\n",
        "8. [Integrating RAG Tools](#integrating-rag-tools)\n",
        "9. [Putting It All Together](#putting-it-all-together)\n",
        "10. [Advanced Techniques](#advanced-techniques)\n",
        "11. [Edge Cases and Error Handling](#edge-cases-and-error-handling)\n",
        "12. [Troubleshooting Guide](#troubleshooting-guide)\n",
        "13. [Exercises](#exercises)\n",
        "14. [Summary](#summary)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n",
        "\n",
        "Before we begin, make sure you have:\n",
        "\n",
        "1. **[Python 3.8+](https://www.python.org/downloads/)** installed on your system\n",
        "2. **Basic knowledge** of [Python](https://www.python.org), object-oriented programming, and APIs\n",
        "3. **Understanding** of LLMs and vision models (helpful but not required)\n",
        "4. **[Ollama](https://ollama.ai)** installed and running locally\n",
        "5. **Required packages** (we'll install these in the next section)\n",
        "\n",
        "### Required Python Packages\n",
        "\n",
        "- [`crewai`](https://github.com/joaomdmoura/crewAI) - Multi-agent orchestration framework\n",
        "- [`crewai-tools`](https://github.com/joaomdmoura/crewai-tools) - Tools for CrewAI agents (web search, file operations, etc.)\n",
        "- [`litellm`](https://docs.litellm.ai) - LLM provider abstraction (supports Ollama, OpenAI, etc.)\n",
        "- [`pydantic`](https://docs.pydantic.dev) - Data validation using Python type annotations\n",
        "- `base64` - Image encoding/decoding (built-in Python module)\n",
        "- [`PIL` (Pillow)](https://pillow.readthedocs.io) - Image processing\n",
        "- [`requests`](https://requests.readthedocs.io) - HTTP requests for API calls\n",
        "\n",
        "### System Requirements\n",
        "\n",
        "- **CPU**: Any modern CPU will work\n",
        "- **GPU**: Optional but recommended for faster inference (CUDA-compatible GPU or Apple Silicon)\n",
        "- **RAM**: At least 8GB recommended (16GB+ for better performance)\n",
        "- **Storage**: ~10GB for models and dependencies\n",
        "- **[Ollama](https://ollama.ai)**: Must be installed and running on localhost:11434\n",
        "\n",
        "### Installing Ollama\n",
        "\n",
        "If you haven't installed Ollama yet:\n",
        "\n",
        "1. **macOS/Linux**: Visit [ollama.ai](https://ollama.ai) and download the installer\n",
        "2. **Windows**: Download from [ollama.ai](https://ollama.ai)\n",
        "3. **After installation**, pull the required models:\n",
        "   ```bash\n",
        "   ollama pull granite3.2:8b-instruct-q8_0\n",
        "   ollama pull granite3.2-vision:2b\n",
        "   ```\n",
        "\n",
        "### Verifying Ollama Installation\n",
        "\n",
        "Run this command to verify Ollama is working:\n",
        "```bash\n",
        "ollama list\n",
        "```\n",
        "\n",
        "You should see your models listed. If not, pull them using the commands above.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages (uncomment if needed)\n",
        "# %pip install crewai crewai-tools litellm pydantic pillow requests\n",
        "\n",
        "# Verify installation\n",
        "try:\n",
        "    import crewai\n",
        "    import crewai_tools\n",
        "    import litellm\n",
        "    import pydantic\n",
        "    from PIL import Image\n",
        "    import base64\n",
        "    import requests\n",
        "    print(\"âœ… All required packages are installed!\")\n",
        "    print(f\"CrewAI version: {crewai.__version__ if hasattr(crewai, '__version__') else 'installed'}\")\n",
        "except ImportError as e:\n",
        "    print(f\"âŒ Missing package: {e}\")\n",
        "    print(\"Please install missing packages using: pip install crewai crewai-tools litellm pydantic pillow requests\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding Key Concepts\n",
        "\n",
        "Before diving into the implementation, let's understand the fundamental concepts:\n",
        "\n",
        "### 1. Vision Models\n",
        "\n",
        "**Vision Models** are AI models that can understand and describe images. They combine:\n",
        "- **Image Encoders**: Convert images into numerical representations\n",
        "- **Language Models**: Generate text descriptions from those representations\n",
        "\n",
        "**Key Capabilities:**\n",
        "- Object detection and recognition\n",
        "- Scene understanding\n",
        "- Text extraction from images (OCR)\n",
        "- Relationship understanding between objects\n",
        "\n",
        "**[Granite 3.2 Vision Model](https://github.com/ibm-granite-community):**\n",
        "- 2B parameter model optimized for vision tasks\n",
        "- Runs efficiently on local hardware\n",
        "- Supports various image formats (PNG, JPEG, etc.)\n",
        "- Can describe images in detail for educational/research purposes\n",
        "\n",
        "### 2. Agentic AI with CrewAI\n",
        "\n",
        "**[CrewAI](https://docs.crewai.com)** is a framework for orchestrating multiple AI agents to work together:\n",
        "\n",
        "**Key Components:**\n",
        "- **Agents**: Individual AI workers with specific roles and goals\n",
        "- **Tasks**: Specific jobs assigned to agents\n",
        "- **Crews**: Teams of agents working together\n",
        "- **Tools**: External capabilities agents can use (web search, file operations, etc.)\n",
        "\n",
        "**Process Types:**\n",
        "- **Sequential**: Tasks run one after another\n",
        "- **Hierarchical**: Tasks organized in a hierarchy\n",
        "- **Parallel**: Multiple tasks run simultaneously (what we'll use for research)\n",
        "\n",
        "### 3. Retrieval-Augmented Generation (RAG)\n",
        "\n",
        "**RAG** combines:\n",
        "- **Retrieval**: Finding relevant information from sources (web, documents, databases)\n",
        "- **Augmentation**: Adding retrieved context to prompts\n",
        "- **Generation**: Using LLMs to generate responses based on augmented context\n",
        "\n",
        "**Why RAG?**\n",
        "- âœ… Provides up-to-date information (not limited by training cutoff)\n",
        "- âœ… Grounds responses in verifiable sources\n",
        "- âœ… Reduces hallucinations\n",
        "- âœ… Enables domain-specific knowledge\n",
        "\n",
        "### 4. Parallel Research Architecture\n",
        "\n",
        "Our system uses a **two-stage parallel architecture**:\n",
        "\n",
        "1. **Stage 1 - Identification**: Single agent analyzes image and identifies research topics\n",
        "2. **Stage 2 - Research**: Multiple agents research topics in parallel\n",
        "\n",
        "**Benefits:**\n",
        "- âš¡ Faster research (parallel execution)\n",
        "- ðŸŽ¯ Focused research (each agent handles one topic)\n",
        "- ðŸ“Š Comprehensive coverage (all topics researched simultaneously)\n",
        "\n",
        "### 5. Pydantic Models for Structured Output\n",
        "\n",
        "**[Pydantic](https://docs.pydantic.dev)** provides data validation and structured output:\n",
        "\n",
        "- Ensures LLM outputs match expected format\n",
        "- Enables type-safe data handling\n",
        "- Simplifies integration between agents\n",
        "\n",
        "**In our system:**\n",
        "- `ResearchItem`: Represents a single research topic\n",
        "- `ResearchItems`: Collection of research items\n",
        "- Used to pass structured data between agents\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Environment Setup\n",
        "\n",
        "### Step 1: Verify Ollama is Running\n",
        "\n",
        "First, let's verify that Ollama is running and accessible:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "# Check if Ollama is running\n",
        "OLLAMA_BASE_URL = \"http://localhost:11434\"\n",
        "\n",
        "def check_ollama_connection():\n",
        "    \"\"\"Verify Ollama is running and accessible.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(f\"{OLLAMA_BASE_URL}/api/tags\", timeout=5)\n",
        "        if response.status_code == 200:\n",
        "            models = response.json().get(\"models\", [])\n",
        "            print(\"âœ… Ollama is running!\")\n",
        "            print(f\"\\nAvailable models ({len(models)}):\")\n",
        "            for model in models[:5]:  # Show first 5 models\n",
        "                print(f\"  - {model.get('name', 'Unknown')}\")\n",
        "            if len(models) > 5:\n",
        "                print(f\"  ... and {len(models) - 5} more\")\n",
        "            \n",
        "            # Check for required models\n",
        "            required_models = [\"granite3.2:8b-instruct-q8_0\", \"granite3.2-vision:2b\"]\n",
        "            available_model_names = [m.get('name', '') for m in models]\n",
        "            \n",
        "            print(\"\\nðŸ“‹ Required models:\")\n",
        "            for model_name in required_models:\n",
        "                if any(model_name in name for name in available_model_names):\n",
        "                    print(f\"  âœ… {model_name} - Available\")\n",
        "                else:\n",
        "                    print(f\"  âŒ {model_name} - Not found (run: ollama pull {model_name})\")\n",
        "            \n",
        "            return True\n",
        "        else:\n",
        "            print(f\"âŒ Ollama returned status code: {response.status_code}\")\n",
        "            return False\n",
        "    except requests.exceptions.ConnectionError:\n",
        "        print(\"âŒ Cannot connect to Ollama. Please ensure:\")\n",
        "        print(\"  1. Ollama is installed\")\n",
        "        print(\"  2. Ollama is running (run 'ollama serve' in terminal)\")\n",
        "        print(\"  3. Ollama is accessible at http://localhost:11434\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error checking Ollama: {e}\")\n",
        "        return False\n",
        "\n",
        "# Check connection\n",
        "ollama_ready = check_ollama_connection()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Project Setup\n",
        "\n",
        "Now let's set up our project by importing all necessary libraries and configuring our LLM connections.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "from crewai import Agent, Task, Crew, Process, LLM\n",
        "from crewai_tools import WebsiteSearchTool, FileReadTool, DirectoryReadTool\n",
        "from pydantic import BaseModel\n",
        "from typing import List\n",
        "import base64\n",
        "from PIL import Image\n",
        "import io\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"âœ… All libraries imported successfully!\")\n",
        "\n",
        "# Display versions\n",
        "try:\n",
        "    import crewai\n",
        "    print(f\"CrewAI: {crewai.__version__ if hasattr(crewai, '__version__') else 'installed'}\")\n",
        "except:\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Set Up Connections to the Two LLMs\n",
        "\n",
        "We need two different LLMs for our system:\n",
        "\n",
        "1. **[Granite Vision LLM](https://github.com/ibm-granite-community)** (`granite3.2-vision:2b`): Transforms images into descriptive text\n",
        "2. **[Granite 3.2 Instruct](https://github.com/ibm-granite-community)** (`granite3.2:8b-instruct-q8_0`): Handles all language tasks including planning, reasoning, and synthesis\n",
        "\n",
        "**Why q8_0 quantization?**\n",
        "- Better tool calling reliability than q4_K_M\n",
        "- Still lightweight enough to run locally\n",
        "- Good balance between quality and performance\n",
        "\n",
        "**Temperature = 0:**\n",
        "- Ensures deterministic, consistent outputs\n",
        "- Important for structured data extraction\n",
        "- Reduces variability in research quality\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure LLM connections\n",
        "OLLAMA_BASE_URL = \"http://localhost:11434\"\n",
        "OLLAMA_API_KEY = \"ollama\"  # Ollama doesn't require a real API key, but CrewAI expects one\n",
        "\n",
        "# Main LLM for language tasks (planning, reasoning, synthesis)\n",
        "llm = LLM(\n",
        "    model=\"granite3.2:8b-instruct-q8_0\",\n",
        "    base_url=OLLAMA_BASE_URL,\n",
        "    api_key=OLLAMA_API_KEY,\n",
        "    temperature=0  # Deterministic outputs for structured data\n",
        ")\n",
        "\n",
        "# Vision LLM for image analysis\n",
        "vision_llm = LLM(\n",
        "    model=\"granite3.2-vision:2b\",\n",
        "    base_url=OLLAMA_BASE_URL,\n",
        "    api_key=OLLAMA_API_KEY,\n",
        "    temperature=0  # Consistent image descriptions\n",
        ")\n",
        "\n",
        "print(\"âœ… LLM connections configured!\")\n",
        "print(f\"Main LLM: granite3.2:8b-instruct-q8_0\")\n",
        "print(f\"Vision LLM: granite3.2-vision:2b\")\n",
        "print(f\"Base URL: {OLLAMA_BASE_URL}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building the Vision Model Integration\n",
        "\n",
        "### Image Processing Functions\n",
        "\n",
        "Before we can analyze images, we need functions to:\n",
        "1. Load images from files\n",
        "2. Convert images to base64 format (required for vision models)\n",
        "3. Call the vision model to get descriptions\n",
        "\n",
        "Let's build these utilities:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def image_to_base64(image_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Convert an image file to base64 encoded string.\n",
        "    \n",
        "    Args:\n",
        "        image_path: Path to the image file\n",
        "        \n",
        "    Returns:\n",
        "        Base64 encoded string with data URI prefix\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(image_path, \"rb\") as image_file:\n",
        "            encoded_string = base64.b64encode(image_file.read()).decode('utf-8')\n",
        "            # Determine file extension\n",
        "            ext = os.path.splitext(image_path)[1].lower()\n",
        "            mime_type = {\n",
        "                '.png': 'image/png',\n",
        "                '.jpg': 'image/jpeg',\n",
        "                '.jpeg': 'image/jpeg',\n",
        "                '.gif': 'image/gif',\n",
        "                '.webp': 'image/webp'\n",
        "            }.get(ext, 'image/png')\n",
        "            \n",
        "            return f\"data:{mime_type};base64,{encoded_string}\"\n",
        "    except FileNotFoundError:\n",
        "        raise FileNotFoundError(f\"Image file not found: {image_path}\")\n",
        "    except Exception as e:\n",
        "        raise Exception(f\"Error encoding image: {e}\")\n",
        "\n",
        "\n",
        "def describe_image(image_path: str, query: str = \"Describe the image in detail for educational purposes.\") -> str:\n",
        "    \"\"\"\n",
        "    Use the vision model to describe an image.\n",
        "    \n",
        "    Args:\n",
        "        image_path: Path to the image file\n",
        "        query: The prompt/question for the vision model\n",
        "        \n",
        "    Returns:\n",
        "        Detailed description of the image\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Convert image to base64\n",
        "        image_data = image_to_base64(image_path)\n",
        "        \n",
        "        # Prepare the message for the vision model\n",
        "        # Note: The exact format depends on how CrewAI's LLM handles vision inputs\n",
        "        # This is a simplified version - you may need to adjust based on your CrewAI version\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"text\", \"text\": query},\n",
        "                    {\"type\": \"image_url\", \"image_url\": {\"url\": image_data}}\n",
        "                ]\n",
        "            }\n",
        "        ]\n",
        "        \n",
        "        # Call the vision model\n",
        "        # Note: This is a simplified call - actual implementation may vary\n",
        "        # You might need to use litellm directly or adjust based on CrewAI's API\n",
        "        response = vision_llm.call(messages)\n",
        "        \n",
        "        return response\n",
        "    except Exception as e:\n",
        "        raise Exception(f\"Error describing image: {e}\")\n",
        "\n",
        "\n",
        "# Test the functions (uncomment when you have an image)\n",
        "# test_image_path = \"path/to/your/test/image.png\"\n",
        "# description = describe_image(test_image_path)\n",
        "# print(description)\n",
        "\n",
        "print(\"âœ… Image processing functions defined!\")\n",
        "print(\"\\nFunctions available:\")\n",
        "print(\"  - image_to_base64(image_path): Convert image to base64\")\n",
        "print(\"  - describe_image(image_path, query): Get image description from vision model\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def describe_image_ollama_direct(image_path: str, query: str = \"Describe the image in detail for educational purposes.\") -> str:\n",
        "    \"\"\"\n",
        "    Alternative: Use Ollama API directly for vision model calls.\n",
        "    This is more reliable if CrewAI's LLM wrapper doesn't support vision well.\n",
        "    \n",
        "    Args:\n",
        "        image_path: Path to the image file\n",
        "        query: The prompt/question for the vision model\n",
        "        \n",
        "    Returns:\n",
        "        Detailed description of the image\n",
        "    \"\"\"\n",
        "    import requests\n",
        "    \n",
        "    try:\n",
        "        # Convert image to base64\n",
        "        image_data = image_to_base64(image_path)\n",
        "        \n",
        "        # Prepare the request payload for Ollama\n",
        "        payload = {\n",
        "            \"model\": \"granite3.2-vision:2b\",\n",
        "            \"prompt\": query,\n",
        "            \"images\": [image_data.split(',')[1]],  # Remove data URI prefix\n",
        "            \"stream\": False\n",
        "        }\n",
        "        \n",
        "        # Make the API call\n",
        "        response = requests.post(\n",
        "            f\"{OLLAMA_BASE_URL}/api/generate\",\n",
        "            json=payload,\n",
        "            timeout=120  # Vision models can take longer\n",
        "        )\n",
        "        \n",
        "        if response.status_code == 200:\n",
        "            result = response.json()\n",
        "            return result.get(\"response\", \"No description generated.\")\n",
        "        else:\n",
        "            raise Exception(f\"Ollama API returned status {response.status_code}: {response.text}\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        raise Exception(f\"Error describing image: {e}\")\n",
        "\n",
        "\n",
        "print(\"âœ… Alternative vision model function (Ollama direct) defined!\")\n",
        "print(\"Use describe_image_ollama_direct() if the CrewAI method doesn't work.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating the Research Item Identifier Agent\n",
        "\n",
        "### Step 2: Define Pydantic Models for Structured Output\n",
        "\n",
        "First, we need to define the data structures that will hold our research items. This ensures the LLM outputs structured, validated data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define Pydantic models for structured output\n",
        "class ResearchItem(BaseModel):\n",
        "    \"\"\"\n",
        "    Represents a single research item identified in an image.\n",
        "    \n",
        "    Attributes:\n",
        "        item_name: The name/title of the item to research\n",
        "        research_instructions: Specific instructions on what to research about this item\n",
        "    \"\"\"\n",
        "    item_name: str\n",
        "    research_instructions: str\n",
        "\n",
        "\n",
        "class ResearchItems(BaseModel):\n",
        "    \"\"\"\n",
        "    Container for multiple research items.\n",
        "    \n",
        "    Attributes:\n",
        "        items: List of ResearchItem objects\n",
        "    \"\"\"\n",
        "    items: List[ResearchItem]\n",
        "\n",
        "print(\"âœ… Pydantic models defined!\")\n",
        "print(\"\\nModels:\")\n",
        "print(\"  - ResearchItem: Single research topic with name and instructions\")\n",
        "print(\"  - ResearchItems: Collection of research items\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Create the Item Identifier Agent\n",
        "\n",
        "This agent analyzes the image description and user query to identify key features and concepts requiring research.\n",
        "\n",
        "**Agent Role:**\n",
        "- **Role**: Item Identifier\n",
        "- **Goal**: Identify concepts in the image requiring research\n",
        "- **Responsibilities**: \n",
        "  - Analyze image descriptions\n",
        "  - Extract researchable concepts\n",
        "  - Create structured research plans\n",
        "\n",
        "**Inputs:**\n",
        "- Image description generated by the Vision Model\n",
        "- User's research goal or context (optional)\n",
        "\n",
        "**Outputs:**\n",
        "- Structured list of items for further research (as ResearchItems Pydantic model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the Item Identifier Agent\n",
        "item_identifier = Agent(\n",
        "    role='Item Identifier',\n",
        "    goal='Identify concepts in the image requiring research.',\n",
        "    backstory=\"\"\"You are an expert at analyzing images and identifying \n",
        "    key concepts, objects, and topics that would benefit from further research. \n",
        "    You break down complex images into researchable components.\"\"\",\n",
        "    llm=llm,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Create the identification task\n",
        "identification_task = Task(\n",
        "    description=\"\"\"Analyze the following image description and identify all \n",
        "    researchable items, concepts, or topics. For each item, provide:\n",
        "    1. A clear name/title\n",
        "    2. Specific research instructions on what to find out about it.\n",
        "    \n",
        "    Image Description: {image_description}\n",
        "    User Query: {user_query}\n",
        "    \n",
        "    Focus on identifying distinct, researchable topics that would provide \n",
        "    educational value or answer the user's question.\"\"\",\n",
        "    agent=item_identifier,\n",
        "    expected_output='A structured list of items and concepts for further research, with clear research instructions for each.',\n",
        "    output_pydantic=ResearchItems\n",
        ")\n",
        "\n",
        "# Create the identifier crew\n",
        "identifier_crew = Crew(\n",
        "    agents=[item_identifier],\n",
        "    tasks=[identification_task],\n",
        "    process=Process.sequential,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "print(\"âœ… Research Item Identifier Agent created!\")\n",
        "print(\"\\nComponents:\")\n",
        "print(\"  - Agent: Item Identifier\")\n",
        "print(\"  - Task: Identification with Pydantic output\")\n",
        "print(\"  - Crew: Sequential process\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Key Points:**\n",
        "\n",
        "1. **output_pydantic=ResearchItems**: This instructs the LLM to output structured data matching our Pydantic model\n",
        "2. **Sequential Process**: Single agent, single task - runs sequentially\n",
        "3. **Structured Output**: The output will be a Python object we can iterate over\n",
        "\n",
        "**Why Pydantic Output?**\n",
        "- Ensures consistent data structure\n",
        "- Enables type checking and validation\n",
        "- Makes it easy to pass data to the next stage (parallel research)\n",
        "- Reduces parsing errors\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building the Research Crew\n",
        "\n",
        "### Step 4: Set Up RAG Tools\n",
        "\n",
        "Before creating the research crew, we need to set up the tools that will enable RAG (Retrieval-Augmented Generation). These tools allow agents to search the web and access knowledge bases.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up RAG tools for research\n",
        "# Note: These are CrewAI tools that provide web search and knowledge base access\n",
        "\n",
        "# Web search tool - searches the internet for current information\n",
        "do_web_search = WebsiteSearchTool(\n",
        "    website=\"https://www.google.com/search?q=\"\n",
        ")\n",
        "\n",
        "# Knowledge base tool - searches local documents/files\n",
        "# You can customize this to point to your knowledge base\n",
        "do_knowledge_search = FileReadTool()\n",
        "\n",
        "# Alternative: If you have a directory of documents\n",
        "# do_knowledge_search = DirectoryReadTool(directory=\"./knowledge_base\")\n",
        "\n",
        "print(\"âœ… RAG tools configured!\")\n",
        "print(\"\\nTools available:\")\n",
        "print(\"  - do_web_search: Search the web for information\")\n",
        "print(\"  - do_knowledge_search: Search local documents/files\")\n",
        "\n",
        "# Note: You can add more tools here, such as:\n",
        "# - Database search tools\n",
        "# - API integration tools\n",
        "# - Custom research tools\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**About CrewAI Tools:**\n",
        "\n",
        "CrewAI provides several built-in tools:\n",
        "- `WebsiteSearchTool`: Web search capabilities\n",
        "- `FileReadTool`: Read and search local files\n",
        "- `DirectoryReadTool`: Search directories of documents\n",
        "- Custom tools: You can create your own tools\n",
        "\n",
        "**Tool Usage:**\n",
        "- Agents automatically decide when to use tools\n",
        "- Tools are called based on task requirements\n",
        "- Results are incorporated into agent responses\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5: Create the Research Agent and Crew\n",
        "\n",
        "For each identified research item, a separate Research agent will run asynchronously, using RAG to search the web and user documents.\n",
        "\n",
        "**Key Features:**\n",
        "- **Parallel Execution**: Multiple research tasks run simultaneously\n",
        "- **RAG Integration**: Each agent uses web search and knowledge base tools\n",
        "- **Focused Research**: Each agent handles one specific research topic\n",
        "\n",
        "**Inputs:**\n",
        "- List of research items from the Research Item Identifier\n",
        "\n",
        "**Outputs:**\n",
        "- Detailed information, including references and explanations, for each item\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the Researcher Agent\n",
        "researcher = Agent(\n",
        "    role='Researcher',\n",
        "    goal='Conduct detailed research for each identified concept.',\n",
        "    backstory=\"\"\"You are an expert researcher with access to web search and \n",
        "    knowledge bases. You conduct thorough research, cite sources, and provide \n",
        "    comprehensive information on any topic. You always include in-line citations \n",
        "    for every claim you make.\"\"\",\n",
        "    tools=[do_web_search, do_knowledge_search],\n",
        "    llm=llm,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Create the research task\n",
        "# Note: The task description uses placeholders that will be filled when tasks are created\n",
        "research_task = Task(\n",
        "    description=\"\"\"Conduct thorough research on the following topic:\n",
        "    \n",
        "    Item: {item_name}\n",
        "    Research Instructions: {research_instructions}\n",
        "    \n",
        "    Requirements:\n",
        "    1. Use web search to find current, accurate information\n",
        "    2. Search knowledge bases if relevant documents are available\n",
        "    3. Provide detailed explanations with in-line citations\n",
        "    4. Include references to all sources used\n",
        "    5. Focus on answering the specific research instructions provided\"\"\",\n",
        "    agent=researcher,\n",
        "    expected_output='Comprehensive information that directly answers the research instructions, with in-line citations for every reference and a list of sources at the end.'\n",
        ")\n",
        "\n",
        "# Create the research crew\n",
        "research_crew = Crew(\n",
        "    agents=[researcher],\n",
        "    tasks=[research_task],\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "print(\"âœ… Research Crew created!\")\n",
        "print(\"\\nComponents:\")\n",
        "print(\"  - Agent: Researcher (with web search and knowledge base tools)\")\n",
        "print(\"  - Task: Research with dynamic placeholders\")\n",
        "print(\"  - Crew: Ready for parallel execution\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Understanding Parallel Execution:**\n",
        "\n",
        "The `kickoff_for_each_async()` method allows us to:\n",
        "1. Take a list of research items\n",
        "2. Create multiple tasks (one per item)\n",
        "3. Execute all tasks in parallel\n",
        "4. Collect all results\n",
        "\n",
        "This is much faster than sequential execution when researching multiple topics!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Putting It All Together\n",
        "\n",
        "### Step 6: Complete Image Research Function\n",
        "\n",
        "Now let's create a complete function that orchestrates the entire workflow:\n",
        "\n",
        "1. **Image Analysis**: Use vision model to describe the image\n",
        "2. **Item Identification**: Identify researchable topics\n",
        "3. **Parallel Research**: Research all topics simultaneously\n",
        "4. **Synthesis**: Combine findings into a comprehensive report\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def research_image(image_path: str, user_query: str = \"Research all interesting aspects of this image.\") -> dict:\n",
        "    \"\"\"\n",
        "    Complete image research workflow.\n",
        "    \n",
        "    Args:\n",
        "        image_path: Path to the image file\n",
        "        user_query: User's research goal or question\n",
        "        \n",
        "    Returns:\n",
        "        Dictionary containing:\n",
        "            - image_description: Description from vision model\n",
        "            - research_items: List of identified research topics\n",
        "            - research_results: Results from parallel research\n",
        "            - final_report: Synthesized comprehensive report\n",
        "    \"\"\"\n",
        "    print(\"ðŸ” Starting image research workflow...\")\n",
        "    print(f\"ðŸ“· Image: {image_path}\")\n",
        "    print(f\"â“ Query: {user_query}\\n\")\n",
        "    \n",
        "    # Step 1: Describe the image using vision model\n",
        "    print(\"Step 1/4: Analyzing image with vision model...\")\n",
        "    try:\n",
        "        image_description = describe_image_ollama_direct(\n",
        "            image_path, \n",
        "            \"Describe the image in detail for educational purposes.\"\n",
        "        )\n",
        "        print(f\"âœ… Image analyzed ({len(image_description)} characters)\\n\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error analyzing image: {e}\")\n",
        "        return {\"error\": f\"Image analysis failed: {e}\"}\n",
        "    \n",
        "    # Step 2: Identify research items\n",
        "    print(\"Step 2/4: Identifying research topics...\")\n",
        "    try:\n",
        "        result = identifier_crew.kickoff(\n",
        "            inputs={\n",
        "                \"image_description\": image_description,\n",
        "                \"user_query\": user_query\n",
        "            }\n",
        "        )\n",
        "        \n",
        "        # Extract research items from the result\n",
        "        research_items = identification_task.output.pydantic.items\n",
        "        print(f\"âœ… Identified {len(research_items)} research topics:\\n\")\n",
        "        for i, item in enumerate(research_items, 1):\n",
        "            print(f\"  {i}. {item.item_name}\")\n",
        "        print()\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error identifying research items: {e}\")\n",
        "        return {\"error\": f\"Research identification failed: {e}\"}\n",
        "    \n",
        "    # Step 3: Parallel research\n",
        "    print(\"Step 3/4: Conducting parallel research...\")\n",
        "    try:\n",
        "        # Prepare tasks for parallel execution\n",
        "        tasks = []\n",
        "        for item in research_items:\n",
        "            tasks.append({\n",
        "                \"item_name\": item.item_name,\n",
        "                \"research_instructions\": item.research_instructions\n",
        "            })\n",
        "        \n",
        "        # Execute research in parallel\n",
        "        research_results = research_crew.kickoff_for_each_async(tasks)\n",
        "        print(f\"âœ… Research completed for {len(tasks)} topics\\n\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error during research: {e}\")\n",
        "        return {\"error\": f\"Research failed: {e}\"}\n",
        "    \n",
        "    # Step 4: Synthesize findings\n",
        "    print(\"Step 4/4: Synthesizing findings into comprehensive report...\")\n",
        "    try:\n",
        "        # Combine all research results\n",
        "        gathered_info = \"\\n\\n\".join([\n",
        "            f\"Topic: {task['item_name']}\\nResearch: {result}\"\n",
        "            for task, result in zip(tasks, research_results)\n",
        "        ])\n",
        "        \n",
        "        # Create synthesis prompt\n",
        "        synthesis_prompt = f\"\"\"Thoroughly answer the user's question, citing all references.\n",
        "\n",
        "User Query: {user_query}\n",
        "\n",
        "Image Description: {image_description}\n",
        "\n",
        "Gathered Information:\n",
        "{gathered_info}\n",
        "\n",
        "Please synthesize this information into a comprehensive, well-organized report that:\n",
        "1. Directly answers the user's query\n",
        "2. Incorporates all research findings\n",
        "3. Maintains all citations and references\n",
        "4. Provides a clear, educational summary\"\"\"\n",
        "        \n",
        "        # Generate final report\n",
        "        final_report = llm.call(synthesis_prompt)\n",
        "        print(\"âœ… Report synthesized\\n\")\n",
        "        \n",
        "        return {\n",
        "            \"image_description\": image_description,\n",
        "            \"research_items\": [\n",
        "                {\"item_name\": item.item_name, \"research_instructions\": item.research_instructions}\n",
        "                for item in research_items\n",
        "            ],\n",
        "            \"research_results\": research_results,\n",
        "            \"final_report\": final_report\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error synthesizing report: {e}\")\n",
        "        return {\"error\": f\"Report synthesis failed: {e}\"}\n",
        "\n",
        "\n",
        "print(\"âœ… Complete image research function created!\")\n",
        "print(\"\\nFunction: research_image(image_path, user_query)\")\n",
        "print(\"This function orchestrates the entire workflow from image to final report.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example Usage\n",
        "\n",
        "Here's how to use the complete system:\n",
        "\n",
        "```python\n",
        "# Example 1: Research a historical photo\n",
        "result = research_image(\n",
        "    image_path=\"statue_of_liberty.jpg\",\n",
        "    user_query=\"Tell me about the history of the Statue of Liberty and anything else interesting in this image.\"\n",
        ")\n",
        "\n",
        "# Access the results\n",
        "print(\"Final Report:\")\n",
        "print(result[\"final_report\"])\n",
        "\n",
        "# Example 2: Analyze an architecture diagram\n",
        "result = research_image(\n",
        "    image_path=\"system_architecture.png\",\n",
        "    user_query=\"Explain the components and their relationships in this architecture diagram.\"\n",
        ")\n",
        "```\n",
        "\n",
        "**Note**: Make sure you have an image file ready to test with. You can use any image format supported by the vision model (PNG, JPEG, etc.).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Advanced Techniques\n",
        "\n",
        "### 1. Custom Research Tools\n",
        "\n",
        "You can create custom tools for specialized research:\n",
        "\n",
        "```python\n",
        "from crewai_tools import tool\n",
        "\n",
        "@tool(\"Database Search\")\n",
        "def search_database(query: str) -> str:\n",
        "    \"\"\"Search internal database for information.\"\"\"\n",
        "    # Your database search logic here\n",
        "    return results\n",
        "\n",
        "# Add to researcher agent\n",
        "researcher = Agent(\n",
        "    role='Researcher',\n",
        "    tools=[do_web_search, do_knowledge_search, search_database],\n",
        "    # ... other parameters\n",
        ")\n",
        "```\n",
        "\n",
        "### 2. Filtering Research Items\n",
        "\n",
        "You can add logic to filter or prioritize research items:\n",
        "\n",
        "```python\n",
        "def filter_research_items(items: List[ResearchItem], max_items: int = 5) -> List[ResearchItem]:\n",
        "    \"\"\"Filter and prioritize research items.\"\"\"\n",
        "    # Add your filtering logic here\n",
        "    # For example, prioritize items mentioned in user query\n",
        "    return items[:max_items]\n",
        "```\n",
        "\n",
        "### 3. Caching Image Descriptions\n",
        "\n",
        "To avoid re-analyzing the same images:\n",
        "\n",
        "```python\n",
        "import hashlib\n",
        "import json\n",
        "\n",
        "image_cache = {}\n",
        "\n",
        "def get_cached_description(image_path: str) -> str:\n",
        "    \"\"\"Get cached image description if available.\"\"\"\n",
        "    # Create hash of image\n",
        "    with open(image_path, 'rb') as f:\n",
        "        image_hash = hashlib.md5(f.read()).hexdigest()\n",
        "    \n",
        "    if image_hash in image_cache:\n",
        "        return image_cache[image_hash]\n",
        "    \n",
        "    # Generate and cache\n",
        "    description = describe_image_ollama_direct(image_path)\n",
        "    image_cache[image_hash] = description\n",
        "    return description\n",
        "```\n",
        "\n",
        "### 4. Streaming Research Results\n",
        "\n",
        "For real-time updates during research:\n",
        "\n",
        "```python\n",
        "def research_with_streaming(image_path: str, user_query: str):\n",
        "    \"\"\"Research with streaming updates.\"\"\"\n",
        "    # Your streaming implementation\n",
        "    # Use async generators or callbacks\n",
        "    pass\n",
        "```\n",
        "\n",
        "### 5. Multi-Language Support\n",
        "\n",
        "Add language detection and translation:\n",
        "\n",
        "```python\n",
        "from langdetect import detect\n",
        "\n",
        "def detect_and_translate(text: str, target_language: str = \"en\") -> str:\n",
        "    \"\"\"Detect language and translate if needed.\"\"\"\n",
        "    detected = detect(text)\n",
        "    if detected != target_language:\n",
        "        # Translate using your translation service\n",
        "        pass\n",
        "    return text\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Edge Cases and Error Handling\n",
        "\n",
        "### Common Edge Cases\n",
        "\n",
        "Let's handle various edge cases that might occur:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def robust_research_image(image_path: str, user_query: str = \"Research all interesting aspects of this image.\") -> dict:\n",
        "    \"\"\"\n",
        "    Robust version with comprehensive error handling.\n",
        "    \"\"\"\n",
        "    errors = []\n",
        "    warnings = []\n",
        "    \n",
        "    # Validate inputs\n",
        "    if not os.path.exists(image_path):\n",
        "        return {\"error\": f\"Image file not found: {image_path}\"}\n",
        "    \n",
        "    if not user_query or len(user_query.strip()) == 0:\n",
        "        user_query = \"Research all interesting aspects of this image.\"\n",
        "        warnings.append(\"Empty user query, using default\")\n",
        "    \n",
        "    # Check file size (very large images might cause issues)\n",
        "    file_size = os.path.getsize(image_path)\n",
        "    if file_size > 10 * 1024 * 1024:  # 10MB\n",
        "        warnings.append(f\"Large image file ({file_size / 1024 / 1024:.2f}MB), processing may be slow\")\n",
        "    \n",
        "    try:\n",
        "        # Step 1: Image analysis with retry logic\n",
        "        max_retries = 3\n",
        "        image_description = None\n",
        "        \n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                image_description = describe_image_ollama_direct(image_path)\n",
        "                if image_description and len(image_description) > 50:\n",
        "                    break\n",
        "            except Exception as e:\n",
        "                if attempt == max_retries - 1:\n",
        "                    return {\"error\": f\"Failed to analyze image after {max_retries} attempts: {e}\"}\n",
        "                print(f\"Retry {attempt + 1}/{max_retries} for image analysis...\")\n",
        "        \n",
        "        if not image_description:\n",
        "            return {\"error\": \"Image analysis returned empty description\"}\n",
        "        \n",
        "        # Step 2: Research identification with fallback\n",
        "        try:\n",
        "            result = identifier_crew.kickoff(\n",
        "                inputs={\n",
        "                    \"image_description\": image_description,\n",
        "                    \"user_query\": user_query\n",
        "                }\n",
        "            )\n",
        "            research_items = identification_task.output.pydantic.items\n",
        "            \n",
        "            if not research_items or len(research_items) == 0:\n",
        "                warnings.append(\"No research items identified, using fallback\")\n",
        "                # Fallback: create a single research item\n",
        "                research_items = [ResearchItem(\n",
        "                    item_name=\"General Image Analysis\",\n",
        "                    research_instructions=\"Research the main subjects and context of this image.\"\n",
        "                )]\n",
        "        except Exception as e:\n",
        "            errors.append(f\"Research identification error: {e}\")\n",
        "            # Fallback\n",
        "            research_items = [ResearchItem(\n",
        "                item_name=\"General Image Analysis\",\n",
        "                research_instructions=\"Research the main subjects and context of this image.\"\n",
        "            )]\n",
        "        \n",
        "        # Step 3: Parallel research with timeout handling\n",
        "        research_results = []\n",
        "        tasks = []\n",
        "        \n",
        "        for item in research_items:\n",
        "            tasks.append({\n",
        "                \"item_name\": item.item_name,\n",
        "                \"research_instructions\": item.research_instructions\n",
        "            })\n",
        "        \n",
        "        try:\n",
        "            research_results = research_crew.kickoff_for_each_async(tasks)\n",
        "            \n",
        "            # Validate results\n",
        "            if not research_results or len(research_results) != len(tasks):\n",
        "                warnings.append(\"Some research tasks may have failed\")\n",
        "        except Exception as e:\n",
        "            errors.append(f\"Research execution error: {e}\")\n",
        "            # Continue with partial results\n",
        "        \n",
        "        # Step 4: Synthesis with fallback\n",
        "        try:\n",
        "            if research_results:\n",
        "                gathered_info = \"\\n\\n\".join([\n",
        "                    f\"Topic: {task['item_name']}\\nResearch: {result}\"\n",
        "                    for task, result in zip(tasks, research_results)\n",
        "                ])\n",
        "            else:\n",
        "                gathered_info = \"No research results available.\"\n",
        "            \n",
        "            synthesis_prompt = f\"\"\"Thoroughly answer the user's question, citing all references.\n",
        "\n",
        "User Query: {user_query}\n",
        "\n",
        "Image Description: {image_description}\n",
        "\n",
        "Gathered Information:\n",
        "{gathered_info}\n",
        "\n",
        "Please synthesize this information into a comprehensive, well-organized report.\"\"\"\n",
        "            \n",
        "            final_report = llm.call(synthesis_prompt)\n",
        "        except Exception as e:\n",
        "            errors.append(f\"Report synthesis error: {e}\")\n",
        "            final_report = f\"Error generating report: {e}\\n\\nImage Description: {image_description}\"\n",
        "        \n",
        "        return {\n",
        "            \"image_description\": image_description,\n",
        "            \"research_items\": [\n",
        "                {\"item_name\": item.item_name, \"research_instructions\": item.research_instructions}\n",
        "                for item in research_items\n",
        "            ],\n",
        "            \"research_results\": research_results if research_results else [],\n",
        "            \"final_report\": final_report,\n",
        "            \"warnings\": warnings,\n",
        "            \"errors\": errors\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        return {\"error\": f\"Unexpected error: {e}\"}\n",
        "\n",
        "\n",
        "print(\"âœ… Robust research function with error handling created!\")\n",
        "print(\"\\nThis version handles:\")\n",
        "print(\"  - Missing or invalid image files\")\n",
        "print(\"  - Empty or invalid queries\")\n",
        "print(\"  - Large image files\")\n",
        "print(\"  - Network/timeout errors\")\n",
        "print(\"  - Empty research results\")\n",
        "print(\"  - Partial failures\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Edge Case: No Research Items Found\n",
        "\n",
        "**Problem**: Vision model identifies image but no researchable items are found.\n",
        "\n",
        "**Solution**: Implement fallback logic to create a general research item.\n",
        "\n",
        "### Edge Case: Research Timeout\n",
        "\n",
        "**Problem**: Some research tasks take too long or fail.\n",
        "\n",
        "**Solution**: Implement timeout handling and continue with partial results.\n",
        "\n",
        "### Edge Case: Empty Image Description\n",
        "\n",
        "**Problem**: Vision model returns empty or very short description.\n",
        "\n",
        "**Solution**: Add validation and retry logic with different prompts.\n",
        "\n",
        "### Edge Case: Network Issues\n",
        "\n",
        "**Problem**: Web search or API calls fail due to network issues.\n",
        "\n",
        "**Solution**: Implement retry logic and graceful degradation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Troubleshooting Guide\n",
        "\n",
        "### Common Issues and Solutions\n",
        "\n",
        "#### Issue 1: Ollama Connection Error\n",
        "\n",
        "**Symptoms:**\n",
        "```\n",
        "ConnectionError: Cannot connect to Ollama\n",
        "```\n",
        "\n",
        "**Solutions:**\n",
        "1. Verify Ollama is running:\n",
        "   ```bash\n",
        "   ollama serve\n",
        "   ```\n",
        "2. Check if Ollama is accessible:\n",
        "   ```bash\n",
        "   curl http://localhost:11434/api/tags\n",
        "   ```\n",
        "3. Verify the base URL in your code matches your Ollama setup\n",
        "4. Check firewall settings if using remote Ollama\n",
        "\n",
        "#### Issue 2: Model Not Found\n",
        "\n",
        "**Symptoms:**\n",
        "```\n",
        "Error: model 'granite3.2:8b-instruct-q8_0' not found\n",
        "```\n",
        "\n",
        "**Solutions:**\n",
        "1. Pull the required models:\n",
        "   ```bash\n",
        "   ollama pull granite3.2:8b-instruct-q8_0\n",
        "   ollama pull granite3.2-vision:2b\n",
        "   ```\n",
        "2. Verify models are available:\n",
        "   ```bash\n",
        "   ollama list\n",
        "   ```\n",
        "3. Check model names match exactly (case-sensitive)\n",
        "\n",
        "#### Issue 3: Vision Model Not Working\n",
        "\n",
        "**Symptoms:**\n",
        "- Image description is empty or generic\n",
        "- Vision model call fails\n",
        "\n",
        "**Solutions:**\n",
        "1. Use the direct Ollama API method (`describe_image_ollama_direct`)\n",
        "2. Verify image format is supported (PNG, JPEG)\n",
        "3. Check image file size (very large images may timeout)\n",
        "4. Try a simpler prompt first\n",
        "5. Verify vision model is properly pulled:\n",
        "   ```bash\n",
        "   ollama show granite3.2-vision:2b\n",
        "   ```\n",
        "\n",
        "#### Issue 4: Pydantic Validation Errors\n",
        "\n",
        "**Symptoms:**\n",
        "```\n",
        "ValidationError: ResearchItems validation failed\n",
        "```\n",
        "\n",
        "**Solutions:**\n",
        "1. Check that the LLM output matches the expected structure\n",
        "2. Add more detailed instructions in the task description\n",
        "3. Use examples in the prompt to guide the LLM\n",
        "4. Consider using a more capable model (q8_0 instead of q4_K_M)\n",
        "\n",
        "#### Issue 5: Research Tasks Failing\n",
        "\n",
        "**Symptoms:**\n",
        "- Some research tasks return empty results\n",
        "- Web search not working\n",
        "\n",
        "**Solutions:**\n",
        "1. Verify internet connection\n",
        "2. Check if web search tool is properly configured\n",
        "3. Add retry logic for failed tasks\n",
        "4. Verify CrewAI tools are properly installed:\n",
        "   ```bash\n",
        "   pip install --upgrade crewai-tools\n",
        "   ```\n",
        "\n",
        "#### Issue 6: Memory Issues\n",
        "\n",
        "**Symptoms:**\n",
        "- Out of memory errors\n",
        "- Slow performance\n",
        "\n",
        "**Solutions:**\n",
        "1. Reduce number of parallel research tasks\n",
        "2. Use smaller quantization (q4_K_M instead of q8_0)\n",
        "3. Process images in batches\n",
        "4. Clear cache between runs\n",
        "5. Close other applications using memory\n",
        "\n",
        "#### Issue 7: Slow Performance\n",
        "\n",
        "**Symptoms:**\n",
        "- Research takes very long\n",
        "- Timeouts occur\n",
        "\n",
        "**Solutions:**\n",
        "1. Reduce number of research items\n",
        "2. Use GPU acceleration if available\n",
        "3. Optimize image size before processing\n",
        "4. Use faster models or quantization\n",
        "5. Implement caching for repeated queries\n",
        "\n",
        "### Debugging Tips\n",
        "\n",
        "1. **Enable Verbose Mode**: Set `verbose=True` in Crew and Agent definitions\n",
        "2. **Check Logs**: Monitor Ollama logs and CrewAI output\n",
        "3. **Test Components Separately**: Test vision model, then identification, then research\n",
        "4. **Use Simple Test Cases**: Start with simple images and queries\n",
        "5. **Validate Inputs**: Always validate image paths and user queries\n",
        "\n",
        "### Getting Help\n",
        "\n",
        "If you encounter issues not covered here:\n",
        "\n",
        "1. Check CrewAI documentation: https://docs.crewai.com\n",
        "2. Check Ollama documentation: https://ollama.ai\n",
        "3. Review error messages carefully - they often contain clues\n",
        "4. Test with minimal examples to isolate the problem\n",
        "5. Check GitHub issues for CrewAI and Ollama\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises\n",
        "\n",
        "### Exercise 1: Basic Image Research\n",
        "\n",
        "**Goal**: Get familiar with the basic workflow.\n",
        "\n",
        "**Tasks:**\n",
        "1. Find or create a simple image (e.g., a photo of a famous landmark)\n",
        "2. Use `research_image()` to analyze it\n",
        "3. Print the final report\n",
        "4. Examine the research items that were identified\n",
        "\n",
        "**Expected Outcome**: You should see:\n",
        "- Image description from vision model\n",
        "- List of identified research topics\n",
        "- Research results for each topic\n",
        "- Final synthesized report\n",
        "\n",
        "**Challenge**: Try with different types of images (diagram, photo, chart, etc.)\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 2: Custom Research Query\n",
        "\n",
        "**Goal**: Learn to customize research queries.\n",
        "\n",
        "**Tasks:**\n",
        "1. Use the same image from Exercise 1\n",
        "2. Create a specific research query (e.g., \"Focus on the historical significance\")\n",
        "3. Compare results with the default query\n",
        "4. Analyze how the query affects research items identified\n",
        "\n",
        "**Expected Outcome**: Different research items based on your query focus.\n",
        "\n",
        "**Challenge**: Try very specific vs. very general queries and compare results.\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 3: Error Handling\n",
        "\n",
        "**Goal**: Practice handling edge cases.\n",
        "\n",
        "**Tasks:**\n",
        "1. Try the function with:\n",
        "   - A non-existent image path\n",
        "   - An invalid image file\n",
        "   - An empty query\n",
        "   - A very large image file\n",
        "2. Implement error handling for each case\n",
        "3. Add user-friendly error messages\n",
        "\n",
        "**Expected Outcome**: Graceful error handling without crashes.\n",
        "\n",
        "**Challenge**: Create a wrapper function that validates all inputs before processing.\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 4: Custom Research Tools\n",
        "\n",
        "**Goal**: Add custom research capabilities.\n",
        "\n",
        "**Tasks:**\n",
        "1. Create a custom tool that searches a specific website (e.g., Wikipedia)\n",
        "2. Add it to the researcher agent\n",
        "3. Compare results with and without the custom tool\n",
        "\n",
        "**Expected Outcome**: Enhanced research results using your custom tool.\n",
        "\n",
        "**Challenge**: Create a tool that searches multiple sources and combines results.\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 5: Research Item Filtering\n",
        "\n",
        "**Goal**: Optimize research by filtering items.\n",
        "\n",
        "**Tasks:**\n",
        "1. Modify the identification step to limit research items to top 3 most important\n",
        "2. Add logic to prioritize items mentioned in user query\n",
        "3. Compare research quality with filtered vs. unfiltered items\n",
        "\n",
        "**Expected Outcome**: Faster research with focused, relevant results.\n",
        "\n",
        "**Challenge**: Implement smart filtering based on item importance scores.\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 6: Batch Processing\n",
        "\n",
        "**Goal**: Process multiple images efficiently.\n",
        "\n",
        "**Tasks:**\n",
        "1. Create a function to process multiple images\n",
        "2. Implement parallel processing for multiple images\n",
        "3. Generate a summary report for all images\n",
        "\n",
        "**Expected Outcome**: Efficient batch processing of image collections.\n",
        "\n",
        "**Challenge**: Add progress tracking and resume capability for large batches.\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 7: Research Quality Metrics\n",
        "\n",
        "**Goal**: Evaluate research quality.\n",
        "\n",
        "**Tasks:**\n",
        "1. Create metrics to evaluate research quality:\n",
        "   - Number of sources cited\n",
        "   - Length of research results\n",
        "   - Relevance to query\n",
        "2. Implement a scoring system\n",
        "3. Compare different research runs\n",
        "\n",
        "**Expected Outcome**: Quantitative measures of research quality.\n",
        "\n",
        "**Challenge**: Create a feedback loop to improve research based on scores.\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 8: Integration with Knowledge Base\n",
        "\n",
        "**Goal**: Integrate local document search.\n",
        "\n",
        "**Tasks:**\n",
        "1. Create a knowledge base directory with relevant documents\n",
        "2. Configure `do_knowledge_search` to use your knowledge base\n",
        "3. Compare research results with and without knowledge base\n",
        "4. Analyze when knowledge base results are preferred over web search\n",
        "\n",
        "**Expected Outcome**: Research that combines web and local knowledge.\n",
        "\n",
        "**Challenge**: Implement smart routing (when to use web vs. knowledge base).\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 9: Multi-Language Support\n",
        "\n",
        "**Goal**: Support research in multiple languages.\n",
        "\n",
        "**Tasks:**\n",
        "1. Detect the language of user query\n",
        "2. Translate research instructions if needed\n",
        "3. Research in the appropriate language\n",
        "4. Translate results back if needed\n",
        "\n",
        "**Expected Outcome**: Research agent that works in multiple languages.\n",
        "\n",
        "**Challenge**: Maintain citations and references across translations.\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 10: Real-Time Research Dashboard\n",
        "\n",
        "**Goal**: Create a monitoring interface.\n",
        "\n",
        "**Tasks:**\n",
        "1. Add progress tracking to the research function\n",
        "2. Create a simple dashboard showing:\n",
        "   - Current step\n",
        "   - Research items being processed\n",
        "   - Progress percentage\n",
        "3. Display results as they come in\n",
        "\n",
        "**Expected Outcome**: Visual feedback during research process.\n",
        "\n",
        "**Challenge**: Add real-time updates using web sockets or similar technology.\n",
        "\n",
        "---\n",
        "\n",
        "### Bonus Exercise: Complete Application\n",
        "\n",
        "**Goal**: Build a complete, production-ready application.\n",
        "\n",
        "**Tasks:**\n",
        "1. Create a web interface (Flask/FastAPI)\n",
        "2. Add image upload functionality\n",
        "3. Implement user authentication\n",
        "4. Add research history and caching\n",
        "5. Create a results display page\n",
        "6. Add export functionality (PDF, Markdown, etc.)\n",
        "\n",
        "**Expected Outcome**: A complete, deployable image research application.\n",
        "\n",
        "**Challenge**: Add advanced features like research templates, scheduled research, and API endpoints.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "Congratulations! You've learned how to build a comprehensive AI research agent for image analysis. Let's recap what we covered:\n",
        "\n",
        "### Key Concepts Learned\n",
        "\n",
        "1. **Vision Models**: Using Granite Vision to analyze and describe images\n",
        "2. **Agentic AI**: Orchestrating multiple AI agents with CrewAI\n",
        "3. **RAG (Retrieval-Augmented Generation)**: Combining web search and knowledge bases with LLMs\n",
        "4. **Parallel Processing**: Running multiple research tasks simultaneously\n",
        "5. **Structured Output**: Using Pydantic for type-safe data handling\n",
        "6. **Error Handling**: Building robust systems that handle edge cases\n",
        "\n",
        "### Architecture Overview\n",
        "\n",
        "```\n",
        "Image Input\n",
        "    â†“\n",
        "Vision Model (Granite 3.2 Vision)\n",
        "    â†“\n",
        "Image Description\n",
        "    â†“\n",
        "Item Identifier Agent\n",
        "    â†“\n",
        "Research Items (Structured List)\n",
        "    â†“\n",
        "Parallel Research Agents (with RAG tools)\n",
        "    â†“\n",
        "Research Results\n",
        "    â†“\n",
        "Synthesis Agent\n",
        "    â†“\n",
        "Final Comprehensive Report\n",
        "```\n",
        "\n",
        "### Components Built\n",
        "\n",
        "1. **Vision Integration**: Functions to process images and get descriptions\n",
        "2. **Item Identifier Agent**: Identifies researchable topics from images\n",
        "3. **Research Crew**: Parallel research agents with RAG capabilities\n",
        "4. **Complete Workflow**: End-to-end image research function\n",
        "5. **Error Handling**: Robust version with comprehensive error handling\n",
        "\n",
        "### Best Practices\n",
        "\n",
        "1. âœ… **Always validate inputs** before processing\n",
        "2. âœ… **Use structured output** (Pydantic) for reliable data flow\n",
        "3. âœ… **Implement error handling** for all external calls\n",
        "4. âœ… **Enable verbose mode** during development\n",
        "5. âœ… **Test components separately** before integration\n",
        "6. âœ… **Cache results** when possible to improve performance\n",
        "7. âœ… **Monitor resource usage** (memory, API calls, etc.)\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. **Experiment**: Try different images and queries\n",
        "2. **Customize**: Add your own tools and agents\n",
        "3. **Optimize**: Improve performance and quality\n",
        "4. **Deploy**: Build a production application\n",
        "5. **Extend**: Add features like multi-language support, caching, etc.\n",
        "\n",
        "### Resources\n",
        "\n",
        "- **CrewAI Documentation**: https://docs.crewai.com\n",
        "- **Ollama Documentation**: https://ollama.ai\n",
        "- **Granite Models**: https://github.com/ibm-granite-community\n",
        "- **Pydantic Documentation**: https://docs.pydantic.dev\n",
        "\n",
        "### Final Thoughts\n",
        "\n",
        "You now have a powerful image research agent that can:\n",
        "- Analyze any image\n",
        "- Identify researchable topics\n",
        "- Conduct parallel research using RAG\n",
        "- Synthesize findings into comprehensive reports\n",
        "\n",
        "This system demonstrates the power of combining:\n",
        "- **Vision AI** for image understanding\n",
        "- **Agentic AI** for orchestration\n",
        "- **RAG** for information retrieval\n",
        "- **LLMs** for synthesis\n",
        "\n",
        "The possibilities are endless - from educational tools to research assistants to content analysis systems. Keep experimenting and building!\n",
        "\n",
        "---\n",
        "\n",
        "**Happy Building! ðŸš€**\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Project 10",
      "language": "python",
      "name": "project10"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
