{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Build an AI Agent to Master a Game using Python\n",
        "\n",
        "Do you know how AI opponents in video games seem to think, adapt, and sometimes, even outsmart you? The secret behind this is a powerful branch of AI called **Reinforcement Learning**.\n",
        "\n",
        "In fact, AI agents trained with these very principles have achieved superhuman performance in complex games like StarCraft II. In this article, I'll explain how to build an AI agent from scratch using Python that learns to master a classic control game, known as the CartPole game.\n",
        "\n",
        "## What Exactly Is an AI Agent?\n",
        "\n",
        "An AI agent is a program that can observe its environment, make decisions, and take actions to achieve a specific goal. It learns through trial and error, much like a person learning a new skill. It's all done using:\n",
        "\n",
        "1. The Environment: The world the Agent lives in (in our case, the CartPole game).\n",
        "2. The Action: A move the Agent can make (e.g., push the cart left or right).\n",
        "3. The Reward: Feedback from the environment. A *positive reward* (like +1) for a good action, and a *negative reward* (or *no reward*) for a bad one.\n",
        "\n",
        "The Agent's only goal is to maximize its total reward over time. It does this by learning a policy, a strategy for choosing the best action in any given situation. This learning process is called **Reinforcement Learning**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“¦ Install Required Dependencies\n",
        "\n",
        "Before our Agent can learn, it needs a world to interact with. We'll use `Gymnasium`, a popular Python library that provides a vast collection of game environments for testing RL algorithms. We also need `PyTorch` to build the Agent's brain, and `pygame` for rendering the game environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-1.2.3-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: torch in /Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages (2.9.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages (from gymnasium) (2.4.0)\n",
            "Collecting cloudpickle>=1.2.0 (from gymnasium)\n",
            "  Downloading cloudpickle-3.1.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages (from gymnasium) (4.15.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Requirement already satisfied: filelock in /Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages (from torch) (3.20.2)\n",
            "Requirement already satisfied: setuptools in /Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages (from torch) (80.9.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages (from torch) (2025.12.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages (from jinja2->torch) (3.0.3)\n",
            "Downloading gymnasium-1.2.3-py3-none-any.whl (952 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m952.1/952.1 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cloudpickle-3.1.2-py3-none-any.whl (22 kB)\n",
            "Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, cloudpickle, gymnasium\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3/3\u001b[0m [gymnasium]/3\u001b[0m [gymnasium]\n",
            "\u001b[1A\u001b[2KSuccessfully installed cloudpickle-3.1.2 farama-notifications-0.0.4 gymnasium-1.2.3\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install gymnasium torch pygame\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Setting Up Our Playground\n",
        "\n",
        "The game we'll be tackling is CartPole-v1. The goal is simple:\n",
        "\n",
        "1. A pole is attached to a cart, and our Agent must move the cart left or right to keep the pole balanced upright.\n",
        "2. For every moment it holds the pole balanced, it gets a reward of +1.\n",
        "3. If the pole falls over, the game ends.\n",
        "\n",
        "Let's see what this environment looks like. The following code initializes the CartPole game and has it take random actions for a few seconds:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "ename": "DependencyNotInstalled",
          "evalue": "pygame is not installed, run `pip install \"gymnasium[classic-control]\"`",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages/gymnasium/envs/classic_control/cartpole.py:258\u001b[39m, in \u001b[36mCartPoleEnv.render\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    257\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpygame\u001b[39;00m\n\u001b[32m    259\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpygame\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gfxdraw\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pygame'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[31mDependencyNotInstalled\u001b[39m                    Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m env = gym.make(\u001b[33m\"\u001b[39m\u001b[33mCartPole-v1\u001b[39m\u001b[33m\"\u001b[39m, render_mode=\u001b[33m\"\u001b[39m\u001b[33mhuman\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# every game starts with a reset\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m state, info = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# run the game for a short period\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m50\u001b[39m):\n\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m# render the current frame\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages/gymnasium/wrappers/common.py:146\u001b[39m, in \u001b[36mTimeLimit.reset\u001b[39m\u001b[34m(self, seed, options)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Resets the environment with :param:`**kwargs` and sets the number of steps elapsed to zero.\u001b[39;00m\n\u001b[32m    137\u001b[39m \n\u001b[32m    138\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    143\u001b[39m \u001b[33;03m    The reset environment\u001b[39;00m\n\u001b[32m    144\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    145\u001b[39m \u001b[38;5;28mself\u001b[39m._elapsed_steps = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages/gymnasium/core.py:333\u001b[39m, in \u001b[36mWrapper.reset\u001b[39m\u001b[34m(self, seed, options)\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreset\u001b[39m(\n\u001b[32m    330\u001b[39m     \u001b[38;5;28mself\u001b[39m, *, seed: \u001b[38;5;28mint\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m, options: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    331\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    332\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Uses the :meth:`reset` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages/gymnasium/wrappers/common.py:400\u001b[39m, in \u001b[36mOrderEnforcing.reset\u001b[39m\u001b[34m(self, seed, options)\u001b[39m\n\u001b[32m    398\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Resets the environment with `kwargs`.\"\"\"\u001b[39;00m\n\u001b[32m    399\u001b[39m \u001b[38;5;28mself\u001b[39m._has_reset = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages/gymnasium/core.py:333\u001b[39m, in \u001b[36mWrapper.reset\u001b[39m\u001b[34m(self, seed, options)\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreset\u001b[39m(\n\u001b[32m    330\u001b[39m     \u001b[38;5;28mself\u001b[39m, *, seed: \u001b[38;5;28mint\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m, options: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    331\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    332\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Uses the :meth:`reset` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages/gymnasium/wrappers/common.py:293\u001b[39m, in \u001b[36mPassiveEnvChecker.reset\u001b[39m\u001b[34m(self, seed, options)\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.checked_reset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m.checked_reset = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43menv_reset_passive_checker\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    295\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.env.reset(seed=seed, options=options)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages/gymnasium/utils/passive_env_checker.py:185\u001b[39m, in \u001b[36menv_reset_passive_checker\u001b[39m\u001b[34m(env, **kwargs)\u001b[39m\n\u001b[32m    180\u001b[39m     logger.deprecation(\n\u001b[32m    181\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCurrent gymnasium version requires that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    182\u001b[39m     )\n\u001b[32m    184\u001b[39m \u001b[38;5;66;03m# Checks the result of env.reset with kwargs\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m result = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    188\u001b[39m     logger.warn(\n\u001b[32m    189\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe result returned by `env.reset()` was not a tuple of the form `(obs, info)`, where `obs` is a observation and `info` is a dictionary containing additional information. Actual type: `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(result)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    190\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages/gymnasium/envs/classic_control/cartpole.py:244\u001b[39m, in \u001b[36mCartPoleEnv.reset\u001b[39m\u001b[34m(self, seed, options)\u001b[39m\n\u001b[32m    241\u001b[39m \u001b[38;5;28mself\u001b[39m.steps_beyond_terminated = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.render_mode == \u001b[33m\"\u001b[39m\u001b[33mhuman\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m np.array(\u001b[38;5;28mself\u001b[39m.state, dtype=np.float32), {}\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages/gymnasium/envs/classic_control/cartpole.py:261\u001b[39m, in \u001b[36mCartPoleEnv.render\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    259\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpygame\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gfxdraw\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m DependencyNotInstalled(\n\u001b[32m    262\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mpygame is not installed, run `pip install \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mgymnasium[classic-control]\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m`\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    263\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    265\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.screen \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    266\u001b[39m     pygame.init()\n",
            "\u001b[31mDependencyNotInstalled\u001b[39m: pygame is not installed, run `pip install \"gymnasium[classic-control]\"`"
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "import time\n",
        "\n",
        "# load the CartPole environment\n",
        "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
        "\n",
        "# every game starts with a reset\n",
        "state, info = env.reset()\n",
        "\n",
        "# run the game for a short period\n",
        "for _ in range(50):\n",
        "    # render the current frame\n",
        "    env.render()\n",
        "\n",
        "    # choose a random action (0 for push left, 1 for push right)\n",
        "    action = env.action_space.sample()\n",
        "\n",
        "    next_state, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "    print(f\"State: {state.shape}, Action: {action}, Reward: {reward}\")\n",
        "\n",
        "    # update the state for the next loop\n",
        "    state = next_state\n",
        "\n",
        "    # if the game is over, reset it to start a new game\n",
        "    if terminated or truncated:\n",
        "        state, info = env.reset()\n",
        "\n",
        "    time.sleep(0.02) # slow down for visualization\n",
        "\n",
        "# close the environment window\n",
        "env.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you run this, you'll see the cart jittering around randomly and the pole falling over very quickly. This is our baseline, an unintelligent agent. Now, let's build its brain.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Building the Agent's Brain with a Neural Network\n",
        "\n",
        "Our Agent needs a way to decide which action is best in any given state. To do this, we'll use a neural network called a **Q-Network**.\n",
        "\n",
        "The network will take the game's **state** as input (which for CartPole is a set of 4 numbers: cart position, cart velocity, pole angle, and pole angular velocity) and output a **Q-value** for each possible action (push left or push right). The higher the Q-value, the more suitable the network perceives that action to be for the current state.\n",
        "\n",
        "Here's how we define this network using `PyTorch`:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class QNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    Neural Network to approximate the Q-value function.\n",
        "    \"\"\"\n",
        "    def __init__(self, state_size, action_size):\n",
        "        \"\"\"\n",
        "        Initializes the network layers.\n",
        "        :param state_size: The number of features in the game state (e.g., 4 for CartPole).\n",
        "        :param action_size: The number of possible actions (e.g., 2 for CartPole).\n",
        "        \"\"\"\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(state_size, 128),\n",
        "            nn.ReLU(), # activation function\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, action_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, state):\n",
        "        \"\"\"\n",
        "        Defines the forward pass of the network.\n",
        "        It takes a state and returns the Q-values for each action.\n",
        "        \"\"\"\n",
        "        return self.network(state)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This simple network is the core of our Agent's intelligence. It will learn to map game states to smart actions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Creating the Agent's Logic\n",
        "\n",
        "Now we wrap our *Q-Network* inside a `DQNAgent` class. This class will handle all the logic for interacting with the environment, learning from experiences, and making decisions. Here are its three key jobs:\n",
        "\n",
        "1. **Remembering Experiences**: The Agent doesn't just learn from its last move. It stores thousands of past experiences (state, action, reward, next_state, done) in a replay buffer. It then learns from random samples of these memories, which is a much more stable way to train.\n",
        "2. **Deciding a Move**: To choose an action, the Agent uses an **epsilon-greedy strategy**. Most of the time, it **exploits** its knowledge by picking the action its network predicts is best. But sometimes (controlled by a value called epsilon), it **explores** by taking a random action to discover potentially better strategies. Early on, the Agent explores a lot, but as it gets smarter, epsilon decreases, and it relies more on what it has learned.\n",
        "3. **Getting Smarter**: This is the core of the learning process. The Agent compares the Q-value its network expects for an action with a more accurate target Q-value calculated from the actual reward and outcome. The difference between the expected and target values is the loss. The Agent then updates its network to minimize this loss, slowly making its predictions closer to reality.\n",
        "\n",
        "Here's how to implement the DQN Algorithm for creating the Agent's logic:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import deque, namedtuple\n",
        "\n",
        "# Hyperparameters\n",
        "BUFFER_SIZE = 10000     # replay buffer size\n",
        "BATCH_SIZE = 64         # minibatch size for training\n",
        "GAMMA = 0.99            # discount factor for future rewards\n",
        "LR = 5e-4               # learning rate\n",
        "UPDATE_EVERY = 4        # how often to update the network\n",
        "\n",
        "# use GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class DQNAgent():\n",
        "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
        "\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "\n",
        "        # Q-Network\n",
        "        self.qnetwork_local = QNetwork(state_size, action_size).to(device)\n",
        "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
        "\n",
        "        # replay memory\n",
        "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE)\n",
        "        # initialize time step\n",
        "        self.t_step = 0\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        # save experience in replay memory\n",
        "        self.memory.add(state, action, reward, next_state, done)\n",
        "\n",
        "        # learn every UPDATE_EVERY time steps.\n",
        "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
        "        if self.t_step == 0:\n",
        "            # if enough samples are available in memory, get random subset and learn\n",
        "            if len(self.memory) > BATCH_SIZE:\n",
        "                experiences = self.memory.sample()\n",
        "                self.learn(experiences, GAMMA)\n",
        "\n",
        "    def act(self, state, eps=0.):\n",
        "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "        self.qnetwork_local.eval() # set network to evaluation mode\n",
        "        with torch.no_grad():\n",
        "            action_values = self.qnetwork_local(state)\n",
        "        self.qnetwork_local.train() # set network back to training mode\n",
        "\n",
        "        # epsilon-greedy action selection\n",
        "        if random.random() > eps:\n",
        "            return np.argmax(action_values.cpu().data.numpy())\n",
        "        else:\n",
        "            return random.choice(np.arange(self.action_size))\n",
        "\n",
        "    def learn(self, experiences, gamma):\n",
        "        \"\"\"Update value parameters using given batch of experience tuples.\"\"\"\n",
        "        states, actions, rewards, next_states, dones = experiences\n",
        "\n",
        "        # get max predicted Q-values for next states from the network\n",
        "        Q_targets_next = self.qnetwork_local(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "\n",
        "        # compute Q targets for current states\n",
        "        # target = reward + gamma * Q_next (if not done)\n",
        "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
        "\n",
        "        # get expected Q values from local model\n",
        "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
        "\n",
        "        # compute loss\n",
        "        loss = F.mse_loss(Q_expected, Q_targets)\n",
        "\n",
        "        # minimize the loss\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "# Replay Buffer\n",
        "class ReplayBuffer:\n",
        "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
        "    def __init__(self, action_size, buffer_size, batch_size):\n",
        "        self.memory = deque(maxlen=buffer_size)\n",
        "        self.batch_size = batch_size\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Add a new experience to memory.\"\"\"\n",
        "        e = self.experience(state, action, reward, next_state, done)\n",
        "        self.memory.append(e)\n",
        "\n",
        "    def sample(self):\n",
        "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "\n",
        "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
        "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
        "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
        "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
        "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
        "\n",
        "        return (states, actions, rewards, next_states, dones)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the current size of internal memory.\"\"\"\n",
        "        return len(self.memory)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Over thousands of these minor updates, the Q-Network's predictions become incredibly accurate, and the Agent's policy becomes very effective.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: The Training Loop\n",
        "\n",
        "Now we bring everything together in a training loop. We'll set some hyperparameters, like the number of episodes to train for and the epsilon decay rate, and then set the Agent loose in the environment:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "\n",
        "# Initialize Environment and Agent\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "agent = DQNAgent(state_size=state_size, action_size=action_size)\n",
        "\n",
        "# Training Hyperparameters\n",
        "n_episodes = 2000       # max number of training episodes\n",
        "max_t = 1000            # max number of timesteps per episode\n",
        "eps_start = 1.0         # starting value of epsilon\n",
        "eps_end = 0.01          # minimum value of epsilon\n",
        "eps_decay = 0.995       # multiplicative factor for decreasing epsilon\n",
        "\n",
        "def train():\n",
        "    scores = []                         # list containing scores from each episode\n",
        "    scores_window = deque(maxlen=100)   # last 100 scores\n",
        "    eps = eps_start                     # initialize epsilon\n",
        "\n",
        "    for i_episode in range(1, n_episodes + 1):\n",
        "        state, info = env.reset()\n",
        "        score = 0\n",
        "        for t in range(max_t):\n",
        "            action = agent.act(state, eps)\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "\n",
        "            state = next_state\n",
        "            score += reward\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        scores_window.append(score)\n",
        "        scores.append(score)\n",
        "\n",
        "        # decrease epsilon\n",
        "        eps = max(eps_end, eps_decay * eps)\n",
        "\n",
        "        print(f'\\rEpisode {i_episode}\\tAverage Score: {np.mean(scores_window):.2f}', end=\"\")\n",
        "        if i_episode % 100 == 0:\n",
        "            print(f'\\rEpisode {i_episode}\\tAverage Score: {np.mean(scores_window):.2f}')\n",
        "\n",
        "        # check if the environment is solved\n",
        "        if np.mean(scores_window) >= 195.0:\n",
        "            print(f'\\nEnvironment solved in {i_episode-100:d} episodes!\\tAverage Score: {np.mean(scores_window):.2f}')\n",
        "            # save the trained model's weights\n",
        "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
        "            break\n",
        "\n",
        "    return scores\n",
        "\n",
        "scores = train()\n",
        "env.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Watching the output, you'll see the average score start low and steadily climb. In the provided test run, the Agent achieved an average score of 196.76 after just 261 episodes, solving the environment in only 161 learning episodes!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Watching Our AI Agent Play the Game\n",
        "\n",
        "Training is done. We've saved our Agent's brain. Now, let's load those trained weights and watch our Agent play flawlessly:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Testing the AI Agent\n",
        "\n",
        "# Step 1: Initialize the Environment and Agent\n",
        "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "\n",
        "# create an agent\n",
        "agent = DQNAgent(state_size=state_size, action_size=action_size)\n",
        "\n",
        "# Step 2: Load the Trained Weights\n",
        "agent.qnetwork_local.load_state_dict(torch.load('checkpoint.pth'))\n",
        "\n",
        "# Step 3: Watch the Smart Agent Play\n",
        "num_episodes_to_watch = 10\n",
        "\n",
        "for i in range(num_episodes_to_watch):\n",
        "    # reset the environment to get the initial state\n",
        "    state, info = env.reset()\n",
        "    done = False\n",
        "    episode_reward = 0\n",
        "\n",
        "    print(f\"--- Watching Episode {i+1} ---\")\n",
        "\n",
        "    while not done:\n",
        "        # render the environment\n",
        "        env.render()\n",
        "\n",
        "        # choose the best action using the trained network (epsilon=0)\n",
        "        action = agent.act(state)\n",
        "\n",
        "        # perform the action in the environment\n",
        "        next_state, reward, terminated, truncated, info = env.step(action)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        # update the state\n",
        "        state = next_state\n",
        "        episode_reward += reward\n",
        "\n",
        "        # add a small delay to make it watchable\n",
        "        time.sleep(0.02)\n",
        "\n",
        "    print(f\"Score for Episode {i+1}: {episode_reward}\")\n",
        "\n",
        "# close the environment window\n",
        "env.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When you run this code, you'll see the AI Agent expertly balancing the pole, often for hundreds of steps. The final scores speak for themselves, with the Agent consistently achieving high scores, like 500, 460, and 498!\n",
        "\n",
        "## Final Words\n",
        "\n",
        "So, in this article, you learned to build a fully trained AI agent to master a game. We didn't just use a pre-built model; we built the Agent's memory, its decision-making logic, and its learning mechanism from the ground up.\n",
        "\n",
        "This is the foundation. The same principles of states, actions, rewards, and learning from experience are used to build AI Agents that can master far more complex challenges, from a self-driving car navigating a city to an AI champion in a professional video game.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "AI Agents",
      "language": "python",
      "name": "ai-agents"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
