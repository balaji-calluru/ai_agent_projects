{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Building an Agentic RAG Pipeline\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook demonstrates how to build a **Retrieval-Augmented Generation (RAG)** pipeline using LangChain and ChromaDB. \n",
        "\n",
        "### What is RAG?\n",
        "\n",
        "RAG is a technique that combines:\n",
        "- **Retrieval**: Finding relevant information from a knowledge base\n",
        "- **Augmentation**: Enhancing prompts with retrieved context\n",
        "- **Generation**: Using LLMs to generate responses based on the augmented context\n",
        "\n",
        "### Pipeline Steps\n",
        "\n",
        "1. **Document Loading**: Load PDF documents from a folder\n",
        "2. **Text Splitting**: Break documents into smaller chunks for better retrieval\n",
        "3. **Embedding**: Convert text chunks into vector representations\n",
        "4. **Vector Store**: Store embeddings in ChromaDB for efficient similarity search\n",
        "5. **Querying**: Search for relevant documents based on user queries\n",
        "6. **Local LLM**: Set up a local language model for text generation\n",
        "7. **Agent Controller**: Build intelligent query routing logic\n",
        "8. **RAG Agent**: Combine retrieval and generation into a complete system\n",
        "9. **Testing**: Test the agent with different query types\n",
        "\n",
        "---\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "Make sure you have the required packages installed:\n",
        "- `langchain`\n",
        "- `langchain-community`\n",
        "- `langchain-chroma`\n",
        "- `transformers`\n",
        "- `sentence-transformers`\n",
        "- `pypdf`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/balaji/Documents/Learning/AI/ai_agent_projects/venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ All libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from transformers import pipeline\n",
        "\n",
        "print(\"‚úÖ All libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Document Loading\n",
        "\n",
        "The first step in building a RAG pipeline is to load your documents. In this example, we'll load PDF files from a specified folder.\n",
        "\n",
        "### Why PDFs?\n",
        "PDFs are a common format for documents, reports, and research papers. LangChain's `PyPDFLoader` can extract text content from PDF files while preserving page structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    üìÑ Loading RAG MEETS LLMS.pdf...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ignoring wrong pointing object 6 0 (offset 0)\n",
            "Ignoring wrong pointing object 8 0 (offset 0)\n",
            "Ignoring wrong pointing object 10 0 (offset 0)\n",
            "Ignoring wrong pointing object 12 0 (offset 0)\n",
            "Ignoring wrong pointing object 14 0 (offset 0)\n",
            "Ignoring wrong pointing object 17 0 (offset 0)\n",
            "Ignoring wrong pointing object 24 0 (offset 0)\n",
            "Ignoring wrong pointing object 36 0 (offset 0)\n",
            "Ignoring wrong pointing object 38 0 (offset 0)\n",
            "Ignoring wrong pointing object 40 0 (offset 0)\n",
            "Ignoring wrong pointing object 45 0 (offset 0)\n",
            "Ignoring wrong pointing object 48 0 (offset 0)\n",
            "Ignoring wrong pointing object 55 0 (offset 0)\n",
            "Ignoring wrong pointing object 57 0 (offset 0)\n",
            "Ignoring wrong pointing object 64 0 (offset 0)\n",
            "Ignoring wrong pointing object 69 0 (offset 0)\n",
            "Ignoring wrong pointing object 87 0 (offset 0)\n",
            "Ignoring wrong pointing object 90 0 (offset 0)\n",
            "Ignoring wrong pointing object 98 0 (offset 0)\n",
            "Ignoring wrong pointing object 113 0 (offset 0)\n",
            "Ignoring wrong pointing object 115 0 (offset 0)\n",
            "Ignoring wrong pointing object 117 0 (offset 0)\n",
            "Ignoring wrong pointing object 133 0 (offset 0)\n",
            "Ignoring wrong pointing object 135 0 (offset 0)\n",
            "Ignoring wrong pointing object 137 0 (offset 0)\n",
            "Ignoring wrong pointing object 165 0 (offset 0)\n",
            "Ignoring wrong pointing object 167 0 (offset 0)\n",
            "Ignoring wrong pointing object 191 0 (offset 0)\n",
            "Ignoring wrong pointing object 193 0 (offset 0)\n",
            "Ignoring wrong pointing object 195 0 (offset 0)\n",
            "Ignoring wrong pointing object 208 0 (offset 0)\n",
            "Ignoring wrong pointing object 225 0 (offset 0)\n",
            "Ignoring wrong pointing object 247 0 (offset 0)\n",
            "Ignoring wrong pointing object 249 0 (offset 0)\n",
            "Ignoring wrong pointing object 304 0 (offset 0)\n",
            "Ignoring wrong pointing object 322 0 (offset 0)\n",
            "Ignoring wrong pointing object 324 0 (offset 0)\n",
            "Ignoring wrong pointing object 335 0 (offset 0)\n",
            "Ignoring wrong pointing object 337 0 (offset 0)\n",
            "Ignoring wrong pointing object 423 0 (offset 0)\n",
            "Ignoring wrong pointing object 433 0 (offset 0)\n",
            "Ignoring wrong pointing object 496 0 (offset 0)\n",
            "Ignoring wrong pointing object 502 0 (offset 0)\n",
            "Ignoring wrong pointing object 547 0 (offset 0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    üìÑ Loading LLM Introduction.pdf...\n",
            "    üìÑ Loading LLM Python.pdf...\n",
            "    üìÑ Loading LLM.pdf...\n",
            "\n",
            "‚úÖ PDF Pages Loaded: 142\n",
            "   Each page is a separate Document object with content and metadata\n"
          ]
        }
      ],
      "source": [
        "def load_docs(folder_path):\n",
        "    \"\"\"\n",
        "    Load all PDF files from a specified folder.\n",
        "    \n",
        "    Args:\n",
        "        folder_path (str): Path to the folder containing PDF files\n",
        "        \n",
        "    Returns:\n",
        "        list: List of Document objects, each representing a page from the PDFs\n",
        "    \"\"\"\n",
        "    docs = []\n",
        "    for file in os.listdir(folder_path):\n",
        "        if file.endswith(\".pdf\"):\n",
        "            print(f\"    üìÑ Loading {file}...\")\n",
        "            loader = PyPDFLoader(os.path.join(folder_path, file))\n",
        "            docs.extend(loader.load())\n",
        "    return docs\n",
        "\n",
        "# Update this path to where your PDFs are stored\n",
        "data_folder = \"/Users/balaji/Documents/Learning/AI/ai_agent_projects/data/AI\"\n",
        "docs = load_docs(data_folder)\n",
        "print(f\"\\n‚úÖ PDF Pages Loaded: {len(docs)}\")\n",
        "print(f\"   Each page is a separate Document object with content and metadata\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Text Splitting (Chunking)\n",
        "\n",
        "After loading documents, we need to split them into smaller chunks. This is crucial because:\n",
        "\n",
        "1. **Token Limits**: LLMs have context window limits\n",
        "2. **Better Retrieval**: Smaller, focused chunks improve search accuracy\n",
        "3. **Relevance**: Retrieving entire documents is often unnecessary\n",
        "\n",
        "### Chunking Strategy\n",
        "\n",
        "We use `RecursiveCharacterTextSplitter` which:\n",
        "- Splits text by characters (recursively tries different separators)\n",
        "- Maintains semantic coherence when possible\n",
        "- Allows overlap between chunks to preserve context\n",
        "\n",
        "### Parameters:\n",
        "- **chunk_size**: Maximum size of each chunk (in characters)\n",
        "- **chunk_overlap**: Number of characters to overlap between chunks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Documents Split into Chunks: 301\n",
            "   Average chunk size: ~500 characters\n",
            "   Overlap between chunks: 80 characters\n"
          ]
        }
      ],
      "source": [
        "# Configure text splitter\n",
        "# chunk_size: Maximum characters per chunk\n",
        "# chunk_overlap: Characters to overlap between chunks (helps preserve context)\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,      # Adjust based on your needs (500 chars ‚âà 100-150 words)\n",
        "    chunk_overlap=80     # 80 chars overlap ensures context continuity\n",
        ")\n",
        "\n",
        "# Split documents into chunks\n",
        "chunks = text_splitter.split_documents(docs)\n",
        "print(f\"‚úÖ Documents Split into Chunks: {len(chunks)}\")\n",
        "print(f\"   Average chunk size: ~{text_splitter._chunk_size} characters\")\n",
        "print(f\"   Overlap between chunks: {text_splitter._chunk_overlap} characters\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Embeddings\n",
        "\n",
        "Embeddings convert text into numerical vectors (arrays of numbers) that capture semantic meaning. Similar texts have similar embeddings, which allows us to find relevant documents through vector similarity.\n",
        "\n",
        "### Why HuggingFace Embeddings?\n",
        "\n",
        "- **Open Source**: Free to use, no API costs\n",
        "- **Local Execution**: Runs on your machine, ensuring privacy\n",
        "- **Good Performance**: `all-MiniLM-L6-v2` is a popular, efficient model\n",
        "- **Small Size**: ~80MB, fast to download and use\n",
        "\n",
        "### How Embeddings Work:\n",
        "1. Text ‚Üí Numerical Vector (e.g., 384 dimensions)\n",
        "2. Similar texts ‚Üí Similar vectors (measured by cosine similarity)\n",
        "3. Vector search ‚Üí Find most similar documents to a query\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/kn/4r8ws4g95q1dwsv9btm_lc000000gn/T/ipykernel_96749/2913166076.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embedding_model = HuggingFaceEmbeddings(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Embedding model initialized\n",
            "   Model: all-MiniLM-L6-v2\n",
            "   Embedding dimension: 384\n",
            "   Note: Model will be downloaded on first use if not already cached\n"
          ]
        }
      ],
      "source": [
        "# Initialize the embedding model\n",
        "# This will download the model on first use (~80MB)\n",
        "# Model: all-MiniLM-L6-v2 (384-dimensional embeddings)\n",
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name=\"all-MiniLM-L6-v2\"\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Embedding model initialized\")\n",
        "print(\"   Model: all-MiniLM-L6-v2\")\n",
        "print(\"   Embedding dimension: 384\")\n",
        "print(\"   Note: Model will be downloaded on first use if not already cached\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Vector Store (ChromaDB)\n",
        "\n",
        "ChromaDB is a vector database that stores embeddings and enables fast similarity search. We'll use it to:\n",
        "- Store document chunks as embeddings\n",
        "- Perform semantic search to find relevant documents\n",
        "- Persist data to disk for reuse\n",
        "\n",
        "### Why Persist to Disk?\n",
        "\n",
        "- **Reusability**: Don't recreate the database every time\n",
        "- **Performance**: Faster startup on subsequent runs\n",
        "- **Persistence**: Data survives script restarts\n",
        "\n",
        "### Implementation Choice\n",
        "\n",
        "We use `Chroma.from_documents()` because it:\n",
        "- ‚úÖ Preserves document metadata (source, page numbers, etc.)\n",
        "- ‚úÖ Persists to disk automatically\n",
        "- ‚úÖ Is a one-step operation (simpler code)\n",
        "- ‚úÖ Handles embedding generation internally\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Chroma Database Created\n",
            "   Database location: ./chroma_db\n",
            "   Documents stored: 301\n",
            "   Database will persist between runs\n"
          ]
        }
      ],
      "source": [
        "# Create ChromaDB vector store\n",
        "# This will:\n",
        "# 1. Generate embeddings for all chunks\n",
        "# 2. Store them in ChromaDB\n",
        "# 3. Save to disk in the 'chroma_db' directory\n",
        "\n",
        "chroma_db = Chroma.from_documents(\n",
        "    documents=chunks,                    # Document chunks to store\n",
        "    embedding=embedding_model,           # Embedding model to use\n",
        "    persist_directory=\"chroma_db\"        # Directory to save the database\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Chroma Database Created\")\n",
        "print(f\"   Database location: ./chroma_db\")\n",
        "print(f\"   Documents stored: {len(chunks)}\")\n",
        "print(f\"   Database will persist between runs\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Querying the Database\n",
        "\n",
        "Now that we have our vector store, we can query it to find relevant documents. The `similarity_search` method:\n",
        "1. Converts the query into an embedding\n",
        "2. Finds the most similar document chunks (by cosine similarity)\n",
        "3. Returns the top-k most relevant chunks\n",
        "\n",
        "### How Similarity Search Works:\n",
        "- Query: \"What is machine learning?\"\n",
        "- System finds chunks with similar embeddings\n",
        "- Returns chunks that semantically match the query\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Query: 'What is the main topic of the documents?'\n",
            "\n",
            "‚úÖ Found 4 relevant chunks:\n",
            "\n",
            "================================================================================\n",
            "\n",
            "üìÑ Result 1:\n",
            "Content: adapted to a wide range of language-related tasks, like generating content or summarizing legal \n",
            "documentation....\n",
            "Metadata: {'producer': 'macOS Version 12.6.1 (Build 21G217) Quartz PDFContext', 'creationdate': '2023-03-17T18:08:34+00:00', 'creator': 'Microsoft Word', 'title': 'A Beginner‚Äôs Guide to Large Language Models', 'page': 2, 'keywords': 'Large Language Models, What is a large language model, llm, how do large language models work', 'moddate': '2023-03-17T11:30:56-07:00', 'page_label': '3', 'source': '/Users/balaji/Documents/Learning/AI/ai_agent_projects/data/AI/LLM.pdf', 'subject': 'Large Language Models', 'total_pages': 25, 'author': 'NVIDIA'}\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "üìÑ Result 2:\n",
            "Content: Table 1. Reference Documents  \n",
            "Document Document Location \n",
            "LLM Application on Arc dGPU https://github.com/violet17/LLM_Arc_dGPU \n",
            "LLM Application on Windows https://github.com/KiwiHana/LLM_UI_Windows_C...\n",
            "Metadata: {'total_pages': 14, 'subject': '', 'creationdate': '2023-12-13T10:01:26+08:00', 'moddate': '2023-12-13T10:03:58+08:00', 'title': 'Large Language Model Application Development Guide on Arc dGPU by BigDL-LLM Python* - User Guide', 'creator': 'Acrobat PDFMaker 23 for Word', 'producer': 'Adobe PDF Library 23.6.156', 'page_label': '14', 'source': '/Users/balaji/Documents/Learning/AI/ai_agent_projects/data/AI/LLM Python.pdf', 'keywords': 'CTPClassification=CTP_IC:VisualMarkings=, CTPClassification=CTP_IC', 'author': 'Intel Corporation', 'page': 13}\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "üìÑ Result 3:\n",
            "Content: A Beginner‚Äôs Guide to Large Language Models 8 \n",
            " \n",
            "Introduction to LLMs \n",
            "A large language model is a type of artificial intelligence (AI) system \n",
            "that is capable of generating human-like text based on t...\n",
            "Metadata: {'title': 'A Beginner‚Äôs Guide to Large Language Models', 'creator': 'Microsoft Word', 'total_pages': 25, 'creationdate': '2023-03-17T18:08:34+00:00', 'source': '/Users/balaji/Documents/Learning/AI/ai_agent_projects/data/AI/LLM.pdf', 'subject': 'Large Language Models', 'moddate': '2023-03-17T11:30:56-07:00', 'producer': 'macOS Version 12.6.1 (Build 21G217) Quartz PDFContext', 'page_label': '8', 'page': 7, 'keywords': 'Large Language Models, What is a large language model, llm, how do large language models work', 'author': 'NVIDIA'}\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "üìÑ Result 4:\n",
            "Content: Integrating Information Retrieval in Generation: RA-LLM\n",
            "Information / Knowledge retrievalContent generationRA-LLMs\n",
            "External Knowledge Base‚Ä¢High-quality knowledge‚Ä¢Specialized knowledge‚Ä¢Scalable‚Ä¢Easy-up...\n",
            "Metadata: {'moddate': \"D:20240825061808Z00'00'\", 'producer': 'macOS ÁâàÊú¨12.6ÔºàÁâàÂè∑21G115Ôºâ Quartz PDFContext', 'page': 12, 'source': '/Users/balaji/Documents/Learning/AI/ai_agent_projects/data/AI/RAG MEETS LLMS.pdf', 'total_pages': 68, 'page_label': '13', 'creator': 'PyPDF', 'creationdate': \"D:20240825061808Z00'00'\"}\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Create a retriever from the vector store\n",
        "# A retriever is an interface that returns documents based on a query\n",
        "retriever = chroma_db.as_retriever(search_kwargs={\"k\": 4})\n",
        "\n",
        "# Example query\n",
        "query = \"What is the main topic of the documents?\"\n",
        "\n",
        "# Perform similarity search\n",
        "# Returns the top-k most similar chunks (default k=4)\n",
        "results = chroma_db.similarity_search(query, k=4)\n",
        "\n",
        "print(f\"üîç Query: '{query}'\")\n",
        "print(f\"\\n‚úÖ Found {len(results)} relevant chunks:\\n\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Display results\n",
        "for i, result in enumerate(results, 1):\n",
        "    print(f\"\\nüìÑ Result {i}:\")\n",
        "    print(f\"Content: {result.page_content[:200]}...\")  # First 200 chars\n",
        "    if hasattr(result, 'metadata'):\n",
        "        print(f\"Metadata: {result.metadata}\")\n",
        "    print(\"-\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Adding a Local LLM\n",
        "\n",
        "To complete the RAG pipeline, we need a Language Model (LLM) that can generate answers based on the retrieved context. Here, we'll use a local LLM from HuggingFace to avoid API costs and ensure privacy.\n",
        "\n",
        "### Why Google FLAN-T5?\n",
        "\n",
        "- **Local Execution**: Runs entirely on your machine\n",
        "- **No API Costs**: Free to use\n",
        "- **Small Model**: `flan-t5-base` is relatively small (~250MB)\n",
        "- **Text Generation**: Good for question-answering tasks\n",
        "\n",
        "### Note on Model Size:\n",
        "- The model will be downloaded on first use (~250MB)\n",
        "- Requires sufficient RAM to run (recommended: 8GB+)\n",
        "- For better quality, consider larger models like `flan-t5-large` or `flan-t5-xl`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use mps:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Local LLM initialized\n",
            "   Model: google/flan-t5-base\n",
            "   Task: text2text-generation\n",
            "   Max tokens: 150\n",
            "   Note: Model will be downloaded on first use if not already cached\n"
          ]
        }
      ],
      "source": [
        "# Import transformers pipeline for local LLM\n",
        "from transformers import pipeline\n",
        "\n",
        "# Initialize a local LLM using HuggingFace Transformers\n",
        "# This will download the model on first use (~250MB)\n",
        "llm = pipeline(\n",
        "    \"text2text-generation\",              # Task type: text-to-text generation\n",
        "    model=\"google/flan-t5-base\",         # Model: Google's FLAN-T5 base model\n",
        "    max_new_tokens=150                   # Maximum tokens to generate\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Local LLM initialized\")\n",
        "print(\"   Model: google/flan-t5-base\")\n",
        "print(\"   Task: text2text-generation\")\n",
        "print(\"   Max tokens: 150\")\n",
        "print(\"   Note: Model will be downloaded on first use if not already cached\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Building an Agentic Controller\n",
        "\n",
        "An **agentic controller** is a decision-making component that determines how to handle a query. It decides whether to:\n",
        "- **SEARCH**: Retrieve information from the document database\n",
        "- **DIRECT**: Answer directly without searching (for general knowledge questions)\n",
        "\n",
        "### Why Use an Agent Controller?\n",
        "\n",
        "- **Efficiency**: Don't search when it's not needed\n",
        "- **Smart Routing**: Different queries need different approaches\n",
        "- **Cost Optimization**: Avoid unnecessary retrieval operations\n",
        "- **Better UX**: Faster responses for simple questions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üß† Testing Agent Controller:\n",
            "\n",
            "Query: 'Give me a summary from the PDF'\n",
            "Action: SEARCH\n",
            "\n",
            "Query: 'What is machine learning?'\n",
            "Action: DIRECT\n",
            "\n",
            "Query: 'Find information about AI'\n",
            "Action: SEARCH\n",
            "\n",
            "Query: 'What is the weather today?'\n",
            "Action: DIRECT\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Agent Controller: Decides whether to search documents or answer directly\n",
        "def agent_controller(query):\n",
        "    \"\"\"\n",
        "    Determines the action to take based on the query.\n",
        "    \n",
        "    Args:\n",
        "        query (str): User's question\n",
        "        \n",
        "    Returns:\n",
        "        str: \"search\" if document search is needed, \"direct\" otherwise\n",
        "    \"\"\"\n",
        "    q = query.lower()\n",
        "    \n",
        "    # Keywords that indicate document search is needed\n",
        "    search_keywords = [\"pdf\", \"document\", \"data\", \"summarize\", \"information\", \"find\"]\n",
        "    \n",
        "    if any(word in q for word in search_keywords):\n",
        "        return \"search\"\n",
        "    return \"direct\"\n",
        "\n",
        "# Test the controller\n",
        "test_queries = [\n",
        "    \"Give me a summary from the PDF\",\n",
        "    \"What is machine learning?\",  # General knowledge\n",
        "    \"Find information about AI\",\n",
        "    \"What is the weather today?\"  # General knowledge\n",
        "]\n",
        "\n",
        "print(\"üß† Testing Agent Controller:\\n\")\n",
        "for query in test_queries:\n",
        "    action = agent_controller(query)\n",
        "    print(f\"Query: '{query}'\")\n",
        "    print(f\"Action: {action.upper()}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Complete RAG Agent\n",
        "\n",
        "Now we'll combine everything into a complete RAG agent that:\n",
        "1. Uses the controller to decide the action\n",
        "2. Retrieves relevant documents if needed\n",
        "3. Augments the query with context\n",
        "4. Generates an answer using the LLM\n",
        "\n",
        "### How RAG Works:\n",
        "\n",
        "1. **Query comes in** ‚Üí Agent controller decides action\n",
        "2. **If SEARCH**: \n",
        "   - Retrieve relevant chunks from vector store\n",
        "   - Combine chunks into context\n",
        "   - Augment query with context\n",
        "   - Generate answer using LLM\n",
        "3. **If DIRECT**: \n",
        "   - Generate answer directly using LLM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ RAG Agent function created\n",
            "   The agent can now intelligently route queries and generate answers\n"
          ]
        }
      ],
      "source": [
        "# Complete RAG Agent Function\n",
        "def rag_answer(query):\n",
        "    \"\"\"\n",
        "    Complete RAG agent that retrieves context and generates answers.\n",
        "    \n",
        "    Args:\n",
        "        query (str): User's question\n",
        "        \n",
        "    Returns:\n",
        "        str: Generated answer\n",
        "    \"\"\"\n",
        "    # Step 1: Agent controller decides the action\n",
        "    action = agent_controller(query)\n",
        "    \n",
        "    if action == \"search\":\n",
        "        # Step 2: Search mode - retrieve relevant documents\n",
        "        print(f\"üïµÔ∏è Agent decided to SEARCH documents for: '{query}'\")\n",
        "        \n",
        "        # Retrieve relevant chunks from the vector store\n",
        "        results = retriever.invoke(query)\n",
        "        \n",
        "        # Step 3: Combine retrieved chunks into context\n",
        "        context = \"\\n\".join([r.page_content for r in results])\n",
        "        \n",
        "        # Step 4: Augment the query with context\n",
        "        final_prompt = f\"Use this context:\\n{context}\\n\\nAnswer:\\n{query}\"\n",
        "        \n",
        "    else:\n",
        "        # Direct mode - answer without searching\n",
        "        print(f\"ü§ñ Agent decided to answer DIRECTLY: '{query}'\")\n",
        "        final_prompt = query\n",
        "    \n",
        "    # Step 5: Generate answer using LLM\n",
        "    response = llm(final_prompt)[0][\"generated_text\"]\n",
        "    return response\n",
        "\n",
        "print(\"‚úÖ RAG Agent function created\")\n",
        "print(\"   The agent can now intelligently route queries and generate answers\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Testing the RAG Agent\n",
        "\n",
        "Let's test our complete RAG agent with different types of queries to see how it handles:\n",
        "- Document-specific questions (should trigger search)\n",
        "- General knowledge questions (should answer directly)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "TEST 1: Document-Specific Question\n",
            "================================================================================\n",
            "üïµÔ∏è Agent decided to SEARCH documents for: 'Give me a 5-point summary from the PDF'\n",
            "\n",
            "üí¨ Answer:\n",
            "SELF-RAG Overview 62 including language translation, summarization, question answering, and text completion. GPT-3 made it evident that large-scale models can accurately perform a wide ‚Äì and previously unheard-of ‚Äì range of NLP tasks, from text summarization to text generation. It also showed that LLMs could generate outputs that are nearly indistinguishable from human-created text, all while learning on their own with minimal human intervention.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test 1: Document-specific question (should trigger search)\n",
        "print(\"=\" * 80)\n",
        "print(\"TEST 1: Document-Specific Question\")\n",
        "print(\"=\" * 80)\n",
        "query1 = \"Give me a 5-point summary from the PDF\"\n",
        "answer1 = rag_answer(query1)\n",
        "print(f\"\\nüí¨ Answer:\\n{answer1}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "TEST 2: General Knowledge Question\n",
            "================================================================================\n",
            "ü§ñ Agent decided to answer DIRECTLY: 'What is an Ideal Resume Format? Explain in 50 words.'\n",
            "\n",
            "üí¨ Answer:\n",
            "An Ideal Resume Format is a format for a resume to be written in a professional manner. An Ideal Resume Format is a format for a resume to be written in a professional manner. An Ideal Resume Format is a format for a resume to be written in a professional manner. An Ideal Resume Format is a format for a resume to be written in a professional manner. An Ideal Resume Format is a format for a resume to be written in a professional manner. An Ideal Resume Format is a format for a resume to be written in a professional manner. An Ideal Resume Format is a format for a resume to be written in a professional manner.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test 2: General knowledge question (should answer directly)\n",
        "print(\"=\" * 80)\n",
        "print(\"TEST 2: General Knowledge Question\")\n",
        "print(\"=\" * 80)\n",
        "query2 = \"What is an Ideal Resume Format? Explain in 50 words.\"\n",
        "answer2 = rag_answer(query2)\n",
        "print(f\"\\nüí¨ Answer:\\n{answer2}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Advanced: Loading an Existing Database\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps: Building a Complete RAG System\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "You've successfully built a **complete Agentic RAG pipeline** that can:\n",
        "- ‚úÖ Load documents from PDFs\n",
        "- ‚úÖ Split them into manageable chunks\n",
        "- ‚úÖ Create semantic embeddings\n",
        "- ‚úÖ Store them in a searchable vector database\n",
        "- ‚úÖ Query for relevant information\n",
        "- ‚úÖ Use a local LLM for text generation\n",
        "- ‚úÖ Intelligently route queries (search vs. direct)\n",
        "- ‚úÖ Generate context-aware answers\n",
        "\n",
        "### Key Components Built:\n",
        "\n",
        "1. **Document Processing**: PDF loading and chunking\n",
        "2. **Vector Store**: ChromaDB with persistent storage\n",
        "3. **Local LLM**: Google FLAN-T5 for text generation\n",
        "4. **Agent Controller**: Smart query routing\n",
        "5. **RAG Agent**: Complete retrieval-augmented generation system\n",
        "\n",
        "### Improvements You Could Make:\n",
        "\n",
        "- **Better LLM**: Use larger models (flan-t5-large, llama-2, mistral) for better quality\n",
        "- **Enhanced Controller**: Add more sophisticated routing logic (e.g., using embeddings)\n",
        "- **Streaming**: Add streaming responses for better UX\n",
        "- **Memory**: Add conversation memory for multi-turn dialogues\n",
        "- **Evaluation**: Add metrics to evaluate answer quality\n",
        "- **UI**: Build a web interface (Gradio, Streamlit) for easy interaction\n",
        "\n",
        "This foundation can be extended to build powerful AI applications!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Advanced: Loading an Existing Database\n",
        "\n",
        "If you've already created a ChromaDB and want to load it (instead of recreating it), use this:\n",
        "\n",
        "```python\n",
        "# Load existing ChromaDB\n",
        "chroma_db = Chroma(\n",
        "    persist_directory=\"chroma_db\",\n",
        "    embedding_function=embedding_model\n",
        ")\n",
        "```\n",
        "\n",
        "This is useful when:\n",
        "- You've already processed documents\n",
        "- You want to add new documents to an existing database\n",
        "- You're running queries without modifying the database\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps: Building a Complete RAG System\n",
        "\n",
        "To make this a complete RAG pipeline, you would:\n",
        "\n",
        "1. **Add an LLM**: Use the retrieved chunks as context for an LLM (e.g., OpenAI, Anthropic, or local models)\n",
        "2. **Create a Chain**: Use LangChain's `RetrievalQA` chain to combine retrieval + generation\n",
        "3. **Add a Chat Interface**: Build a conversational interface for your RAG system\n",
        "4. **Implement Agents**: Add agentic capabilities for more complex reasoning\n",
        "\n",
        "### Example: Complete RAG Chain\n",
        "\n",
        "```python\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=OpenAI(),\n",
        "    retriever=chroma_db.as_retriever(),\n",
        "    return_source_documents=True\n",
        ")\n",
        "\n",
        "response = qa_chain({\"query\": \"What is the main topic?\"})\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "You've successfully built a RAG pipeline that can:\n",
        "- ‚úÖ Load documents from PDFs\n",
        "- ‚úÖ Split them into manageable chunks\n",
        "- ‚úÖ Create semantic embeddings\n",
        "- ‚úÖ Store them in a searchable vector database\n",
        "- ‚úÖ Query for relevant information\n",
        "\n",
        "This foundation can be extended to build powerful AI applications!\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "AI Agents",
      "language": "python",
      "name": "ai-agents"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
