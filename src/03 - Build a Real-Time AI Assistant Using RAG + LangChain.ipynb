{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Real-Time AI Assistant Using RAG + LangChain\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- âœ… Understand RAG (Retrieval-Augmented Generation) concepts\n",
    "- âœ… Learn LangChain Expression Language (LCEL)\n",
    "- âœ… Build a real-time AI assistant that searches the web\n",
    "- âœ… Implement advanced features like memory, caching, and streaming\n",
    "- âœ… Test and validate your assistant\n",
    "\n",
    "## ðŸ“š What You'll Build\n",
    "\n",
    "A powerful AI assistant that:\n",
    "- Answers questions using real-time web search\n",
    "- Combines retrieval (search) with generation (LLM)\n",
    "- Runs completely locally with free, open-source tools\n",
    "- Can be extended with local documents\n",
    "\n",
    "## ðŸ› ï¸ Toolkit\n",
    "\n",
    "1. **Ollama**: Run open-source LLMs locally (no API keys!)\n",
    "2. **LangChain**: Framework for building LLM applications\n",
    "3. **DuckDuckGo Search**: Free web search (no API key needed)\n",
    "4. **LCEL**: Pythonic way to build chains\n",
    "\n",
    "---\n",
    "\n",
    "## âš ï¸ Prerequisites\n",
    "\n",
    "Before starting, make sure you have:\n",
    "\n",
    "1. **Ollama installed**: https://ollama.com\n",
    "2. **Model pulled**: Run `ollama pull llama3:8b` in terminal\n",
    "3. **Python packages**: Install with `pip install langchain langchain-community langchain-ollama duckduckgo-search`\n",
    "\n",
    "Let's verify your setup in the next cell!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Verify Your Setup\n",
    "\n",
    "Let's check if everything is installed correctly. Run this cell to verify your environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Checking your setup...\n",
      "\n",
      "âœ… Python version: 3.13.5\n",
      "âœ… langchain installed\n",
      "âœ… langchain_community installed\n",
      "âœ… langchain_ollama installed\n",
      "âœ… duckduckgo_search installed\n",
      "\n",
      "ðŸŽ‰ All packages installed!\n",
      "\n",
      "âœ… Ollama is installed and accessible\n",
      "Available models:\n",
      "NAME                 ID              SIZE      MODIFIED     \n",
      "mistral:7b           6577803aa9a0    4.4 GB    6 days ago      \n",
      "llama3.1:8b          46e0c10c039e    4.9 GB    6 days ago      \n",
      "llama3:latest        365c0bd3c000    4.7 GB    5 months ago    \n",
      "llama3.2:latest      a80c4f17acd5    2.0 GB    5 months ago    \n",
      "gemma3:latest        a2af6cc3eb7f    3.3 GB    5 months ago    \n",
      "gemma3:4b-it-fp16    c4da438ae756    8.6 GB    6 months ago    \n",
      "qwen3:0.6b           7df6b6e09427    522 MB    6 months ago    \n",
      "\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Verify Setup\n",
    "import sys\n",
    "\n",
    "print(\"ðŸ” Checking your setup...\\n\")\n",
    "\n",
    "# Check Python version\n",
    "python_version = sys.version_info\n",
    "print(f\"âœ… Python version: {python_version.major}.{python_version.minor}.{python_version.micro}\")\n",
    "\n",
    "# Check required packages\n",
    "required_packages = [\n",
    "    \"langchain\",\n",
    "    \"langchain_community\", \n",
    "    \"langchain_ollama\",\n",
    "    \"duckduckgo_search\"\n",
    "]\n",
    "\n",
    "missing_packages = []\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        __import__(package.replace(\"-\", \"_\"))\n",
    "        print(f\"âœ… {package} installed\")\n",
    "    except ImportError:\n",
    "        print(f\"âŒ {package} NOT installed\")\n",
    "        missing_packages.append(package)\n",
    "\n",
    "if missing_packages:\n",
    "    print(f\"\\nâš ï¸ Missing packages: {', '.join(missing_packages)}\")\n",
    "    print(\"Install with: pip install \" + \" \".join(missing_packages))\n",
    "else:\n",
    "    print(\"\\nðŸŽ‰ All packages installed!\")\n",
    "\n",
    "# Check Ollama (optional - will fail gracefully if not running)\n",
    "try:\n",
    "    import subprocess\n",
    "    result = subprocess.run([\"ollama\", \"list\"], capture_output=True, text=True, timeout=5)\n",
    "    if result.returncode == 0:\n",
    "        print(\"\\nâœ… Ollama is installed and accessible\")\n",
    "        print(\"Available models:\")\n",
    "        print(result.stdout)\n",
    "    else:\n",
    "        print(\"\\nâš ï¸ Ollama command failed. Make sure Ollama is installed and running.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"\\nâš ï¸ Ollama not found in PATH. Make sure it's installed: https://ollama.com\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nâš ï¸ Could not check Ollama: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Understanding RAG - Core Concepts\n",
    "\n",
    "### What is RAG?\n",
    "\n",
    "**RAG (Retrieval-Augmented Generation)** is a technique that combines:\n",
    "\n",
    "1. **Retrieval**: Finding relevant information from a knowledge source (web, documents, databases)\n",
    "2. **Augmentation**: Enhancing the LLM prompt with retrieved context  \n",
    "3. **Generation**: Using LLMs to generate responses based on the augmented context\n",
    "\n",
    "### Why RAG?\n",
    "\n",
    "- âŒ **LLMs have knowledge cutoffs** - They don't know recent events\n",
    "- âŒ **LLMs can hallucinate** - They make up information\n",
    "- âœ… **RAG grounds responses** - Uses real, verifiable sources\n",
    "\n",
    "### RAG Flow Diagram\n",
    "\n",
    "```\n",
    "User Question \n",
    "    â†“\n",
    "Search/Retrieve (DuckDuckGo)\n",
    "    â†“\n",
    "Context + Question â†’ LLM Prompt\n",
    "    â†“\n",
    "LLM Generates Answer\n",
    "    â†“\n",
    "Response to User\n",
    "```\n",
    "\n",
    "### What is LangChain?\n",
    "\n",
    "**LangChain** provides:\n",
    "- **Chains**: Composable sequences of operations\n",
    "- **Tools**: Integrations with external systems (search, databases, APIs)\n",
    "- **Memory**: Conversation history management\n",
    "- **LCEL**: Pythonic syntax using pipe operator (`|`)\n",
    "\n",
    "### What is LCEL?\n",
    "\n",
    "**LCEL (LangChain Expression Language)** uses Python's pipe operator:\n",
    "\n",
    "```python\n",
    "chain = step1 | step2 | step3\n",
    "```\n",
    "\n",
    "This is equivalent to: `result = step3(step2(step1(input)))`\n",
    "\n",
    "**Benefits:**\n",
    "- âœ… Readable and intuitive\n",
    "- âœ… Easy to debug\n",
    "- âœ… Supports streaming, batching, and async\n",
    "- âœ… Type-safe\n",
    "\n",
    "Now let's build it step by step!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Import Required Libraries\n",
    "\n",
    "First, let's import all the libraries we need. Each import serves a specific purpose:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All libraries imported successfully!\n",
      "\n",
      "ðŸ“¦ What each import does:\n",
      "   â€¢ OllamaLLM: Run LLMs locally on your machine (from langchain_ollama)\n",
      "   â€¢ DuckDuckGoSearchRun: Free web search (no API key needed)\n",
      "   â€¢ ChatPromptTemplate: Create reusable prompt templates\n",
      "   â€¢ RunnablePassthrough: Pass data through chain while adding new data\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from langchain_ollama import OllamaLLM              # Interface to local LLM via Ollama (updated import)\n",
    "from langchain_community.tools import DuckDuckGoSearchRun  # Web search tool (no API key!)\n",
    "from langchain_core.prompts import ChatPromptTemplate      # Template for LLM prompts\n",
    "from langchain_core.runnables import RunnablePassthrough   # Pass data through chain\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")\n",
    "print(\"\\nðŸ“¦ What each import does:\")\n",
    "print(\"   â€¢ OllamaLLM: Run LLMs locally on your machine (from langchain_ollama)\")\n",
    "print(\"   â€¢ DuckDuckGoSearchRun: Free web search (no API key needed)\")\n",
    "print(\"   â€¢ ChatPromptTemplate: Create reusable prompt templates\")\n",
    "print(\"   â€¢ RunnablePassthrough: Pass data through chain while adding new data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Initialize Components\n",
    "\n",
    "Now let's initialize the two main components:\n",
    "\n",
    "1. **The LLM**: Llama 3 running via Ollama (local, free, private)\n",
    "2. **The Search Tool**: DuckDuckGo (free, no API key)\n",
    "\n",
    "### Understanding the Components\n",
    "\n",
    "**Ollama LLM:**\n",
    "- Runs locally on your machine\n",
    "- No internet required after model download\n",
    "- Free and private (data stays on your machine)\n",
    "- Model options: `llama3:8b` (fast), `llama3:70b` (better quality, slower)\n",
    "\n",
    "**DuckDuckGo Search:**\n",
    "- Free web search (no API key)\n",
    "- Returns text snippets from search results\n",
    "- Can be rate-limited (use responsibly)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– Initializing LLM with model: llama3.1:8b\n",
      "   Testing LLM connection...\n",
      "   âœ… LLM is working! Response: Hiya! (Just kidding, I'll use the more conventiona\n",
      "\n",
      "ðŸ” Initializing search tool...\n",
      "   Testing search connection...\n",
      "   âœ… Search is working! Got 783 characters\n",
      "\n",
      "âœ… Components initialized!\n"
     ]
    }
   ],
   "source": [
    "# Initialize the LLM\n",
    "# Change model if you have a different one: \"mistral:7b\", \"gemma:7b\", etc.\n",
    "MODEL_NAME = \"llama3.1:8b\"  # You can change this to any model you have\n",
    "\n",
    "print(f\"ðŸ¤– Initializing LLM with model: {MODEL_NAME}\")\n",
    "llm = OllamaLLM(model=MODEL_NAME)  # Using updated import from langchain_ollama\n",
    "\n",
    "# Test the LLM (optional - this will take a moment)\n",
    "print(\"   Testing LLM connection...\")\n",
    "try:\n",
    "    test_response = llm.invoke(\"Say 'Hello' in one word.\")\n",
    "    print(f\"   âœ… LLM is working! Response: {test_response[:50]}\")\n",
    "except Exception as e:\n",
    "    print(f\"   âŒ LLM error: {e}\")\n",
    "    print(\"   ðŸ’¡ Make sure Ollama is running and model is pulled:\")\n",
    "    print(f\"      ollama pull {MODEL_NAME}\")\n",
    "\n",
    "print(\"\\nðŸ” Initializing search tool...\")\n",
    "search = DuckDuckGoSearchRun()\n",
    "\n",
    "# Test the search tool\n",
    "print(\"   Testing search connection...\")\n",
    "try:\n",
    "    test_search = search.run(\"test\")\n",
    "    print(f\"   âœ… Search is working! Got {len(test_search)} characters\")\n",
    "except Exception as e:\n",
    "    print(f\"   âŒ Search error: {e}\")\n",
    "    print(\"   ðŸ’¡ Check your internet connection\")\n",
    "\n",
    "print(\"\\nâœ… Components initialized!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create the Prompt Template\n",
    "\n",
    "The prompt template is the \"instruction manual\" we give to our LLM. It tells the LLM:\n",
    "- What role to play (helpful assistant)\n",
    "- What data to use (search results only)\n",
    "- What to do if information is missing\n",
    "\n",
    "### Prompt Engineering Tips:\n",
    "- âœ… Be explicit about using only search results\n",
    "- âœ… Specify what to do when information is missing\n",
    "- âœ… Use clear structure (context, then question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Prompt template created!\n",
      "\n",
      "ðŸ“ Template structure:\n",
      "   â€¢ Role: Helpful AI assistant\n",
      "   â€¢ Instruction: Use only search results\n",
      "   â€¢ Placeholders: {context} and {question}\n",
      "\n",
      "ðŸ’¡ You can modify this template to change the assistant's behavior!\n"
     ]
    }
   ],
   "source": [
    "# Create the prompt template\n",
    "# Notice the placeholders: {context} and {question}\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are a helpful AI assistant. You must answer the user's question \n",
    "    based *only* on the following search results. If the search results \n",
    "    are empty or do not contain the answer, say 'I could not find \n",
    "    any information on that.'\n",
    "\n",
    "    Search Results:\n",
    "    {context}\n",
    "\n",
    "    Question:\n",
    "    {question}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "print(\"âœ… Prompt template created!\")\n",
    "print(\"\\nðŸ“ Template structure:\")\n",
    "print(\"   â€¢ Role: Helpful AI assistant\")\n",
    "print(\"   â€¢ Instruction: Use only search results\")\n",
    "print(\"   â€¢ Placeholders: {context} and {question}\")\n",
    "print(\"\\nðŸ’¡ You can modify this template to change the assistant's behavior!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Build the RAG Chain with LCEL\n",
    "\n",
    "This is the **KEY PART** of our real-time AI Assistant! \n",
    "\n",
    "### Understanding the Chain\n",
    "\n",
    "The chain uses LCEL (LangChain Expression Language) with the pipe operator (`|`).\n",
    "\n",
    "**Data Flow:**\n",
    "1. **Input**: `{\"question\": \"What is the weather today?\"}`\n",
    "2. **RunnablePassthrough.assign**: \n",
    "   - Keeps original input\n",
    "   - Adds `context` key by running search\n",
    "   - Result: `{\"question\": \"...\", \"context\": \"search results...\"}`\n",
    "3. **Prompt**: Formats the template with context and question\n",
    "4. **LLM**: Generates response based on formatted prompt\n",
    "\n",
    "### Visual Flow:\n",
    "```\n",
    "Input: {\"question\": \"...\"}\n",
    "  â†“\n",
    "RunnablePassthrough.assign\n",
    "  â†’ Search runs with question\n",
    "  â†’ Adds \"context\" key\n",
    "  â†“\n",
    "{\"question\": \"...\", \"context\": \"search results...\"}\n",
    "  â†“\n",
    "Prompt Template\n",
    "  â†’ Formats: \"Search Results: {context}\\nQuestion: {question}\"\n",
    "  â†“\n",
    "LLM\n",
    "  â†’ Generates answer\n",
    "  â†“\n",
    "Output: \"Answer text...\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”— Building RAG chain...\n",
      "âœ… RAG chain built!\n",
      "\n",
      "ðŸ” Chain structure:\n",
      "   1. RunnablePassthrough.assign â†’ Adds 'context' via search\n",
      "   2. prompt â†’ Formats template with context and question\n",
      "   3. llm â†’ Generates response\n",
      "\n",
      "ðŸ’¡ The pipe operator (|) chains these steps together!\n"
     ]
    }
   ],
   "source": [
    "# Build the RAG chain\n",
    "# This is where the magic happens!\n",
    "\n",
    "print(\"ðŸ”— Building RAG chain...\")\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        # \"context\" is a new key we add to the dictionary.\n",
    "        # Its value is the *output* of running the 'search' tool\n",
    "        # with the original 'question' as input.\n",
    "        context=lambda x: search.run(x[\"question\"])\n",
    "    )\n",
    "    | prompt  # The dictionary (now with 'context' and 'question') is \"piped\" into the prompt\n",
    "    | llm     # The formatted prompt is \"piped\" into the LLM\n",
    ")\n",
    "\n",
    "print(\"âœ… RAG chain built!\")\n",
    "print(\"\\nðŸ” Chain structure:\")\n",
    "print(\"   1. RunnablePassthrough.assign â†’ Adds 'context' via search\")\n",
    "print(\"   2. prompt â†’ Formats template with context and question\")\n",
    "print(\"   3. llm â†’ Generates response\")\n",
    "print(\"\\nðŸ’¡ The pipe operator (|) chains these steps together!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Test Your First Query! ðŸŽ‰\n",
    "\n",
    "Now let's test the chain with a simple question. This will:\n",
    "1. Search the web for your question\n",
    "2. Format the prompt with search results\n",
    "3. Generate an answer using the LLM\n",
    "\n",
    "**Note**: The first run may take longer as the model loads.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– Question: What is Python programming language?\n",
      "ðŸ¤– Thinking... (this may take 10-30 seconds)\n",
      "\n",
      "============================================================\n",
      "ðŸ¤– Response:\n",
      "============================================================\n",
      "Python is a high-level, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation.\n",
      "============================================================\n",
      "\n",
      "âœ… Success! Got response with 152 characters\n"
     ]
    }
   ],
   "source": [
    "# Test the chain with a simple question\n",
    "test_question = \"What is Python programming language?\"\n",
    "\n",
    "print(f\"ðŸ¤– Question: {test_question}\")\n",
    "print(\"ðŸ¤– Thinking... (this may take 10-30 seconds)\\n\")\n",
    "\n",
    "try:\n",
    "    # This one line runs the whole RAG process!\n",
    "    response = chain.invoke({\"question\": test_question})\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"ðŸ¤– Response:\")\n",
    "    print(\"=\"*60)\n",
    "    print(response)\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\nâœ… Success! Got response with {len(response)} characters\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error: {e}\")\n",
    "    print(\"\\nðŸ’¡ Troubleshooting:\")\n",
    "    print(\"   1. Make sure Ollama is running: ollama serve\")\n",
    "    print(f\"   2. Verify model is available: ollama list\")\n",
    "    print(\"   3. Check your internet connection for search\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Understanding What Happened\n",
    "\n",
    "Let's break down what just happened step by step to understand the RAG process:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Debugging the RAG chain step by step:\n",
      "\n",
      "============================================================\n",
      "Step 1: Original Input\n",
      "   Input: {'question': 'What is machine learning?'}\n",
      "\n",
      "Step 2: Running Search...\n",
      "   Search returned 731 characters\n",
      "   Preview: Machine learning ( ML ) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can ... The central premise of machine learning (ML) is t...\n",
      "\n",
      "Step 3: Adding Context\n",
      "   Data now has: ['question', 'context']\n",
      "\n",
      "Step 4: Formatting Prompt\n",
      "   Formatted prompt length: 1052 characters\n",
      "   Preview:\n",
      "Human: You are a helpful AI assistant. You must answer the user's question \n",
      "    based *only* on the following search results. If the search results \n",
      "    are empty or do not contain the answer, say 'I could not find \n",
      "    any information on that.'\n",
      "\n",
      "    Search Results:\n",
      "    Machine learning ( ML ) is a ...\n",
      "\n",
      "Step 5: Generating Response with LLM...\n",
      "   Response length: 456 characters\n",
      "   Response: Machine learning is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data, with its central premise being that if you ...\n",
      "\n",
      "============================================================\n",
      "âœ… This is exactly what the chain does automatically!\n"
     ]
    }
   ],
   "source": [
    "# Let's see what happens at each step\n",
    "debug_question = \"What is machine learning?\"\n",
    "\n",
    "print(\"ðŸ” Debugging the RAG chain step by step:\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Step 1: Original input\n",
    "print(\"Step 1: Original Input\")\n",
    "input_data = {\"question\": debug_question}\n",
    "print(f\"   Input: {input_data}\\n\")\n",
    "\n",
    "# Step 2: Search (this is what RunnablePassthrough.assign does)\n",
    "print(\"Step 2: Running Search...\")\n",
    "search_results = search.run(debug_question)\n",
    "print(f\"   Search returned {len(search_results)} characters\")\n",
    "print(f\"   Preview: {search_results[:200]}...\\n\")\n",
    "\n",
    "# Step 3: Add context (what RunnablePassthrough.assign does)\n",
    "print(\"Step 3: Adding Context\")\n",
    "data_with_context = {\n",
    "    \"question\": debug_question,\n",
    "    \"context\": search_results\n",
    "}\n",
    "print(f\"   Data now has: {list(data_with_context.keys())}\\n\")\n",
    "\n",
    "# Step 4: Format prompt\n",
    "print(\"Step 4: Formatting Prompt\")\n",
    "# Use format_prompt() which returns ChatPromptValue, then convert to string\n",
    "formatted_prompt = prompt.format_prompt(**data_with_context)\n",
    "prompt_str = formatted_prompt.to_string()\n",
    "print(f\"   Formatted prompt length: {len(prompt_str)} characters\")\n",
    "print(f\"   Preview:\\n{prompt_str[:300]}...\\n\")\n",
    "\n",
    "# Step 5: Generate response\n",
    "print(\"Step 5: Generating Response with LLM...\")\n",
    "# Ollama expects a string input\n",
    "response = llm.invoke(prompt_str)\n",
    "print(f\"   Response length: {len(response)} characters\")\n",
    "print(f\"   Response: {response[:200]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… This is exactly what the chain does automatically!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Try Different Questions\n",
    "\n",
    "Now that you understand how it works, try asking different types of questions:\n",
    "\n",
    "- **Current events**: \"What's the latest news about AI?\"\n",
    "- **General knowledge**: \"What is RAG?\"\n",
    "- **Real-time info**: \"What is the weather today?\"\n",
    "- **Technical questions**: \"How does LangChain work?\"\n",
    "\n",
    "Experiment with different questions to see how the assistant responds!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– Question: What is the latest news about artificial intelligence?\n",
      "ðŸ¤– Thinking...\n",
      "\n",
      "============================================================\n",
      "ðŸ¤– Response:\n",
      "============================================================\n",
      "I could not find any information on the specific latest news about artificial intelligence in the search results provided. However, there are links to \"News coverage on artificial intelligence\" and articles from The Guardian that may contain relevant information.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Try your own questions here!\n",
    "# Modify the question variable and run the cell\n",
    "\n",
    "your_question = \"What is the latest news about artificial intelligence?\"\n",
    "\n",
    "print(f\"ðŸ¤– Question: {your_question}\")\n",
    "print(\"ðŸ¤– Thinking...\\n\")\n",
    "\n",
    "try:\n",
    "    response = chain.invoke({\"question\": your_question})\n",
    "    print(\"=\"*60)\n",
    "    print(\"ðŸ¤– Response:\")\n",
    "    print(\"=\"*60)\n",
    "    print(response)\n",
    "    print(\"=\"*60)\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error: {e}\")\n",
    "\n",
    "# ðŸ’¡ Try different questions:\n",
    "# - \"What is the weather today?\"\n",
    "# - \"Explain RAG in simple terms\"\n",
    "# - \"What are the latest developments in Python?\"\n",
    "# - \"Who is the current president of the USA?\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Create an Interactive Assistant\n",
    "\n",
    "Now let's create a reusable function that makes it easy to ask questions. This is the foundation for building a more advanced assistant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the assistant function:\n",
      "\n",
      "ðŸ¤– Question: What is LangChain?\n",
      "ðŸ¤– Thinking...\n",
      "\n",
      "============================================================\n",
      "ðŸ¤– Response:\n",
      "============================================================\n",
      "LangChain is an open-source framework designed to simplify the creation of applications using large language models (LLMs). It provides a standard interface for handling tasks that would otherwise require repetitive coding.\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'LangChain is an open-source framework designed to simplify the creation of applications using large language models (LLMs). It provides a standard interface for handling tasks that would otherwise require repetitive coding.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a simple assistant function\n",
    "def ask_assistant(question):\n",
    "    \"\"\"\n",
    "    Ask a question to the assistant.\n",
    "    \n",
    "    Args:\n",
    "        question (str): The question to ask\n",
    "        \n",
    "    Returns:\n",
    "        str: The assistant's response\n",
    "    \"\"\"\n",
    "    print(f\"ðŸ¤– Question: {question}\")\n",
    "    print(\"ðŸ¤– Thinking...\\n\")\n",
    "    \n",
    "    try:\n",
    "        response = chain.invoke({\"question\": question})\n",
    "        print(\"=\"*60)\n",
    "        print(\"ðŸ¤– Response:\")\n",
    "        print(\"=\"*60)\n",
    "        print(response)\n",
    "        print(\"=\"*60)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        error_msg = f\"âŒ An error occurred: {e}\"\n",
    "        print(error_msg)\n",
    "        return error_msg\n",
    "\n",
    "# Test the function\n",
    "print(\"Testing the assistant function:\\n\")\n",
    "ask_assistant(\"What is LangChain?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸš€ Advanced Improvements\n",
    "\n",
    "Now that you understand the basics, let's add advanced features to make the assistant production-ready!\n",
    "\n",
    "### Improvement 1: Add Conversation Memory\n",
    "\n",
    "**Problem**: The assistant doesn't remember previous conversation.\n",
    "\n",
    "**Solution**: Use LangChain's memory components to remember context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Memory initialized!\n",
      "\n",
      "ðŸ’¡ Memory will store:\n",
      "   â€¢ Previous questions\n",
      "   â€¢ Previous responses\n",
      "   â€¢ Context for follow-up questions\n",
      "\n",
      "ðŸ“ Memory contains 2 messages\n",
      "   1. HumanMessage: My name is Alice...\n",
      "   2. AIMessage: Nice to meet you, Alice!...\n"
     ]
    }
   ],
   "source": [
    "# Improvement 1: Add Conversation Memory\n",
    "# LangChain 1.x uses chat history instead of the old memory module\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# Create a simple memory wrapper for compatibility\n",
    "class ConversationBufferMemory:\n",
    "    \"\"\"Simple memory wrapper for LangChain 1.x compatibility\"\"\"\n",
    "    def __init__(self, memory_key=\"chat_history\", return_messages=True, max_token_limit=1000):\n",
    "        self.memory_key = memory_key\n",
    "        self.return_messages = return_messages\n",
    "        self.max_token_limit = max_token_limit\n",
    "        self.chat_memory = InMemoryChatMessageHistory()\n",
    "    \n",
    "    def save_context(self, inputs, outputs):\n",
    "        \"\"\"Save input and output to memory\"\"\"\n",
    "        if isinstance(inputs, dict):\n",
    "            input_msg = inputs.get('input', '')\n",
    "        else:\n",
    "            input_msg = str(inputs)\n",
    "        \n",
    "        if isinstance(outputs, dict):\n",
    "            output_msg = outputs.get('output', '')\n",
    "        else:\n",
    "            output_msg = str(outputs)\n",
    "        \n",
    "        self.chat_memory.add_user_message(input_msg)\n",
    "        self.chat_memory.add_ai_message(output_msg)\n",
    "    \n",
    "    @property\n",
    "    def messages(self):\n",
    "        \"\"\"Get messages as a list\"\"\"\n",
    "        return self.chat_memory.messages\n",
    "\n",
    "# Create memory\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    "    max_token_limit=1000  # Limit memory size\n",
    ")\n",
    "\n",
    "print(\"âœ… Memory initialized!\")\n",
    "print(\"\\nðŸ’¡ Memory will store:\")\n",
    "print(\"   â€¢ Previous questions\")\n",
    "print(\"   â€¢ Previous responses\")\n",
    "print(\"   â€¢ Context for follow-up questions\")\n",
    "\n",
    "# Example: Store a conversation\n",
    "memory.save_context(\n",
    "    {\"input\": \"My name is Alice\"},\n",
    "    {\"output\": \"Nice to meet you, Alice!\"}\n",
    ")\n",
    "\n",
    "# Retrieve memory\n",
    "history = memory.chat_memory.messages\n",
    "print(f\"\\nðŸ“ Memory contains {len(history)} messages\")\n",
    "for i, msg in enumerate(history):\n",
    "    # Get message type and content\n",
    "    msg_type = type(msg).__name__\n",
    "    msg_content = msg.content if hasattr(msg, 'content') else str(msg)\n",
    "    print(f\"   {i+1}. {msg_type}: {msg_content[:50]}...\")\n",
    "\n",
    "# Note: To use memory in the chain, you'd need to modify the prompt\n",
    "# to include chat_history. This is shown in the markdown guide.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvement 2: Add Result Caching\n",
    "\n",
    "**Problem**: Same questions trigger repeated searches (slow and wasteful).\n",
    "\n",
    "**Solution**: Cache search results and responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First call (will search):\n",
      "ðŸ” Searching and generating response...\n",
      "Response length: 308 characters\n",
      "\n",
      "Second call (will use cache):\n",
      "ðŸ’¾ Using cached response\n",
      "Response length: 308 characters\n",
      "\n",
      "âœ… Cache working! Cache size: 1 entries\n"
     ]
    }
   ],
   "source": [
    "# Improvement 2: Add Result Caching\n",
    "import hashlib\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Simple cache implementation\n",
    "cache = {}\n",
    "CACHE_TTL = timedelta(hours=1)  # Cache for 1 hour\n",
    "\n",
    "def get_cache_key(question: str) -> str:\n",
    "    \"\"\"Generate cache key for question.\"\"\"\n",
    "    return hashlib.md5(question.lower().strip().encode()).hexdigest()\n",
    "\n",
    "def is_cache_valid(cache_entry: dict) -> bool:\n",
    "    \"\"\"Check if cache entry is still valid.\"\"\"\n",
    "    cache_time = datetime.fromisoformat(cache_entry[\"timestamp\"])\n",
    "    return datetime.now() - cache_time < CACHE_TTL\n",
    "\n",
    "def ask_with_cache(question: str) -> str:\n",
    "    \"\"\"Ask with caching.\"\"\"\n",
    "    cache_key = get_cache_key(question)\n",
    "    \n",
    "    # Check cache\n",
    "    if cache_key in cache:\n",
    "        entry = cache[cache_key]\n",
    "        if is_cache_valid(entry):\n",
    "            print(\"ðŸ’¾ Using cached response\")\n",
    "            return entry[\"response\"]\n",
    "        else:\n",
    "            # Remove expired entry\n",
    "            del cache[cache_key]\n",
    "    \n",
    "    # Generate new response\n",
    "    print(\"ðŸ” Searching and generating response...\")\n",
    "    response = chain.invoke({\"question\": question})\n",
    "    \n",
    "    # Store in cache\n",
    "    cache[cache_key] = {\n",
    "        \"response\": response,\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Test caching\n",
    "print(\"First call (will search):\")\n",
    "response1 = ask_with_cache(\"What is Python?\")\n",
    "print(f\"Response length: {len(response1)} characters\\n\")\n",
    "\n",
    "print(\"Second call (will use cache):\")\n",
    "response2 = ask_with_cache(\"What is Python?\")\n",
    "print(f\"Response length: {len(response2)} characters\\n\")\n",
    "\n",
    "print(f\"âœ… Cache working! Cache size: {len(cache)} entries\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvement 3: Add Streaming Responses\n",
    "\n",
    "**Problem**: User waits for complete response (poor UX).\n",
    "\n",
    "**Solution**: Stream tokens as they're generated for real-time feedback.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing streaming response:\n",
      "\n",
      "ðŸ¤– Question: Explain RAG in one sentence.\n",
      "ðŸ¤– Response (streaming): Retriever-Augmented Generation (RAG) is a hybrid model that integrates retrieval of relevant information with generation.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Improvement 3: Add Streaming Responses\n",
    "def ask_streaming(question: str):\n",
    "    \"\"\"Stream the response token by token.\"\"\"\n",
    "    try:\n",
    "        print(f\"ðŸ¤– Question: {question}\")\n",
    "        print(\"ðŸ¤– Response (streaming): \", end=\"\", flush=True)\n",
    "        \n",
    "        # Stream response\n",
    "        for chunk in chain.stream({\"question\": question}):\n",
    "            print(chunk, end=\"\", flush=True)\n",
    "        \n",
    "        print(\"\\n\")  # New line after response\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ Error: {e}\")\n",
    "\n",
    "# Test streaming\n",
    "# Note: Streaming works best in terminal, but you'll see it here too\n",
    "print(\"Testing streaming response:\\n\")\n",
    "ask_streaming(\"Explain RAG in one sentence.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvement 4: Add Error Handling and Retries\n",
    "\n",
    "**Problem**: Network errors cause failures.\n",
    "\n",
    "**Solution**: Implement retry logic with exponential backoff.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing error handling with retry logic:\n",
      "\n",
      "Response: Python is a general-purpose, object-oriented programming language. It has several implications that ...\n",
      "\n",
      "âœ… Error handling implemented!\n"
     ]
    }
   ],
   "source": [
    "# Improvement 4: Add Error Handling and Retries\n",
    "import time\n",
    "\n",
    "def ask_with_retry(question: str, max_retries: int = 3) -> str:\n",
    "    \"\"\"Ask with retry logic.\"\"\"\n",
    "    last_error = None\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = chain.invoke({\"question\": question})\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            last_error = e\n",
    "            \n",
    "            if attempt < max_retries - 1:\n",
    "                wait_time = 2 ** attempt  # Exponential backoff: 1s, 2s, 4s\n",
    "                print(f\"âš ï¸ Error occurred. Retrying in {wait_time}s... (Attempt {attempt + 1}/{max_retries})\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                return f\"âŒ Failed after {max_retries} attempts: {str(e)}\"\n",
    "    \n",
    "    return f\"âŒ Unexpected error: {last_error}\"\n",
    "\n",
    "# Test error handling\n",
    "print(\"Testing error handling with retry logic:\\n\")\n",
    "response = ask_with_retry(\"What is Python?\")\n",
    "print(f\"Response: {response[:100]}...\")\n",
    "print(\"\\nâœ… Error handling implemented!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvement 5: Add Source Citations\n",
    "\n",
    "**Problem**: No way to verify information sources.\n",
    "\n",
    "**Solution**: Track and display sources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing source citations:\n",
      "\n",
      "Python is a computer programming language often used to build websites and software, automate tasks, and conduct data analysis.\n",
      "\n",
      "ðŸ“š Sources:\n",
      "  â€¢ DuckDuckGo Search: What is Python?\n"
     ]
    }
   ],
   "source": [
    "# Improvement 5: Add Source Citations\n",
    "def ask_with_sources(question: str) -> tuple[str, list[str]]:\n",
    "    \"\"\"Ask with source tracking.\"\"\"\n",
    "    # Perform search\n",
    "    search_results = search.run(question)\n",
    "    \n",
    "    # Store source\n",
    "    sources = [f\"DuckDuckGo Search: {question}\"]\n",
    "    \n",
    "    # Generate response\n",
    "    response = chain.invoke({\n",
    "        \"question\": question,\n",
    "        \"context\": search_results\n",
    "    })\n",
    "    \n",
    "    return response, sources\n",
    "\n",
    "def format_response_with_sources(question: str) -> str:\n",
    "    \"\"\"Ask and format response with sources.\"\"\"\n",
    "    response, sources = ask_with_sources(question)\n",
    "    \n",
    "    # Format with sources\n",
    "    formatted = f\"{response}\\n\\nðŸ“š Sources:\\n\"\n",
    "    formatted += \"\\n\".join(f\"  â€¢ {s}\" for s in sources)\n",
    "    \n",
    "    return formatted\n",
    "\n",
    "# Test source citations\n",
    "print(\"Testing source citations:\\n\")\n",
    "result = format_response_with_sources(\"What is Python?\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ§ª Testing Your Assistant\n",
    "\n",
    "Let's create some test cases to validate your assistant works correctly:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ðŸ§ª RUNNING TEST SUITE\n",
      "============================================================\n",
      "\n",
      "ðŸ§ª Test 1: Basic Functionality\n",
      "ðŸ¤– Question: What is Python?\n",
      "ðŸ¤– Thinking...\n",
      "\n",
      "============================================================\n",
      "ðŸ¤– Response:\n",
      "============================================================\n",
      "Python is one of the most popular programming languages. Itâ€™s simple to use, packed with features and supported by a wide range of libraries and frameworks. It is a recent, general-purpose and high level programming language. It is a popular programming language. It was created by Guido van Rossum, and released in 1991.\n",
      "============================================================\n",
      "âœ… Test 1 PASSED\n",
      "\n",
      "ðŸ§ª Test 2: Real-Time Information\n",
      "ðŸ¤– Question: What is the current year?\n",
      "ðŸ¤– Thinking...\n",
      "\n",
      "============================================================\n",
      "ðŸ¤– Response:\n",
      "============================================================\n",
      "According to the search results, the answer is: 2025.\n",
      "============================================================\n",
      "âœ… Test 2 PASSED: Found current year\n",
      "\n",
      "ðŸ§ª Test 3: Different Question Types\n",
      "ðŸ¤– Question: What is machine learning?\n",
      "ðŸ¤– Thinking...\n",
      "\n",
      "============================================================\n",
      "ðŸ¤– Response:\n",
      "============================================================\n",
      "Machine learning is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalise to unseen data, and thus perform tasks without explicit instructions. It is also described as \"a subfield of AI that uses algorithms trained on data sets to create self-learning models\" or \"the subset of AI focused on algorithms that analyze and â€œlearnâ€ the patterns of training data in order to make accurate inferences about new data.\"\n",
      "============================================================\n",
      "   âœ… Answered: What is machine learning?...\n",
      "ðŸ¤– Question: How does RAG work?\n",
      "ðŸ¤– Thinking...\n",
      "\n",
      "============================================================\n",
      "ðŸ¤– Response:\n",
      "============================================================\n",
      "RAG works by combining two components: a Retriever (a search engine that retrieves relevant context from an external knowledge base) and a Generator (an LLM that uses the retrieved context as part of its input prompt to generate answers). This process is described in more detail in various sources, including a guide on how Simple RAG works with Python code implementation.\n",
      "============================================================\n",
      "   âœ… Answered: How does RAG work?...\n",
      "ðŸ¤– Question: Why is Python popular?\n",
      "ðŸ¤– Thinking...\n",
      "\n",
      "============================================================\n",
      "ðŸ¤– Response:\n",
      "============================================================\n",
      "According to the search results, Python is popular due to its:\n",
      "\n",
      "* Simplicity\n",
      "* Vast libraries\n",
      "* Strong community\n",
      "* Its versatility and high-level nature\n",
      "* Holding the #1 position as the most popular programming language (TIOBE Index 2025)\n",
      "* Being the most loved and wanted language for the sixth consecutive year (Stack Overflow survey 2025)\n",
      "============================================================\n",
      "   âœ… Answered: Why is Python popular?...\n",
      "âœ… Test 3 PASSED\n",
      "\n",
      "ðŸ§ª Test 4: Performance\n",
      "ðŸ¤– Question: What is Python?\n",
      "ðŸ¤– Thinking...\n",
      "\n",
      "============================================================\n",
      "ðŸ¤– Response:\n",
      "============================================================\n",
      "According to the search results, Python is a general-purpose, object-oriented programming language. \n",
      "\n",
      "Additionally, it's mentioned that \"Python is a general-purpose, object-oriented programming language that has several implications...\" and also described as \"What is Python ? ... this sense (what do typical implementations do), Python is...\".\n",
      "============================================================\n",
      "   Response time: 59.53 seconds\n",
      "   Response length: 344 characters\n",
      "âœ… Test 4 PASSED\n",
      "\n",
      "============================================================\n",
      "ðŸ“Š Results: 4/4 tests passed\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Test Suite\n",
    "import time\n",
    "\n",
    "def test_basic_functionality():\n",
    "    \"\"\"Test 1: Basic Functionality\"\"\"\n",
    "    print(\"ðŸ§ª Test 1: Basic Functionality\")\n",
    "    response = ask_assistant(\"What is Python?\")\n",
    "    assert len(response) > 0, \"Response should not be empty\"\n",
    "    print(\"âœ… Test 1 PASSED\\n\")\n",
    "    return True\n",
    "\n",
    "def test_realtime_information():\n",
    "    \"\"\"Test 2: Real-Time Information\"\"\"\n",
    "    print(\"ðŸ§ª Test 2: Real-Time Information\")\n",
    "    response = ask_assistant(\"What is the current year?\")\n",
    "    # Check for recent years\n",
    "    current_years = [\"2024\", \"2025\", \"2026\"]\n",
    "    found = any(year in response for year in current_years)\n",
    "    if found:\n",
    "        print(\"âœ… Test 2 PASSED: Found current year\\n\")\n",
    "    else:\n",
    "        print(\"âš ï¸ Test 2 WARNING: Current year not explicitly found\\n\")\n",
    "    return True\n",
    "\n",
    "def test_different_question_types():\n",
    "    \"\"\"Test 3: Different Question Types\"\"\"\n",
    "    print(\"ðŸ§ª Test 3: Different Question Types\")\n",
    "    questions = [\n",
    "        \"What is machine learning?\",\n",
    "        \"How does RAG work?\",\n",
    "        \"Why is Python popular?\"\n",
    "    ]\n",
    "    \n",
    "    for question in questions:\n",
    "        response = ask_assistant(question)\n",
    "        assert len(response) > 0, f\"Should get response for: {question}\"\n",
    "        print(f\"   âœ… Answered: {question[:30]}...\")\n",
    "    \n",
    "    print(\"âœ… Test 3 PASSED\\n\")\n",
    "    return True\n",
    "\n",
    "def test_performance():\n",
    "    \"\"\"Test 4: Performance\"\"\"\n",
    "    print(\"ðŸ§ª Test 4: Performance\")\n",
    "    start = time.time()\n",
    "    response = ask_assistant(\"What is Python?\")\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    print(f\"   Response time: {elapsed:.2f} seconds\")\n",
    "    print(f\"   Response length: {len(response)} characters\")\n",
    "    \n",
    "    if elapsed < 60:  # Should respond within 60 seconds\n",
    "        print(\"âœ… Test 4 PASSED\\n\")\n",
    "    else:\n",
    "        print(\"âš ï¸ Test 4 WARNING: Response took longer than expected\\n\")\n",
    "    return True\n",
    "\n",
    "# Run tests\n",
    "print(\"=\"*60)\n",
    "print(\"ðŸ§ª RUNNING TEST SUITE\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "\n",
    "tests = [\n",
    "    test_basic_functionality,\n",
    "    test_realtime_information,\n",
    "    test_different_question_types,\n",
    "    test_performance\n",
    "]\n",
    "\n",
    "passed = 0\n",
    "for test in tests:\n",
    "    try:\n",
    "        if test():\n",
    "            passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Test failed: {e}\\n\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"ðŸ“Š Results: {passed}/{len(tests)} tests passed\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“š Learning Exercises\n",
    "\n",
    "Try these exercises to deepen your understanding:\n",
    "\n",
    "### Exercise 1: Modify the Prompt\n",
    "\n",
    "Try different prompt styles and see how they affect responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing different prompt styles:\n",
      "\n",
      "Style 1:\n",
      "   Response: RAG stands for Retrieval-Augmented Generation. It's an advanced AI framework that combines information retrieval with text generation models, like GPT...\n",
      "\n",
      "Style 2:\n",
      "   Response: Based on the provided information, I can provide a detailed explanation of what Retrieval-Augmented Generation (RAG) is:\n",
      "\n",
      "**Definition:** RAG is an ar...\n",
      "\n",
      "Style 3:\n",
      "   Response: Based on the text, here are the key points about RAG:\n",
      "\n",
      "1. **Definition**: Retrieval-Augmented Generation (RAG) is an advanced AI framework that combin...\n",
      "\n",
      "ðŸ’¡ Notice how different prompts affect the response style!\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1: Try Different Prompt Styles\n",
    "\n",
    "# Style 1: Direct and concise\n",
    "prompt_style1 = ChatPromptTemplate.from_template(\n",
    "    \"Answer this: {question}\\n\\nContext: {context}\"\n",
    ")\n",
    "\n",
    "# Style 2: Detailed and structured\n",
    "prompt_style2 = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are an expert assistant. Use the following information to answer.\n",
    "\n",
    "    Information:\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    Provide a detailed answer:\"\"\"\n",
    ")\n",
    "\n",
    "# Style 3: Question-answer format\n",
    "prompt_style3 = ChatPromptTemplate.from_template(\n",
    "    \"Q: {question}\\nA (based on: {context}):\"\n",
    ")\n",
    "\n",
    "# Test different prompts\n",
    "test_question = \"What is RAG?\"\n",
    "\n",
    "print(\"Testing different prompt styles:\\n\")\n",
    "\n",
    "for i, prompt_style in enumerate([prompt_style1, prompt_style2, prompt_style3], 1):\n",
    "    print(f\"Style {i}:\")\n",
    "    chain_style = (\n",
    "        RunnablePassthrough.assign(\n",
    "            context=lambda x: search.run(x[\"question\"])\n",
    "        )\n",
    "        | prompt_style\n",
    "        | llm\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        response = chain_style.invoke({\"question\": test_question})\n",
    "        print(f\"   Response: {response[:150]}...\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"   Error: {e}\\n\")\n",
    "\n",
    "print(\"ðŸ’¡ Notice how different prompts affect the response style!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Compare Different Models\n",
    "\n",
    "If you have multiple models, compare their responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ¤– Testing model: qwen3:0.6b\n",
      "   Response: Python is a high-level, interpreted programming language known for its readable syntax. Created by Guido van Rossum in 1991, it was designed to emphasize code readability and simplicity. It's used for...\n",
      "\n",
      "ðŸ¤– Testing model: mistral:7b\n",
      "   Response:  Python is a versatile programming language used in web development, data science, automation, and AI. It is known for its readable syntax and powerful libraries, and it's favored for tasks like machi...\n",
      "\n",
      "ðŸ¤– Testing model: gemma3:latest\n",
      "   Response: Python is a high-level, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation. Python is dynamically type-checked and garbage-...\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2: Compare Different Models\n",
    "# Uncomment and modify if you have multiple models\n",
    "\n",
    "models_to_test = [\"qwen3:0.6b\", \"mistral:7b\", \"gemma3:latest\"]\n",
    "\n",
    "test_question = \"What is Python?\"\n",
    "\n",
    "for model_name in models_to_test:\n",
    "    try:\n",
    "        print(f\"\\nðŸ¤– Testing model: {model_name}\")\n",
    "        model_llm = OllamaLLM(model=model_name)  # Using updated import\n",
    "        \n",
    "        chain_model = (\n",
    "            RunnablePassthrough.assign(\n",
    "                context=lambda x: search.run(x[\"question\"])\n",
    "            )\n",
    "            | prompt\n",
    "            | model_llm\n",
    "        )\n",
    "        \n",
    "        response = chain_model.invoke({\"question\": test_question})\n",
    "        print(f\"   Response: {response[:200]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Error with {model_name}: {e}\")\n",
    "\n",
    "# print(\"ðŸ’¡ Uncomment the code above and add your models to compare!\")\n",
    "# print(\"   To get more models: ollama pull mistral:7b\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Measure Performance\n",
    "\n",
    "Track how long different operations take.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Performance Analysis:\n",
      "\n",
      "Question | Search Time | LLM Time | Total Time | Response Length\n",
      "----------------------------------------------------------------------\n",
      "What is Python?           |       1.42s |    31.84s |     33.26s |            225 chars\n",
      "What is machine learning? |       2.59s |    20.75s |     23.34s |            240 chars\n",
      "What is RAG?              |       1.46s |    28.01s |     29.47s |            213 chars\n",
      "\n",
      "ðŸ’¡ Use this to identify bottlenecks!\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3: Measure Performance\n",
    "import time\n",
    "\n",
    "questions = [\n",
    "    \"What is Python?\",\n",
    "    \"What is machine learning?\",\n",
    "    \"What is RAG?\"\n",
    "]\n",
    "\n",
    "print(\"ðŸ“Š Performance Analysis:\\n\")\n",
    "print(\"Question | Search Time | LLM Time | Total Time | Response Length\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for question in questions:\n",
    "    # Measure search time\n",
    "    search_start = time.time()\n",
    "    search_results = search.run(question)\n",
    "    search_time = time.time() - search_start\n",
    "    \n",
    "    # Measure LLM time\n",
    "    llm_start = time.time()\n",
    "    # Use format_prompt() and convert to string for Ollama\n",
    "    formatted = prompt.format_prompt(context=search_results, question=question)\n",
    "    prompt_str = formatted.to_string()\n",
    "    response = llm.invoke(prompt_str)\n",
    "    llm_time = time.time() - llm_start\n",
    "    \n",
    "    total_time = search_time + llm_time\n",
    "    \n",
    "    print(f\"{question[:25]:<25} | {search_time:>10.2f}s | {llm_time:>8.2f}s | {total_time:>9.2f}s | {len(response):>14} chars\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Use this to identify bottlenecks!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ“ Summary & Next Steps\n",
    "\n",
    "### What You've Learned\n",
    "\n",
    "âœ… **RAG Concepts**: Retrieval-Augmented Generation combines search with LLM generation  \n",
    "âœ… **LangChain**: Framework for building LLM applications  \n",
    "âœ… **LCEL**: Pythonic way to chain operations using pipe operator  \n",
    "âœ… **Real-Time Search**: Using DuckDuckGo for current information  \n",
    "âœ… **Advanced Features**: Memory, caching, streaming, error handling  \n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **RAG Flow**: Question â†’ Search â†’ Context + Question â†’ LLM â†’ Answer\n",
    "2. **LCEL Syntax**: `chain = step1 | step2 | step3`\n",
    "3. **RunnablePassthrough.assign**: Adds new keys to data while passing through\n",
    "4. **Prompt Engineering**: Clear instructions improve responses\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Extend the Assistant**: Add your own improvements\n",
    "2. **Integrate Local Docs**: Combine web search with your documents (see markdown guide)\n",
    "3. **Deploy**: Make it accessible via web interface or API\n",
    "4. **Optimize**: Improve speed and accuracy\n",
    "5. **Experiment**: Try different models, prompts, and configurations\n",
    "\n",
    "### Resources\n",
    "\n",
    "- **LangChain Docs**: https://python.langchain.com/\n",
    "- **Ollama**: https://ollama.com\n",
    "- **Full Guide**: See the markdown file for detailed explanations\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸš€ Your Turn!\n",
    "\n",
    "Now it's your turn to experiment. Try:\n",
    "- Different questions\n",
    "- Modifying the prompt\n",
    "- Adding new features\n",
    "- Combining with local documents\n",
    "\n",
    "**Happy Building! ðŸŽ‰**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI Agents",
   "language": "python",
   "name": "ai-agents"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
